{"id":"CP-02un","title":"Reuse document::resolve_path_to_uuid in files API","description":"## Summary\nfiles.rs implements its own fs-root schema traversal in resolve_path, duplicating document::resolve_path_to_uuid. Reuse the shared resolver to keep path resolution consistent.\n\n## Files to modify\n- src/files.rs\n- src/document.rs (if resolver needs small API tweak)\n\n## Implementation steps\n1. Replace manual JSON traversal in files.rs::resolve_path with document::resolve_path_to_uuid.\n2. Keep fs_root_id prefix handling identical.\n3. Ensure error mapping (NoFsRoot/FsRootNotFound/PathNotFound) remains the same.\n\n## Example\nBefore: files.rs parses schema JSON manually.\nAfter: resolve_path uses document::resolve_path_to_uuid(fs_root_content, path, fs_root_id).","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:01:47.787849367Z","created_by":"jes","updated_at":"2026-01-10T10:01:47.787849367Z","comments":[{"id":34,"issue_id":"CP-02un","author":"jes","text":"Not a simple deduplication: document::resolve_path_to_uuid handles single-level schemas synchronously, while files.rs::resolve_path follows node-backed directories by fetching multiple documents asynchronously from DocumentStore. Would require significant refactoring to unify.","created_at":"2026-01-10T12:15:44Z"}]}
{"id":"CP-02vo","title":"CI integration: orchestrator and base processes start correctly","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-11T00:37:09.503061659Z","created_by":"jes","updated_at":"2026-01-11T00:37:09.503061659Z","comments":[{"id":41,"issue_id":"CP-02vo","author":"jes","text":"Covers acceptance criteria P1-P3 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nP1: Orchestrator is running (commonplace-ps shows orchestrator PID)\nP2: Server is running with database persistence\nP3: Workspace sync is running\n\nTest should:\n1. Build release binaries\n2. Start orchestrator\n3. Verify commonplace-ps shows orchestrator PID\n4. Verify server process is running\n5. Verify sync process is running","created_at":"2026-01-11T00:41:08Z"}]}
{"id":"CP-05qu","title":"CI integration: JSONL file append/sync behavior","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T00:37:10.632236261Z","created_by":"jes","updated_at":"2026-01-11T00:37:10.632236261Z","comments":[{"id":42,"issue_id":"CP-05qu","author":"jes","text":"Covers acceptance criteria J1-J8 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nJ1: Create JSONL file with first line\nJ2: Verify appears in sandbox\nJ3: Content matches\nJ4: Append second line\nJ5: Verify sandbox has both lines\nJ6: Append from sandbox\nJ7: Verify workspace has all three lines\nJ8: Delete test file\n\nTest should verify JSONL append-style updates work correctly across sync boundaries.","created_at":"2026-01-11T00:41:09Z"}]}
{"id":"CP-08v","title":"Add MQTT support","description":"Add MQTT protocol support to commonplace. Spec will be supplied (TBD).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T21:56:45.192898672Z","created_by":"jes","updated_at":"2025-12-29T00:39:05.428000759Z","closed_at":"2025-12-29T00:39:05.428000759Z","close_reason":"MQTT support implemented with client, edits/sync/commands handlers, and topic routing. Merged in PR #12."}
{"id":"CP-0cd","title":"Blocks acceptance: File deletion not propagating to sandbox","description":"## Summary\nWhen a file is deleted from the workspace, the deletion does not propagate to the sandbox.\n\n## Repro\n1. Create `workspace/text-to-telegram/test-file.txt`\n2. Wait for sync to sandbox\n3. Delete `workspace/text-to-telegram/test-file.txt`\n4. Check sandbox\n\n## Expected\nFile should be deleted from sandbox\n\n## Actual\nFile still exists in sandbox\n\n## Blocks\nAcceptance criterion D2","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T02:21:00.600383948Z","created_by":"jes","updated_at":"2026-01-03T02:48:55.9217856Z","closed_at":"2026-01-03T02:48:55.9217856Z","close_reason":"Fixed file deletion propagation for node-backed subdirectories. Modified handle_file_deleted to use find_owning_document and push schema to correct subdirectory document."}
{"id":"CP-0dus","title":"Deduplicate upload refresh handling in file_sync","description":"## Summary\nUpload+refresh logic in file_sync.rs is duplicated for JSON and JSONL branches (success check, refresh_from_head, needs_head_refresh). Factor into a shared helper to reduce drift and ensure consistent behavior.\n\n## Files to modify\n- src/sync/file_sync.rs (refactor upload/refresh handling)\n- src/sync/mod.rs or src/sync/file_sync_helpers.rs (new helper if needed)\n\n## Implementation steps\n1. Extract a helper that takes an upload future/result and performs the refresh/needs_head_refresh logic.\n2. Use it for JSON and JSONL branches (and optionally text) to keep behavior aligned.\n3. Ensure failed uploads do not trigger refresh and properly re-set needs_head_refresh.\n\n## Example\nBefore: JSON and JSONL branches duplicate refresh logic.\nAfter: shared helper handles refresh path given upload success flag.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:11:02.886094977Z","created_by":"jes","updated_at":"2026-01-10T09:56:09.002317675Z","closed_at":"2026-01-10T09:56:09.002317675Z","close_reason":"Added handle_upload_refresh helper. Updated 4 call sites (JSON/JSONL in upload_task and upload_task_with_flock) to use shared refresh logic."}
{"id":"CP-0f4m","title":"P2: Restart all processes sharing a script","description":"The script watch map stores a single process name per script UUID. If multiple evaluate processes use the same script, only one will restart when the script changes.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T06:34:07.434565092Z","created_by":"jes","updated_at":"2026-01-07T01:12:38.990946447Z","closed_at":"2026-01-07T01:12:38.990946447Z","close_reason":"Changed ScriptWatchMap to Vec\u003cString\u003e - now restarts all processes sharing a script"}
{"id":"CP-0fy","title":"Sync tool should checkout directory JSON definition as .json file","description":"The sync tool (commonplace-sync) should write the JSON definition of a directory as a file named `.json` in that directory. This would allow users to see and potentially edit the directory's metadata/structure definition alongside the directory contents.","design":"Needs design discussion: What content should be in the .json file? Just metadata, or full fs-root schema? How does editing .json interact with sync? Need specification before implementation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T14:57:56.428338-08:00","updated_at":"2025-12-29T22:37:26.417918062Z","closed_at":"2025-12-29T22:37:26.417918062Z","close_reason":"Sync client now writes fs-root schema to .commonplace.json in synced directories. Updates on server changes via SSE. Merged in PR #26."}
{"id":"CP-0j4","title":"Fix P1: Respect explicit node_id from server schema (PR #4)","description":"From PR #4 Codex review: When the server's FsSchema uses DocEntry.node_id for stable IDs, this should be respected instead of deriving IDs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:23.072384-08:00","updated_at":"2025-12-26T23:43:14.374964-08:00","closed_at":"2025-12-26T23:43:14.374964-08:00","close_reason":"Already fixed and merged to main. The handle_schema_change function now uses collect_paths_from_entry which extracts explicit node_id from DocEntry and uses it instead of deriving from path. See sync.rs lines 1249-1265 and 1358-1379."}
{"id":"CP-0kk7","title":"Show full timestamps in commonplace-log","description":"Dates in log output should show full timestamp (date + time), not just YYYY-MM-DD","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T21:27:15.366276684Z","created_by":"jes","updated_at":"2026-01-03T21:28:13.598758936Z","closed_at":"2026-01-03T21:28:13.598758936Z","close_reason":"Short format now shows YYYY-MM-DD HH:MM instead of just date"}
{"id":"CP-0vu5","title":"Handle missing head.state in delete_schema_entry","description":"## Summary\n\nFrom codex review on PR #104:\n\nThe delete path always calls `create_yjs_json_delete_key` with `base_state.as_deref()` from `/docs/:id/head`, but `state` is optional in the API. In environments where `state` isn't provided (e.g., servers without Yjs state), this call errors and `handle_file_deleted` just logs a warning, so deleted files never get removed from the schema.\n\n## Suggested Fix\n\nConsider detecting a missing `state` and falling back to a safe recovery path (e.g., rescan+merge or an explicit error that triggers retries) so deletions still propagate.\n\n## Current Behavior\n\nThe current implementation was designed to \"fail loud\" rather than silently produce a no-op update. This is intentional but may not be ideal for all environments.\n\n## Context\n\n- PR #104: Fix schema deletion propagation for targeted deletes\n- File: src/sync/client.rs:125","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T21:29:44.512626214Z","created_by":"jes","updated_at":"2026-01-06T21:51:04.1510163Z","closed_at":"2026-01-06T21:51:04.1510163Z","close_reason":"Won't fix - head.state is always present for schema documents in commonplace. The fail-loud behavior is correct and surfaces issues immediately if this assumption ever breaks."}
{"id":"CP-0wfh","title":"Deduplicate watcher initialization","description":"## Summary\nFile/directory/shadow watcher tasks in sync/watcher.rs duplicate watcher setup (notify channel, RecommendedWatcher::new, watch call). Extract a helper to create watchers consistently.\n\n## Files to modify\n- src/sync/watcher.rs\n\n## Implementation steps\n1. Add a helper to create a RecommendedWatcher with the common channel wiring.\n2. Use it in file_watcher_task, directory_watcher_task, and shadow_watcher_task.\n3. Keep debounce configs and watch modes unchanged.\n\n## Example\nBefore: three repeated blocks creating notify_tx/notify_rx and RecommendedWatcher.\nAfter: create_watcher(...) used in each task.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:59:19.175216491Z","created_by":"jes","updated_at":"2026-01-10T11:25:16.732860906Z","closed_at":"2026-01-10T11:25:16.732860906Z","close_reason":"Added create_watcher() helper to deduplicate watcher setup across file_watcher_task, directory_watcher_task, and shadow_watcher_task. Helper encapsulates mpsc channel creation, RecommendedWatcher instantiation, and poll interval configuration."}
{"id":"CP-0x3","title":"Ensure synced JSON files end with a newline","description":"Summary: When syncing .json files, ensure the content written to disk ends with a trailing newline to match expected formatting.\n\nFiles to modify:\n- src/sync/file_sync.rs (text upload/download handling for JSON content)\n- src/services/document.rs (optional: JSON serialization path if server normalizes output)\n- docs/DEVELOPMENT.md or README.md (document newline behavior if needed)\n\nImplementation steps:\n1. Define a helper to normalize JSON content with a trailing newline when writing to disk.\n2. Apply the helper in sync download/refresh paths for JSON files.\n3. Ensure upload paths preserve the newline (avoid stripping).\n4. Add a test case for JSON sync roundtrip with trailing newline.\n\nExample:\nBefore: file ends with `}`\nAfter: file ends with `}` + `\\n`\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T05:42:36.419552735Z","created_by":"jes","updated_at":"2026-01-03T07:05:06.905703074Z","closed_at":"2026-01-03T07:05:06.905703074Z","close_reason":"Added ensure_trailing_newline() helper in file_sync.rs for text file writes"}
{"id":"CP-13l","title":"Server path resolution doesn't follow subdirectory documents","description":"When a subdirectory in the fs-root schema has entries: null and a node_id (meaning the subdirectory contents are in a separate document), the server's resolve_path function in files.rs fails to look up nested paths.\n\nExample: Path 'bartleby/output.txt' should:\n1. Look up 'bartleby' in fs-root (gets node_id 32eb1257-...)\n2. Fetch that document to get bartleby's schema\n3. Look up 'output.txt' in that schema\n\nBut currently resolve_path uses the synchronous resolve_path_to_uuid which only looks at the fs-root content passed to it and can't fetch intermediate documents.\n\nThe fix is to make resolve_path iterate through path segments, fetching intermediate directory documents when entries is null.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T00:25:32.097370344Z","created_by":"jes","updated_at":"2026-01-02T01:04:06.977544302Z","closed_at":"2026-01-02T01:04:06.977544302Z","close_reason":"Fixed by implementing iterative path resolution in files.rs and sse.rs that follows subdirectory node_id references. Merged in PR #82."}
{"id":"CP-1ba","title":"Fix P1: Use text fs-root or send JSON-compatible updates (PR #4)","description":"From PR #4 Codex review: The fs-root node is created with content_type application/json but the updates being sent may not be JSON-compatible.","design":"Resolved by CP-qmk (PR #9). The push_schema_to_server function now uses create_yjs_map_diff_update which generates Y.Map updates instead of Y.Text updates. This makes the updates JSON-compatible as required by the fs-root node's application/json content type.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:23.464711-08:00","updated_at":"2025-12-27T00:14:06.868523-08:00","closed_at":"2025-12-27T00:14:06.868523-08:00","close_reason":"Resolved by CP-qmk fix - client now applies server CRDT state before making changes, ensuring proper tombstone creation for deletions. Merged in PR #9.","dependencies":[{"issue_id":"CP-1ba","depends_on_id":"CP-qmk","type":"blocks","created_at":"2025-12-26T23:41:34.266588-08:00","created_by":"daemon"}]}
{"id":"CP-1evh","title":"cbd: preserve full beads JSONL schema and unknown fields","description":"Summary: Expand the cbd Issue schema to match beads JSONL fields and preserve unknown fields so updates don't drop data.\n\nFiles to modify:\n- src/cbd.rs\n- tests (new cbd schema/roundtrip tests)\n\nImplementation steps:\n1. Inventory fields used in .beads/issues.jsonl (assignee, parent, due/defer, estimate, acceptance, design, notes, external_ref, comments, etc.) and add them to the Issue struct with serde defaults.\n2. Add a #[serde(flatten)] extra map to preserve unknown fields that cbd doesn't explicitly model.\n3. When updating issues, ensure extra fields are retained when appending the new JSONL line.\n4. Ensure JSON output includes the full Issue struct (including optional fields) without losing unknown fields.\n5. Add tests that parse a JSONL line with extra fields, update status, and verify the extra fields are preserved in the appended issue.\n\nExample:\nBefore: An issue with a design field is updated via cbd update and the design field disappears in the new JSONL line.\nAfter: cbd update appends a new line that retains design and other unknown fields.\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T07:02:30.178176522Z","created_by":"jes","updated_at":"2026-01-05T07:51:23.634121945Z","closed_at":"2026-01-05T07:51:23.634121945Z","close_reason":"Added design, notes, comments fields and #[serde(flatten)] extra map for round-trip fidelity","dependencies":[{"issue_id":"CP-1evh","depends_on_id":"CP-1j5m","type":"discovered-from","created_at":"2026-01-05T07:02:30.186405686Z","created_by":"jes"}]}
{"id":"CP-1isz","title":"Add MQTT topic for getting document content","description":"HTTP has GET /docs/:id to retrieve raw document content but there's no MQTT equivalent.\n\nOptions:\n- Add to sync protocol: {type: 'content', req: '...'} request/response\n- Or use commands port with a 'get' verb\n\nThis allows MQTT clients to fetch current content without HTTP.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:05.184991524Z","created_by":"jes","updated_at":"2026-01-09T08:40:10.699448976Z","closed_at":"2026-01-09T08:40:10.699448976Z","close_reason":"Added MQTT get-content command: topic {workspace}/commands/get-content with GetContentRequest/Response message types"}
{"id":"CP-1j5m","title":"Add Commonplace Bug Database CLI backed by JSONL-in-commonplace","description":"Summary: Create the **Commonplace Bug Database** CLI (commonplace-bd) that mimics beads but uses JSONL stored in commonplace as the only source of truth.\n\nFiles to modify:\n- src/bin (new binary, e.g., src/bin/commonplace-bd.rs)\n- src/services/document.rs or src/sync (helpers to read/write JSONL docs in commonplace)\n- docs/DEVELOPMENT.md or README.md (document usage and limitations)\n\nImplementation steps:\n1. Define how the JSONL doc is located (fixed UUID/path or configurable).\n2. Implement core commands (ready/show/create/update/close/sync) against the JSONL doc.\n3. Ensure concurrency handling (append-only writes, optimistic retries).\n4. Add `--json` output parity with beads where practical.\n\nExample:\ncommonplace-bd ready --json\ncommonplace-bd create \"Issue title\" --type task --priority 2 --json\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T20:54:13.887626138Z","created_by":"jes","updated_at":"2026-01-05T06:44:18.080646168Z","closed_at":"2026-01-05T06:44:18.080646168Z","close_reason":"Implemented cbd CLI with list/ready/show/create/close/update commands. Output format matches bd. Added commonplace-bd alias."}
{"id":"CP-1j5o","title":"Deduplicate CLI head fetch logic","description":"## Summary\nShow and replay CLIs duplicate head fetch logic (build URL, GET, status check, parse HeadResponse). Extract a shared helper to fetch document head.\n\n## Files to modify\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/cli/http.rs (new helper)\n\n## Implementation steps\n1. Add helper fetch_head(client, server, uuid, at_commit) -\u003e HeadResponse.\n2. Replace repeated GET/status checks in show/replay with the helper.\n3. Keep error messages identical.\n\n## Example\nBefore: both show and replay build /docs/{id}/head URLs and parse response.\nAfter: cli::http::fetch_head used in both.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:08:28.831268274Z","created_by":"jes","updated_at":"2026-01-10T11:42:56.897139438Z","closed_at":"2026-01-10T11:42:56.897139438Z","close_reason":"Added fetch_head() helper to cli.rs and updated show.rs and replay.rs to use it."}
{"id":"CP-1juq","title":"Filetree-to-xml process fails with XML output merging issue","description":"The filetree-to-xml process starts but fails shortly after. The XML output shows incorrect content merging: default XML template gets mixed with process output. Likely a Yjs/SDK issue with how XML content type documents are handled.","notes":"Fixed underlying XmlFragment support (closed CP-dscp, CP-80z2, CP-az8d). XML documents now use proper XmlFragment internally with compute_xml_diff_update() handling diff/replace operations. Ready to test filetree-to-xml process.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T04:31:26.427100233Z","created_by":"jes","updated_at":"2026-01-07T20:35:23.270765799Z","closed_at":"2026-01-07T20:35:23.270765799Z","close_reason":"Fixed XML XmlFragment CRDT support with proper server state synchronization. Added compute_xml_diff_update_with_base() and fixed filetree-to-xml to use HTTP replace.","dependencies":[{"issue_id":"CP-1juq","depends_on_id":"CP-a85","type":"discovered-from","created_at":"2026-01-07T04:31:26.432732129Z","created_by":"jes"}]}
{"id":"CP-1no1","title":"CI integration: process termination cascade on orchestrator shutdown","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T00:37:13.421808873Z","created_by":"jes","updated_at":"2026-01-11T00:37:13.421808873Z","comments":[{"id":44,"issue_id":"CP-1no1","author":"jes","text":"Covers acceptance criteria T1-T5 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nT1: Record all PIDs via commonplace-ps\nT2: Kill only the orchestrator (SIGTERM, not SIGKILL)\nT3: Verify all processes terminated within 5 seconds\nT4: Verify no orphaned python/node processes remain\nT5: Verify sandbox directories still exist\n\nTests graceful shutdown cascade - orchestrator propagates signals to children.","created_at":"2026-01-11T00:41:12Z"}]}
{"id":"CP-1s6","title":"Orchestrator needs lock file to prevent multiple instances","description":"Running multiple orchestrator instances causes conflicts (e.g., Telegram bot conflict). Need a lock file or similar mechanism to prevent duplicate instances.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T19:41:29.49526294Z","created_by":"jes","updated_at":"2026-01-03T07:09:28.043796045Z","closed_at":"2026-01-03T07:09:28.043796045Z","close_reason":"Already implemented - lock file at lines 60-87 in src/bin/orchestrator.rs using fs2::FileExt"}
{"id":"CP-1ywk","title":"Deduplicate watcher setup in sync binary","description":"## Summary\nSync main (src/bin/sync.rs) repeats watcher setup/teardown logic for file, directory, and shadow watchers across multiple code paths. Extract shared setup helpers to reduce duplication and keep behavior consistent.\n\n## Files to modify\n- src/bin/sync.rs\n- src/sync/watcher.rs or src/sync/watcher_helpers.rs (new helpers)\n\n## Implementation steps\n1. Extract helper functions to start/stop file watcher, directory watcher, and shadow watcher groups.\n2. Replace repeated blocks in sync.rs with helper calls.\n3. Keep pull-only behavior and logging identical.\n\n## Example\nBefore: multiple identical blocks to start directory watcher + shadow watcher.\nAfter: start_watchers(...) used in each mode.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:56:45.741789403Z","created_by":"jes","updated_at":"2026-01-10T09:56:45.741789403Z"}
{"id":"CP-21wm","title":"Deduplicate MQTT doc ID resolution","description":"## Summary\nMQTT edits and sync handlers duplicate fs-root path tracking and path-\u003edocument ID resolution logic. Extract a shared helper to resolve doc IDs from fs-root content/path.\n\n## Files to modify\n- src/mqtt/edits.rs\n- src/mqtt/sync.rs\n- src/mqtt/resolve.rs (new helper)\n\n## Implementation steps\n1. Add a helper to resolve document_id given fs_root_path, fs_root_content, and path.\n2. Replace duplicated fs_root_path checks in edits/sync with the helper.\n3. Keep fs-root special-case behavior unchanged.\n\n## Example\nBefore: both modules check fs_root_path and call resolve_path_to_uuid.\nAfter: mqtt::resolve::resolve_document_id(...) used in both.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:04:40.101659586Z","created_by":"jes","updated_at":"2026-01-10T10:04:40.101659586Z"}
{"id":"CP-2cql","title":"Add MQTT topic for fs-root discovery","description":"HTTP has GET /fs-root to discover the fs-root document ID. No MQTT equivalent.\n\nOptions:\n- Well-known topic: {workspace}/_system/fs-root\n- Or publish fs-root ID as retained message on a system topic\n\nThis is needed for clients to bootstrap without HTTP.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:23.401309702Z","created_by":"jes","updated_at":"2026-01-09T07:05:23.401309702Z"}
{"id":"CP-2ee8","title":"cbd: add list filters/sort and JSON output parity","description":"Summary: Extend cbd list output with bd-style filters/sorting and JSON parity fields.\n\nFiles to modify:\n- src/cbd.rs\n- tests (list filtering and JSON output)\n\nImplementation steps:\n1. Add list flags for --type, --priority, --label, --assignee, --parent, --search, and --sort (created/updated/priority).\n2. Keep existing --status behavior and allow multiple filters to combine.\n3. When --json is set, include dependency_count and dependent_count fields in the output objects.\n4. Preserve deterministic ordering for list output (priority, then created/updated, then id).\n5. Add tests covering filter combinations and count calculations.\n\nExample:\ncommonplace-bd list --status open --label future-work --sort updated --json\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T07:02:50.286670756Z","created_by":"jes","updated_at":"2026-01-05T08:00:40.734788236Z","closed_at":"2026-01-05T08:00:40.734788236Z","close_reason":"Added list filters: --issue-type, --priority, --label, --search, --sort","dependencies":[{"issue_id":"CP-2ee8","depends_on_id":"CP-1evh","type":"blocks","created_at":"2026-01-05T07:02:50.296258852Z","created_by":"jes"},{"issue_id":"CP-2ee8","depends_on_id":"CP-1j5m","type":"discovered-from","created_at":"2026-01-05T07:02:50.301282121Z","created_by":"jes"}]}
{"id":"CP-2gv7","title":"cbd: add dep add/remove and blocked/ready semantics","description":"Summary: Add dependency management commands and make ready/blocked respect blockers and defer dates.\n\nFiles to modify:\n- src/cbd.rs\n- tests (dependency and ready/blocked behavior)\n\nImplementation steps:\n1. Add a dep subcommand with add/remove that follows beads semantics (\"X needs Y\" means Y blocks X).\n2. Update ready to exclude issues that are blocked by dependencies and issues deferred into the future.\n3. Add a blocked command that lists open issues with blockers.\n4. Ensure JSON output includes dependency metadata for ready/blocked results.\n5. Add tests for dep add/remove, ready filtering with blockers, and defer date handling.\n\nExample:\nBefore: cbd ready lists issues even if they depend on other open issues.\nAfter: cbd ready excludes blocked issues; cbd blocked shows them.\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T07:02:44.818770932Z","created_by":"jes","updated_at":"2026-01-05T08:16:03.555495072Z","closed_at":"2026-01-05T08:16:03.555495072Z","close_reason":"Added dep add/remove/list commands, blocked command, and fixed ready to check open blockers only","dependencies":[{"issue_id":"CP-2gv7","depends_on_id":"CP-1evh","type":"blocks","created_at":"2026-01-05T07:02:44.826902316Z","created_by":"jes"},{"issue_id":"CP-2gv7","depends_on_id":"CP-1j5m","type":"discovered-from","created_at":"2026-01-05T07:02:44.830298599Z","created_by":"jes"}]}
{"id":"CP-2ie","title":"Store images as external blobs with versioned JSON references","description":"Summary: Store images as external blobs (outside redb), with versioned JSON documents in commonplace referencing the blobs. Sync should handle uploads/downloads transparently.\n\nFiles to modify:\n- src/services/document.rs (new content type or metadata for blob references)\n- src/sync/file_sync.rs (upload/download logic for image files)\n- src/sync/content_type.rs (detect image types and map to blob handling)\n- docs/ARCHITECTURE.md or README.md (document blob storage)\n\nImplementation steps:\n1. Define a blob storage location and format (e.g., content-addressed files on disk).\n2. Create a JSON schema for image documents that references blobs and preserves version history.\n3. Update sync to detect image files, store/update blobs, and sync JSON metadata instead of raw bytes.\n4. Ensure deletion and GC of unused blobs is safe (future work acceptable).\n\nExample:\n- Upload image.png → stores blob at /var/commonplace/blobs/sha256-... and document content becomes JSON referencing that blob.\n- Sync downloads JSON, then fetches blob to materialize image file locally.\n\nLabels:\n- future-work","status":"closed","priority":4,"issue_type":"feature","created_at":"2026-01-03T07:02:24.612595692Z","created_by":"jes","updated_at":"2026-01-05T08:45:42.006485547Z","closed_at":"2026-01-05T08:45:42.006485547Z","close_reason":"Won't implement: images stored as base64 inline instead of external blobs - simpler approach chosen","labels":["future-work"]}
{"id":"CP-2is8","title":"P2: Array merges in create_yjs_json_merge still delete items - additive_only flag not respected for arrays in yjs.rs:96-103","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T20:37:44.87543036Z","created_by":"jes","updated_at":"2026-01-07T00:57:17.604097142Z","closed_at":"2026-01-07T00:57:17.604097142Z","close_reason":"Fixed array handling to respect additive_only flag - now appends instead of clearing when merging"}
{"id":"CP-2mgl","title":"Sync clients should not push empty Yjs commits","description":"Summary: Prevent filesystem sync clients from uploading empty Yjs updates (e.g., base64 \"AAA=\") that create no-op commits and can confuse merge semantics.\n\nFiles to modify:\n- src/sync/file_sync.rs\n- src/sync/sse.rs\n- src/mqtt/edits.rs (if validation is done on ingest)\n\nImplementation steps:\n1. Identify the code path that uploads Yjs updates from local file changes; add a guard that rejects empty updates and logs a debug message.\n2. If the server accepts edits over MQTT, add a validation step that drops empty updates before persisting.\n3. Add tests for a local edit that produces an empty update to ensure no commit is created.\n\nExample:\nBefore: local change produces update \"AAA=\", sync pushes it and a commit is recorded.\nAfter: sync detects empty update and skips upload; no commit is created.\n","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-05T02:05:35.901054776Z","created_by":"jes","updated_at":"2026-01-05T02:24:22.73674133Z","closed_at":"2026-01-05T02:24:22.73674133Z","close_reason":"Server-side guard in replace_content() is sufficient - returns existing HEAD when no changes detected (1deb1d0)"}
{"id":"CP-2oh6","title":"Deduplicate MQTT path subscribe/unsubscribe","description":"## Summary\nMQTT edits and sync handlers duplicate subscribe/unsubscribe logic (topic building, QoS, subscribed_paths tracking). Extract shared helpers to reduce boilerplate.\n\n## Files to modify\n- src/mqtt/edits.rs\n- src/mqtt/sync.rs\n- src/mqtt/subscriptions.rs (new helper)\n\n## Implementation steps\n1. Add helper functions for subscribe/unsubscribe given Topic + QoS + subscribed_paths set.\n2. Replace duplicated blocks in edits/sync handlers.\n3. Keep QoS differences (edits=AtLeastOnce, sync=AtMostOnce) intact.\n\n## Example\nBefore: both handlers build topic string and update subscribed_paths.\nAfter: mqtt::subscriptions::subscribe_path(client, topic, qos, paths).","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:04:17.123042906Z","created_by":"jes","updated_at":"2026-01-10T10:04:17.123042906Z"}
{"id":"CP-2pr","title":"Break sync.rs into smaller modules","description":"src/bin/sync.rs is 2734 lines and too large for a single module. It should be refactored into smaller, focused modules for better maintainability and readability.\n\nSuggested breakdown:\n- Core sync state machine\n- File system operations  \n- MQTT message handling\n- Conflict resolution logic\n- Document path resolution","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T07:03:46.71615207Z","created_by":"jes","updated_at":"2026-01-03T07:48:47.031028968Z","closed_at":"2026-01-03T07:48:47.031028968Z","close_reason":"Already factored - sync is split into 13 modules under src/sync/. Main binary is 1296 lines.","labels":["refactor","tech-debt"]}
{"id":"CP-2u5","title":"Replace endpoint doesn't work for JSON documents","description":"The /docs/{id}/replace endpoint uses character-level diffing via Y.Text, but JSON documents use Y.Map internally. When calling replace on a JSON document (like fs-root), the content is not updated. Workaround: Use the /edit endpoint with create_yjs_json_update instead.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-01T00:27:16.371497913Z","created_by":"jes","updated_at":"2026-01-01T02:05:49.98180474Z","closed_at":"2026-01-01T02:05:49.98180474Z","close_reason":"Fixed by modifying replace_content to use create_yjs_json_update for JSON documents"}
{"id":"CP-2xd9","title":"Centralize SCHEMA_FILENAME constant","description":"## Summary\nSCHEMA_FILENAME constant is duplicated across many modules (workspace.rs, sync/schema_io.rs, sync/directory.rs, multiple CLI binaries). Centralize it in one module and re-export to avoid drift.\n\n## Files to modify\n- src/workspace.rs or src/sync/schema_io.rs (single source of truth)\n- src/bin/link.rs, src/bin/uuid.rs, src/bin/replay.rs, src/bin/cmd.rs\n- src/sync/directory.rs (use shared constant)\n\n## Implementation steps\n1. Pick a single module to define SCHEMA_FILENAME (e.g., workspace.rs or sync/schema_io.rs).\n2. Re-export it from a common module (src/lib.rs or src/workspace.rs).\n3. Replace local const definitions and imports with the shared constant.\n\n## Example\nBefore: SCHEMA_FILENAME defined in 6+ files.\nAfter: one constant, reused everywhere.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:32:32.200348901Z","created_by":"jes","updated_at":"2026-01-10T10:25:49.48676732Z","closed_at":"2026-01-10T10:25:49.48676732Z","close_reason":"Removed 6 duplicate SCHEMA_FILENAME definitions. All files now import from the canonical location in sync::schema_io (already re-exported via sync module)."}
{"id":"CP-2zoz","title":"Deduplicate CLI output formatting helpers","description":"## Summary\nCLI output formatting is duplicated across log/show/replay/ps (JSON pretty printing, timestamp formatting, header layouts). Extract shared output helpers to keep formatting consistent.\n\n## Files to modify\n- src/bin/log.rs\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/bin/ps.rs\n- src/cli/output.rs (new shared helpers)\n\n## Implementation steps\n1. Add helpers for JSON pretty printing and timestamp formatting (reuse workspace::format_timestamp where possible).\n2. Replace local print/format blocks in CLI binaries with shared helpers.\n3. Keep output identical for compatibility.\n\n## Example\nBefore: each CLI binary prints JSON and timestamps on its own.\nAfter: shared output::print_json, output::format_timestamp used everywhere.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:51:25.3529804Z","created_by":"jes","updated_at":"2026-01-10T09:51:25.3529804Z","comments":[{"id":36,"issue_id":"CP-2zoz","author":"jes","text":"Analysis: ps.rs format_timestamp uses Duration::from_secs(ts) while workspace::format_timestamp uses Duration::from_millis(ts). The timestamps come from different sources with different units. Would need to either standardize units or add a second helper.","created_at":"2026-01-10T12:16:56Z"}]}
{"id":"CP-320","title":"Sync push loses node_ids when updating fs-root schema on server","description":"The sync correctly preserves node_ids when scanning locally (CP-8ou), but when it pushes the schema to the server, the node_ids become null.\n\nLocal .commonplace.json has correct shared UUIDs:\n- bartleby/prompts.txt: 0b250a22-3163-49df-8634-534585865cdd\n- telegram/content.txt: 0b250a22-3163-49df-8634-534585865cdd\n\nBut server's fs-root document has null for both.\n\nThis breaks file linking because the reconciler sees null and generates new random UUIDs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T00:11:19.958347431Z","created_by":"jes","updated_at":"2026-01-01T02:06:52.691341058Z","closed_at":"2026-01-01T02:06:52.691341058Z","close_reason":"Symptom of CP-g0c schema corruption - node_id preservation tests pass once schema is not corrupted"}
{"id":"CP-345n","title":"Share NATS subject formatting helpers between server NatsManager and actor NatsSubscriptionProvider","description":"Summary: commonplaced-2025/src/server/nats.ts and commonplaced-2025/src/actors/common/nats-subscription-provider.ts both build NATS subjects by normalizing paths (replace '/' with '.') and parse JSON payloads; extract shared helpers to keep subject formatting consistent.\n\nFiles to modify:\n- commonplaced-2025/src/server/nats.ts\n- commonplaced-2025/src/actors/common/nats-subscription-provider.ts\n- commonplaced-2025/src/server (or commonplaced-2025/src/actors/common) shared NATS helper module\n\nImplementation steps:\n1) Add helper functions like formatDirSubject(path) and formatDocSubject(docId) plus safePath normalization.\n2) Replace duplicated subject building in both modules with the helper(s).\n3) Optionally centralize JSON encode/decode + error logging for NATS messages.\n\nExample:\nBefore: path.replace(/\\//g, '.').replace(/^\\./, '') duplicated in two files.\nAfter: const subject = formatDirSubject(path, { wildcard: true }) used in both.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:21:02.452267955Z","created_by":"jes","updated_at":"2026-01-10T10:21:20.815373732Z"}
{"id":"CP-365","title":"commonplace-sync --node should accept path relative to fs-root","description":"According to SANDBOX_LINKING.md and bartleby-integration.md, --node should accept path-style names like 'bartleby-workspace' or 'workspace/bartleby' relative to the server's fs-root.\n\n**Expected:**\n```bash\ncommonplace-sync --node workspace/bartleby --server http://localhost:3000\n```\n\n**Actual:**\nMust use UUIDs:\n```bash\ncommonplace-sync --node 2750f8b7-7296-41f9-854d-7b130a707930 --server http://localhost:3000\n```\n\n**Requirements:**\n1. Accept path strings relative to server's --fs-root\n2. Resolve path → UUID by querying the server's fs-root schema\n3. Work with --sandbox mode for dynamic process management\n\n**Workaround:** Use UUIDs directly (look them up from server's .commonplace.json or API)","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T16:56:49.691337582Z","created_by":"jes","updated_at":"2026-01-01T19:02:28.161177467Z","closed_at":"2026-01-01T19:02:28.161177467Z","close_reason":"Implemented sandbox-exec config, path resolution (--path flag), recursive discovery (--recursive flag), and source_path tracking to prevent multi-config interference. All merged in PR #78.","comments":[{"id":15,"issue_id":"CP-365","author":"jes","text":"**Updated understanding:** The original design was even simpler - no --node flag needed at all.\n\nThe sync process should discover its server path automatically:\n1. Look at where processes.json is located on the filesystem\n2. Use the process name from that config\n3. Derive the server path: `{processes.json parent}/{process_name}`\n\nExample: If processes.json is at `/home/jes/commonplace/workspace/processes.json` and contains a 'bartleby' process, the sandbox should automatically sync to the server path `workspace/bartleby` (relative to fs-root).\n\nThis eliminates the need for explicit --node or --path arguments entirely.","created_at":"2026-01-01T16:58:55Z"}]}
{"id":"CP-36cy","title":"Schema sync should use merkle-CRDT on JSON for reliable concurrent edits","notes":"Ideas:\n- Treat schema JSON as a CRDT doc and always merge via Yjs state: fetch head.state, build update with create_yjs_json_update (or merge+delete) against base_state, instead of fetch-modify-push overwrites.\n- For deletions, emit explicit delete-key updates (create_yjs_json_delete_key) rather than relying on local cleanup races.\n- Tie cleanup to authoritative server schema changes only; skip deleting local files right after local schema writes (use written_schemas echo detection) to avoid self-races.\n- Consider moving schema sync onto the same JSON file-sync path (push_json_content) so it shares diff logic and CRDT semantics.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T03:28:29.478348701Z","created_by":"jes","updated_at":"2026-01-10T04:37:31.108760163Z","closed_at":"2026-01-10T04:37:31.108760163Z","close_reason":"Implemented echo detection for schema deletions - checks written_schemas before deleting files to prevent race conditions where newly created files get deleted by stale SSE updates","comments":[{"id":25,"issue_id":"CP-36cy","author":"jes","text":"Current schema sync is flaky because:\n1. Multiple sync clients can update the same schema concurrently\n2. The sync logic deletes files that aren't in the server schema (race condition)\n3. Schema changes aren't merged using CRDT - they're just overwritten\n\nSolution: Use the existing Yjs CRDT on the schema JSON document to merge edits properly instead of fetch-modify-push pattern. This would prevent:\n- Race conditions where newly created files get deleted\n- Lost updates when multiple sandboxes modify the same schema\n- Need for timing hacks (sleep before exec starts)\n\nCurrently affecting bartleby and other sandbox processes - log files get deleted immediately after creation.","created_at":"2026-01-10T03:28:37Z"}]}
{"id":"CP-3hfo","title":"Use workspace::find_workspace_root across CLI","description":"## Summary\nDuplicate find_workspace_root implementations exist in bin/link.rs, bin/uuid.rs, and bin/replay.rs even though workspace.rs already provides one. Consolidate to use workspace::find_workspace_root.\n\n## Files to modify\n- src/bin/link.rs\n- src/bin/uuid.rs\n- src/bin/replay.rs\n- src/workspace.rs (if signature tweaks are needed)\n\n## Implementation steps\n1. Remove local find_workspace_root in each CLI file.\n2. Import and use workspace::find_workspace_root.\n3. Ensure error types are mapped consistently (WorkspaceError -\u003e Box\u003cdyn Error\u003e or convert).\n\n## Example\nBefore: three separate find_workspace_root functions.\nAfter: all CLI commands use workspace::find_workspace_root.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:30:37.258904862Z","created_by":"jes","updated_at":"2026-01-10T09:30:37.258904862Z"}
{"id":"CP-3jcs","title":"Sandbox processes should log stdout/stderr to files in sandbox folder","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-09T18:21:20.521746183Z","created_by":"jes","updated_at":"2026-01-09T20:08:13.337144563Z","closed_at":"2026-01-09T20:08:13.337144563Z","close_reason":"Implemented in sync.rs - sandbox exec stdout/stderr now written to __\u003cprogram\u003e.stdout.txt and __\u003cprogram\u003e.stderr.txt files in sandbox directory","comments":[{"id":23,"issue_id":"CP-3jcs","author":"jes","text":"Sandbox sync processes (commonplace-sync --sandbox --exec) should capture and persist their stdout/stderr output to files in the sandbox directory.\n\nProposed filenames:\n- __\u003cprocess-name\u003e.stdout.txt\n- __\u003cprocess-name\u003e.stderr.txt\n\nThis would allow debugging sandbox processes by inspecting their logs, and the logs would be visible/synced through commonplace.\n\nImplementation location: src/orchestrator/discovered_manager.rs where sandbox processes are spawned (around line 246).","created_at":"2026-01-09T18:21:35Z"}]}
{"id":"CP-3kmf","title":"spawn_file_sync_tasks_with_flock never called - flock protection disabled","description":"The function spawn_file_sync_tasks_with_flock exists and provides flock-protected SSE handling, but the sync binary (src/bin/sync.rs) only calls spawn_file_sync_tasks, which uses sse_task_with_tracker instead of sse_task_with_flock. This means the flock protection designed to prevent race conditions between process writes (e.g., bartleby) and sync overwrites is not being used. Result: Double-sends when bartleby writes a line, sync pushes it, then sync receives SSE and overwrites bartleby's subsequent deletion, causing the line to reappear and be sent again.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T18:51:14.672852874Z","created_by":"jes","updated_at":"2026-01-10T18:57:34.730218143Z","closed_at":"2026-01-10T18:57:34.730218143Z","close_reason":"Wired up spawn_file_sync_tasks_with_flock in both directory mode and exec mode. Committed as 2909ee4."}
{"id":"CP-3ky","title":"Implement Router Documents (docs/ROUTER.md)","description":"Implement the router document feature as specified in docs/ROUTER.md:\n\n- Add `--router \u003cnode-id\u003e` CLI flag (repeatable)\n- Router documents are JSON with `version`, `nodes`, and `edges` \n- Server subscribes to router document blue port for changes\n- Apply/diff wiring based on router document content\n- Emit `router.error` events on red port for errors\n- Handle node creation hints from `nodes` section\n- Track router-created wires separately from other wires","design":"Implementation approach:\n\n1. New module: src/router/ with:\n   - mod.rs: Module exports\n   - error.rs: RouterError enum for parse/validation/cycle errors\n   - schema.rs: RouterSchema, Edge, NodeSpec, PortType structs with serde\n   - manager.rs: RouterManager that subscribes to router document's blue port\n\n2. CLI: Added --router \u003cnode-id\u003e flag (repeatable) in cli.rs\n\n3. RouterConfig: Added routers: Vec\u003cString\u003e field\n\n4. Initialization: In lib.rs, for each router ID:\n   - Get or create JSON document node\n   - Create RouterManager with registry reference\n   - Start background watcher task\n   - Perform initial wiring\n\n5. Wire management:\n   - RouterManager tracks wires it created (WireKey -\u003e SubscriptionId)\n   - On each reconcile, diffs desired vs current wires\n   - Removes wires no longer declared, adds new ones\n   - Cycle detection handled by registry, emits router.error on failure\n\n6. Error handling:\n   - Parse errors, unsupported version, missing nodes, cycles → router.error event on red port","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-26T18:29:01.089173-08:00","updated_at":"2025-12-26T23:22:36.003263-08:00","closed_at":"2025-12-26T23:22:36.003263-08:00","close_reason":"Router documents feature implemented. Added --router CLI flag, RouterManager, schema validation, wiring management, and error events. Addressed 4 P2 review suggestions. Merged in PR #8."}
{"id":"CP-3ouw","title":"Deduplicate MQTT fs-root content cache","description":"## Summary\nMQTT edits and sync handlers duplicate fs_root_content storage and setter methods. Extract a shared struct or helper to manage fs-root cache consistently.\n\n## Files to modify\n- src/mqtt/edits.rs\n- src/mqtt/sync.rs\n- src/mqtt/fs_root_cache.rs (new helper)\n\n## Implementation steps\n1. Add a shared FsRootCache with get/set helpers.\n2. Replace duplicated RwLock\u003cString\u003e fields and setters in edits/sync.\n3. Keep existing behavior (initial empty string).\n\n## Example\nBefore: both handlers define fs_root_content: RwLock\u003cString\u003e and set_fs_root_content.\nAfter: shared FsRootCache used in both.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:05:38.093856717Z","created_by":"jes","updated_at":"2026-01-10T10:05:38.093856717Z"}
{"id":"CP-3st","title":"Reconciler doesn't create documents for UUIDs in node-backed subdirectory schemas","description":"When a subdirectory has entries: null and a node_id (node-backed), the reconciler creates that subdirectory document but doesn't recursively reconcile the UUIDs defined within that subdirectory's schema.\n\nExample: sandbox-workspace/bartleby has entries: null, node_id: a8e1bff4-...\n1. Sync pushes bartleby/.commonplace.json to document a8e1bff4\n2. That schema defines output.txt -\u003e 06f938e2, prompts.txt -\u003e 6fe7bbef\n3. But these UUIDs never get created as documents\n4. Sync fails with 'Identifier not found' after 30 retries\n\nThe fix: When reconciling a node-backed directory, the server should fetch that directory's schema document and recursively reconcile any UUIDs defined within it.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T01:35:20.370037954Z","created_by":"jes","updated_at":"2026-01-02T02:09:46.171363236Z","closed_at":"2026-01-02T02:09:46.171363236Z","close_reason":"Closed"}
{"id":"CP-3tnk","title":"P2: path.rs rejects directories with inline schema entries","description":"In path.rs around lines 48-57, path resolution fails when a directory has an inline (non-file) .commonplace.json entry. The code assumes all entries point to files but inline entries are valid in the schema.\n\nLocation: src/path.rs:48-57\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:10:02.56506803Z","created_by":"jes","updated_at":"2026-01-06T00:55:15.237327857Z","closed_at":"2026-01-06T00:55:15.237327857Z","close_reason":"Won't fix - inline subdirectories are deprecated and migration exists"}
{"id":"CP-3ve","title":"Identify and refactor large files into smaller modules","description":"Audit the codebase for files that have grown too large and could benefit from being split into smaller, more focused modules. Look for files with multiple distinct responsibilities that could be separated.","design":"Test coverage now in place (40 tests). Ready for refactoring.\n\nTarget module structure for src/bin/sync.rs split:\n\n1. src/sync/mod.rs - Module root, re-exports\n2. src/sync/urls.rs - URL building functions (encode_node_id, encode_path, normalize_path, build_*_url)\n3. src/sync/yjs.rs - Yjs update creation (create_yjs_text_update, create_yjs_json_update, json_value_to_any)\n4. src/sync/types.rs - Data structures (HeadResponse, EditResponse, ForkResponse, FileEvent, SyncState, etc.)\n5. src/sync/client.rs - HTTP client operations (SyncClient trait + impl)\n6. src/sync/file_sync.rs - File mode sync (run_file_mode, file_watcher_task, upload_task)\n7. src/sync/dir_sync.rs - Directory mode sync (run_directory_mode, directory_watcher_task, handle_schema_change)\n8. src/sync/sse.rs - SSE handling (sse_task, directory_sse_task, handle_server_edit, refresh_from_head)\n\nsrc/bin/sync.rs becomes thin wrapper: just main() + Args parsing, delegates to sync module.\n\nTests move with their functions to respective modules.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-29T13:47:42.244104-08:00","updated_at":"2025-12-30T03:24:24.008280981Z","closed_at":"2025-12-30T03:24:24.008280981Z","close_reason":"Refactored sync.rs into modular structure. Created src/sync/ with urls.rs, yjs.rs, types.rs (implemented) and client.rs, dir_sync.rs, file_sync.rs, sse.rs (placeholders for Phase 2). Added 44 tests. PR #33 merged."}
{"id":"CP-3xx","title":"commonplace-ps doesn't work with recursive mode orchestrator","description":"The recursive mode orchestrator (DiscoveredProcessManager) doesn't write to the status file, making commonplace-ps report 'Orchestrator is not running'.\n\nThe non-recursive mode (ProcessManager) works correctly after the fix in commit 8610987.\n\nNeed to verify write_status() is being called in DiscoveredProcessManager and that it's writing to the correct path.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T13:20:38.148247847Z","created_by":"jes","updated_at":"2026-01-03T19:05:59.824799775Z","closed_at":"2026-01-03T19:05:59.824799775Z","close_reason":"Already fixed. DiscoveredProcessManager correctly writes status file with process info."}
{"id":"CP-42ll","title":"Deduplicate API response structs","description":"## Summary\nDocHeadResponse/DocEditResponse/ReplaceResponse structs are duplicated between api.rs and files.rs. Extract shared response structs to avoid drift across HTTP endpoints.\n\n## Files to modify\n- src/api.rs\n- src/files.rs\n- src/http/response_types.rs (new shared module)\n\n## Implementation steps\n1. Move shared response structs (DocHeadResponse, DocEditResponse, ReplaceResponse, ReplaceSummary) into a common module.\n2. Update api.rs and files.rs to use the shared types.\n3. Ensure serde annotations stay identical.\n\n## Example\nBefore: DocHeadResponse defined twice.\nAfter: http::response_types::DocHeadResponse used in both.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:00:09.301696851Z","created_by":"jes","updated_at":"2026-01-10T12:07:26.435928975Z","closed_at":"2026-01-10T12:07:26.435928975Z","close_reason":"Made response structs public in api.rs; files.rs now imports them instead of duplicating"}
{"id":"CP-47e3","title":"Blackboard like functionality (\"black\")","description":"Future exploration: Add blackboard like functionality to commonplace, potentially called \"black\". Details TBD.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T23:45:04.079809774Z","created_by":"jes","updated_at":"2026-01-05T23:45:04.079809774Z","labels":["future-work"]}
{"id":"CP-4a0","title":"Refactor discovery.rs: Extract process state machine","description":"orchestrator/discovery.rs is 542 lines. Extract `ManagedDiscoveredProcess` state machine and lifecycle handling into separate module, keeping only config parsing and discovery logic in discovery.rs.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-30T00:34:51.339252-08:00","updated_at":"2025-12-30T16:55:35.033866881Z","closed_at":"2025-12-30T16:55:35.033866881Z","close_reason":"Extracted process state machine to discovered_manager.rs - PR #44 merged"}
{"id":"CP-4br","title":"Investigate excessive SSE connections during sync","description":"Summary: Logs show repeated \"SSE connection opened\" lines during sync; investigate why the sync client is opening many SSE connections for the same targets instead of maintaining one per file (plus fs-root), and fix the spawn/reconnect logic to avoid duplicates.\\n\\nFiles to modify: src/bin/sync.rs, src/sync/file_sync.rs, src/sync/sse.rs, src/sync/dir_sync.rs.\\n\\nImplementation steps:\\n1. Reproduce by running sync with logs enabled and count connection opens per file and for fs-root.\\n2. Trace where SSE tasks are spawned (per-file and fs-root) and ensure each target gets exactly one EventSource per lifecycle.\\n3. Identify whether reconnection loops or restart logic cause duplicate tasks; add guards or reuse existing connections.\\n4. Add/adjust logging or tests to assert single SSE connection per target (per file + fs-root).\\n\\nExample: For a sync of 3 files, expect 4 SSE connections total (3 files + fs-root); logs currently show repeated opens for the same file on each sync cycle.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-01T05:58:35.263828365Z","created_by":"jes","updated_at":"2026-01-01T11:20:25.079063026Z","closed_at":"2026-01-01T11:20:25.079063026Z","close_reason":"Investigated and found no bug. SSE connection architecture is correct: one connection per file plus fs-root, with automatic reconnection on disconnect. The 'excessive' connection logs are expected behavior from reconnection after server restarts or network issues.","comments":[{"id":11,"issue_id":"CP-4br","author":"jes","text":"Investigation notes:\n\n1. SSE task architecture looks correct:\n   - One SSE task per file (spawned via spawn_file_sync_tasks)\n   - One SSE task for fs-root directory (directory_sse_task)\n   - handle_schema_change has guard: if !known_paths.contains(path) before spawning\n\n2. Reconnection behavior is intentional:\n   - Both sse_task and directory_sse_task have reconnect loops with 5s delay\n   - 'SSE connection opened' is logged on each (re)connection\n\n3. Possible causes of 'excessive' logs:\n   - Server restarts causing all clients to reconnect\n   - Network issues causing disconnects\n   - Server SSE timeout dropping idle connections\n\n4. To verify if this is a real bug:\n   - Need to reproduce with a stable server\n   - Count unique SSE URLs being connected (should match file count + 1)\n   - Check if same URL is connected multiple times concurrently\n\nWill check server-side SSE connection handling next.","created_at":"2026-01-01T11:19:35Z"},{"id":12,"issue_id":"CP-4br","author":"jes","text":"Server-side SSE analysis:\n\n1. Server uses 30-second keep-alive interval (sse.rs:396)\n2. SSE connections stay open as long as:\n   - Client maintains connection\n   - Document exists\n   - No broadcast channel errors\n\n3. Connection drops would happen on:\n   - Server restart\n   - Document deletion\n   - Network issues\n   - Broadcast channel closure\n\nConclusion: The code appears correct. SSE connections are intentionally reconnecting (with 5s delay) when closed. The 'excessive' logs may be:\n1. Normal reconnection after server restarts during testing\n2. Expected behavior for initial connection setup\n3. Debug-level info logs appearing frequently\n\nRecommendation: Either close as not-a-bug, or demote the 'SSE connection opened' log from info! to debug! to reduce noise.\n\nI'll close this as the architecture appears correct and the behavior is expected.","created_at":"2026-01-01T11:20:17Z"}]}
{"id":"CP-4e9","title":"Sync protocol infinite loop: commit:null parsed as request","design":"When store has no HEAD commit, it responds with {type:head, req:..., commit:null}. The Rust code distinguishes requests (commit:None) from responses (commit:Some). But null deserializes to None, so the store receives its own response and re-handles it as a request, creating an infinite loop. Fix: use distinct message types like head_request vs head_response, or add a direction field.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-30T00:30:17.030876486Z","created_by":"jes","updated_at":"2025-12-30T01:17:06.619813304Z","closed_at":"2025-12-30T01:17:06.619813304Z","close_reason":"Fixed by splitting Head message into Head (request) and HeadResponse (response). Committed in ded73c4."}
{"id":"CP-4f7","title":"Deprecate derived IDs - all files and directories should have UUIDs","description":"Derived IDs (like fs-root:path) are fragile - they break on rename. Every file and directory entry should have an explicit UUID node_id instead.\n\nThis affects:\n- Reconciler: should generate UUIDs for entries without node_id\n- Sync tool: should ensure all entries have UUIDs when scanning\n- Link tool: already does this for source files\n- Schema migration: existing derived IDs should be converted to UUIDs\n\nBenefits:\n- Stable references across renames\n- Links don't break when source is renamed\n- Simpler path resolution (just lookup by UUID)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:48:39.651047828Z","created_by":"jes","updated_at":"2025-12-30T23:12:28.538534864Z","closed_at":"2025-12-30T23:12:28.538534864Z","close_reason":"Migration now generates UUIDs for all entries. Removed derive_doc_id method. PR #53 merged."}
{"id":"CP-4k28","title":"Single-file sync should create file if it doesn't exist","description":"When using `commonplace-sync --file \u003cpath\u003e --path \u003cserver-path\u003e`, if the server path doesn't exist, sync fails with:\n\n```\nFailed to resolve path 'beads/commonplace-issues.jsonl': Path not found\n```\n\nExpected behavior: sync should create the document on the server if it doesn't exist, similar to how directory sync creates new files.\n\nWorkaround: manually copy the file to a synced directory first so main sync creates it, then beads-sync can take over.\n\nUse case: syncing a file from outside the main workspace tree (e.g., .beads/issues.jsonl) into the workspace.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T19:46:28.849935844Z","created_by":"jes","updated_at":"2026-01-04T00:02:41.755867441Z","closed_at":"2026-01-04T00:02:41.755867441Z","close_reason":"Implemented resolve_or_create_path function that creates documents on the server when using single-file sync with a non-existent path. Generates local UUID, updates parent schema, triggers reconciler. Commit: c1d6dbd"}
{"id":"CP-4q3e","title":"Reuse SSESubscriptionProvider for decorator-triggerer document subscriptions","description":"Summary: decorator-triggerer.ts manually creates EventSource subscriptions and TODO cleanup, duplicating logic already in SSESubscriptionProvider; reuse the provider to centralize SSE subscription behavior.\n\nFiles to modify:\n- commonplaced-2025/src/actors/decorator-triggerer.ts\n- commonplaced-2025/src/actors/common/sse-subscription-provider.ts (if new helper is needed)\n- commonplaced-2025/src/actors/common/subscription-system.ts\n\nImplementation steps:\n1) Instantiate SSESubscriptionProvider (or the shared ISubscriptionProvider) in decorator-triggerer.\n2) Replace manual EventSource setup with provider.subscribeToDocument() and track ISubscription handles.\n3) Implement unsubscribe/cleanup using provider.unsubscribe() instead of TODO placeholders.\n\nExample:\nBefore: new EventSource(url) with onmessage/onerror in decorator-triggerer.\nAfter: subscriptionProvider.subscribeToDocument(docId, cb) and cleanup via subscription.unsubscribe().","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:20:14.261890225Z","created_by":"jes","updated_at":"2026-01-10T10:20:32.888472068Z"}
{"id":"CP-4qm","title":"commonplace-replay should work from outside sync directory","description":"commonplace-replay fails when run from outside the sync directory, even when given a valid path to a synced file.\n\nExample:\n```\njes@commonplace:~/commonplace$ ./target/release/commonplace-replay workspace/bartleby/prompts.txt\nError: \"Not in a commonplace sync directory: .commonplace.json not found\"\n\njes@commonplace:~/commonplace$ cd workspace/\njes@commonplace:~/commonplace/workspace$ ../target/release/commonplace-replay bartleby/prompts.txt\nCommit: 9eb1736331c96195d0b5102a88a9ea7b6f02170a6708493c345b1c241bc1ea12\n```\n\nExpected: Should find .commonplace.json by walking up from the file path, not from cwd.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T11:45:47.616212936Z","created_by":"jes","updated_at":"2026-01-03T18:49:04.229496192Z","closed_at":"2026-01-03T18:49:04.229496192Z","close_reason":"Fixed. Now searches for workspace root starting from file's directory, not cwd."}
{"id":"CP-4r4d","title":"Shadow inode GC never called - memory/disk leak","description":"InodeTracker::gc() is implemented (state.rs:258-283) but never invoked anywhere. Shadow hardlinks and inode state entries accumulate indefinitely during sync sessions.\n\nImpact: Long-running sync with frequent edits causes unbounded growth of:\n- InodeTracker.states HashMap (memory)\n- Shadow directory hardlinks (disk)\n\nFix: Add a periodic GC task (e.g., every 5 minutes) that:\n1. Calls tracker.gc()\n2. Removes returned shadow file paths from disk\n\nRelated: CP-txg0 review","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T08:57:10.943768718Z","created_by":"jes","updated_at":"2026-01-05T23:50:35.408929138Z","closed_at":"2026-01-05T23:50:35.408929138Z","close_reason":"Added shadow_gc_task that runs every 5 minutes and cleans up stale shadow hardlinks"}
{"id":"CP-4t1k","title":"Deduplicate doc URL construction in sync","description":"## Summary\nURL construction for doc head/info endpoints is duplicated in dir_sync.rs and file_sync.rs instead of using sync/urls helpers. Consolidate these to use build_head_url/build_info_url to reduce drift.\n\n## Files to modify\n- src/sync/dir_sync.rs\n- src/sync/file_sync.rs\n- src/sync/urls.rs (add build_info_url if missing)\n\n## Implementation steps\n1. Add missing URL helpers (e.g., build_info_url) to sync/urls.rs.\n2. Replace inline format!(\"{}/docs/{}/head\", ...) uses with build_head_url/build_info_url.\n3. Keep behavior unchanged for encode_node_id vs encode_path handling.\n\n## Example\nBefore: inline format! calls in multiple files.\nAfter: url helpers used consistently.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:51:50.897513381Z","created_by":"jes","updated_at":"2026-01-10T09:51:50.897513381Z"}
{"id":"CP-4w00","title":"Spawn SSE tasks dynamically for newly discovered node-backed subdirs","description":"Currently subdir_sse_task is only spawned at startup for known node-backed subdirectories. When handle_schema_change discovers a new node-backed subdir via SSE event, it creates local directories and files but does NOT spawn a new SSE task to watch that subdir's schema.\n\nThis means changes to newly discovered subdirs won't propagate until the next sync restart.\n\nThe workaround is the retry loop in sync_schema() (dir_sync.rs:2089-2118) which polls 3 times with 3s delays at startup hoping other sync clients will have pushed their schemas.\n\nFix: In handle_schema_change (or handle_subdir_schema_change), when a new node-backed subdirectory is discovered, spawn a subdir_sse_task for it dynamically.\n\nRelated: CP-75fq (UUID coordination issue)","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-09T06:19:36.922242004Z","created_by":"jes","updated_at":"2026-01-09T06:49:14.934707069Z","closed_at":"2026-01-09T06:49:14.934707069Z","close_reason":"Fixed by spawning SSE tasks dynamically when new node-backed subdirs are discovered via SSE events"}
{"id":"CP-4xd","title":"Directory-attached process management for orchestrator","description":"Similar to the existing file-attached process mechanism, add support for declaring that a process is running 'in a directory' that the orchestrator manages. This will be the primary paradigm for the sync-sandbox tool.\n\nUnlike file-attached processes which are tied to a specific document, directory-attached processes operate on a directory scope and can interact with multiple files within that directory tree.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-31T06:47:45.584747427Z","created_by":"jes","updated_at":"2025-12-31T16:27:44.602430837Z","closed_at":"2025-12-31T16:27:44.602430837Z","close_reason":"Merged in PR #55","comments":[{"id":6,"issue_id":"CP-4xd","author":"jes","text":"Implementation complete. Changes:\n- Made owns field optional in DiscoveredProcess (absent = directory-attached)\n- Added COMMONPLACE_SERVER env var to spawned processes\n- Added resolve_server_url() to OrchestratorConfig with fallback chain\n- Added COMMONPLACE_PATH env var support in sync.rs as alias for --node\n- Updated examples/.processes.json with directory-attached example\n\nCommits: 4d7963a, 0c5f293, 06b2975, 2e7d89b, 81574fa, 43367f8, 866e4b4, d7f1549","created_at":"2025-12-31T08:32:26Z"}]}
{"id":"CP-51t","title":"Verify diff module returns correct summary struct for replace endpoint","design":"The /nodes/:id/replace endpoint needs diff_to_yjs_update to return (update_bytes, DiffSummary) where DiffSummary has inserted, deleted, operations fields. Need to verify this interface exists or add it.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-29T08:03:48.888669529Z","created_by":"jes","updated_at":"2025-12-29T09:01:47.131481523Z","closed_at":"2025-12-29T09:01:47.131481523Z","close_reason":"Verified: diff module returns DiffResult with update_bytes, update_b64, operation_count, and summary (DiffSummary with chars_inserted, chars_deleted). replace_node endpoint correctly uses all fields."}
{"id":"CP-55s","title":"Blocks acceptance: commonplace-link creates schema but file doesn't sync","description":"## Summary\ncommonplace-link creates the local schema entry for the target file, but:\n1. The file doesn't get materialized on disk from the source content\n2. The schema change doesn't push to server\n3. Edits to the manually-created target don't sync back\n\n## Related\nMay be same as CP-a31 'commonplace-link doesn't push schema changes to server'\n\n## Repro\n1. Create source file with content\n2. Run commonplace-link source target\n3. Wait for sync\n4. Check if target exists (it doesn't)\n5. Manually create target with content\n6. Check if source gets updated (it doesn't)\n\n## Expected\nTarget file should be created with source content, and edits should sync bidirectionally.\n\n## Blocks\nAcceptance criteria L4, L5, L6, L7","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T02:23:46.200448694Z","created_by":"jes","updated_at":"2026-01-03T03:12:26.196893938Z","closed_at":"2026-01-03T03:12:26.196893938Z","close_reason":"Fixed commonplace-link to push schema changes to server. Added server push capability with --server flag (defaults to localhost:3000). For root schemas, discovers fs-root ID from server. For subdirectory schemas, uses node_id from parent schema. P2 edge case remaining for repeated directory names."}
{"id":"CP-56w","title":"Refactor api.rs: Separate path-based vs ID-based routing","description":"api.rs has mixed concerns for /docs/{id} and /files/{path} routing. Extract path-based API (`/files/*`) into separate module to clarify the two API styles.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-30T00:34:51.228276-08:00","updated_at":"2025-12-30T16:31:46.635504385Z","closed_at":"2025-12-30T16:31:46.635504385Z","close_reason":"Closed","dependencies":[{"issue_id":"CP-56w","depends_on_id":"CP-ejh","type":"blocks","created_at":"2025-12-30T00:36:37.094635-08:00","created_by":"daemon"}]}
{"id":"CP-59h","title":"Define and implement websocket protocol for realtime sync","description":"Summary: WebSocket endpoint with subprotocol negotiation for y-websocket compatibility and commonplace-extended mode.\n\n## Endpoint\n\n```\nws://localhost:3000/ws/docs/{id}\n```\n\n## Subprotocol Negotiation\n\nClient sends `Sec-WebSocket-Protocol` header to select mode:\n\n| Subprotocol | Description |\n|-------------|-------------|\n| `y-websocket` | Pure Yjs compatibility for browser tools (Tiptap, Monaco, etc.) |\n| `commonplace` | Extended protocol with commit metadata and blue/red ports |\n\n## Message Types\n\n### y-websocket mode (types 0-1)\n- 0: Sync messages (sync step 1, sync step 2, update)\n- 1: Awareness messages (cursor positions, presence)\n\n### commonplace mode (types 0-5)\n- 0-1: Same as y-websocket\n- 3: Commit metadata (parent cid, timestamp)\n- 4: Blue port events (commit notifications)\n- 5: Red port events (ephemeral JSON)\n\n## Implementation Steps\n\n1. Add websocket endpoint using axum's WebSocket support\n2. Implement subprotocol negotiation via Sec-WebSocket-Protocol header\n3. Implement y-websocket mode first (milestone: browser Yjs tools work)\n4. Add commonplace mode extensions\n5. Add reconnection handling and backpressure\n6. Tests for both modes\n\n## Use Cases\n\n- Browser-based collaborative editing (Tiptap, ProseMirror)\n- Compatibility with existing Yjs ecosystem tools\n- Real-time sync without polling","notes":"Phase 1 (y-websocket core) committed: 4fdbd67. Endpoint /ws/docs/{id} with subprotocol negotiation, sync protocol, room coordination. Phase 2 (commonplace extensions) pending.","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-03T19:19:07.492551245Z","created_by":"jes","updated_at":"2026-01-04T02:02:18.545827145Z","closed_at":"2026-01-04T02:02:18.545827145Z","close_reason":"Phase 1 (y-websocket core) complete. Follow-on work in CP-9abt, CP-ow96, CP-w5sw."}
{"id":"CP-5af","title":"Telegram messages ↔ file integration","description":"Bidirectional sync between Telegram messages and commonplace documents:\n- Receive Telegram messages as document updates or events\n- Send document content/edits to Telegram chats\n- Subscribe to specific chats/channels\n- Bot or user account authentication\n\nEnables messaging-as-document patterns and chat automation.","design":"OPEN QUESTIONS for implementation:\n1. Bot token or user session auth?\n2. What's the document schema for messages?\n3. Handle message history or just new messages?\n4. Support for media (photos, files) or text only?\n5. Rate limiting and Telegram API compliance?\n6. How to map chats to document paths?","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T23:53:18.825068-08:00","updated_at":"2026-01-04T01:03:22.052424894Z","closed_at":"2026-01-04T01:03:22.052424894Z","close_reason":"Done","labels":["future-work"],"dependencies":[{"issue_id":"CP-5af","depends_on_id":"CP-nno","type":"blocks","created_at":"2025-12-28T23:53:18.82599-08:00","created_by":"daemon"}]}
{"id":"CP-5et6","title":"Server replace_content should treat JSONL as text, not JSON","description":"Summary: The server's replace_content needs to parse incoming JSONL text into Y.Array\u003cY.Map\u003e format for proper Yjs diff.\n\nCurrent: JSONL documents use Y.Array\u003cY.Map\u003e, but replace receives raw JSONL text and fails to parse it.\n\nFix: In replace_content, when content_type is JSONL:\n1. Parse incoming text as newline-delimited JSON\n2. Convert each line to a Y.Map entry\n3. Diff against existing Y.Array\u003cY.Map\u003e state\n\nFiles to modify:\n- src/services/document.rs (replace_content)\n- src/sync/yjs.rs (may need create_yjs_jsonl_diff_update or similar)","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T04:26:24.852577094Z","created_by":"jes","updated_at":"2026-01-05T05:52:44.409864068Z","closed_at":"2026-01-05T05:52:44.409864068Z","close_reason":"Fixed: compute_json_diff now uses create_yjs_jsonl_update for JSONL content type"}
{"id":"CP-5i1","title":"HTML + WebSocket server process","description":"Server process that serves static HTML viewer and provides WebSocket connections for live document streaming. Powers the live HTML viewer (CP-6oj).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T18:00:20.018424314Z","created_by":"jes","updated_at":"2026-01-04T05:25:41.914326057Z","closed_at":"2026-01-04T05:25:41.915427847Z","labels":["future-work"]}
{"id":"CP-5mag","title":"Deduplicate path normalization helpers","description":"## Summary\nPath normalization helpers are duplicated across CLI binaries (link/replay/uuid) and sync modules (sync/urls.rs and sync/directory.rs). Consolidate to shared helpers to avoid inconsistent behavior.\n\n## Files to modify\n- src/workspace.rs (reuse normalize_path for CLI)\n- src/bin/link.rs\n- src/bin/replay.rs\n- src/bin/uuid.rs\n- src/sync/directory.rs (use sync::urls::normalize_path)\n\n## Implementation steps\n1. Replace local normalize_path functions in CLI binaries with workspace::normalize_path.\n2. Replace sync/directory.rs normalize_path with sync::urls::normalize_path.\n3. Add tests if needed to ensure consistent slash handling on Windows paths.\n\n## Example\nBefore: multiple normalize_path implementations.\nAfter: CLI uses workspace::normalize_path; sync uses urls::normalize_path.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:32:50.704561337Z","created_by":"jes","updated_at":"2026-01-10T09:32:50.704561337Z"}
{"id":"CP-5p5","title":"Investigate B-\u003eA sync not working: file watcher or echo detection issue","design":"Root cause found: Atomic writes break file watcher.\n\nWhen handle_server_edit writes to file, it uses atomic write (temp + rename). On Linux with inotify, this can cause the watcher to lose track of the file because:\n1. Watcher watches original inode\n2. Rename replaces file with new inode\n3. Watcher still watching old (deleted) inode\n4. Subsequent edits to new file not detected\n\nPotential fixes:\n1. Watch parent directory instead of file\n2. Re-register file watcher after each write\n3. Don't use atomic writes (risk: partial writes on crash)\n4. Use inotify IN_MOVED_TO event to re-watch\n\nNeeds implementation to test which approach works best.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-29T08:18:50.200861401Z","created_by":"jes","updated_at":"2025-12-29T16:02:38.701416395Z","closed_at":"2025-12-29T16:02:38.701416395Z","close_reason":"Fixed inotify file watcher issue by using direct writes instead of atomic (temp+rename). Testing revealed a separate race condition where upload_task can send stale content before server edits are written - needs follow-up fix. PR #21 merged."}
{"id":"CP-5pcq","title":"CI integration: commonplace-link schema push updates server","description":"Summary: Add a CI integration test that verifies commonplace-link pushes schema changes to the server so linked entries propagate to other clients.\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/commonplace-link-test.sh)\n- .github/workflows/ci.yml\n- src/bin/link.rs (only if test reveals missing flags/options)\n\nImplementation steps:\n1) Start server and sync with a temp workspace.\n2) Use commonplacelink to link a file (modify .commonplace.json) and ensure it triggers a schema push.\n3) Fetch server schema and assert the linked entry is present with expected node_id.\n4) Optionally start a second sync client and assert the linked file appears.\n5) Run in CI to prevent regressions.\n\nExample:\nBefore: commonplacelink updates .commonplace.json but server schema stays unchanged.\nAfter: server schema includes the new entry and other clients can see it.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T23:16:47.855563322Z","created_by":"jes","updated_at":"2026-01-10T23:17:05.071977497Z","comments":[{"id":50,"issue_id":"CP-5pcq","author":"jes","text":"Covers acceptance criteria L1-L7 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nL1: Create two files in different directories\nL2: Run commonplace-link to share UUID\nL3: Verify both files have same UUID in .commonplace.json\nL4: Verify target file now contains source content\nL5-L6: Edit linked file, verify change propagates\nL7: Verify linked file appears in sandbox with correct content","created_at":"2026-01-11T00:41:37Z"}]}
{"id":"CP-5qb","title":"Sandbox file linking not working - sync creates forks instead of using linked UUIDs","description":"When setting up linked files between sandbox processes (per SANDBOX_LINKING.md), the sync is creating new forks instead of using the shared UUIDs.\n\n**Expected behavior:**\n- text-to-telegram/content.txt and bartleby/prompts.txt share the same UUID\n- Changes to one file sync to the other\n\n**Actual behavior:**\n- Sync detects 'new file has identical content' and FORKS a new UUID\n- Each sandbox ends up with different UUIDs for the linked files\n- No sync happens between them\n\n**Root cause:**\n- Node-backed directories (entries: null, node_id: X) store schemas separately\n- Local schema edits aren't pushed to the server's subdirectory nodes\n- When sandbox syncs pull schemas, they get wrong/missing UUIDs\n\n**To fix:**\n1. Need to push linked schemas directly to subdirectory node documents\n2. Or change sync to preserve UUIDs from local schema when pushing\n3. Update SANDBOX_LINKING.md with correct procedure for node-backed directories","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T19:29:39.76376046Z","created_by":"jes","updated_at":"2026-01-01T19:37:40.407375761Z","closed_at":"2026-01-01T19:37:40.407375761Z","close_reason":"Fixed by adding path option to processes.json. Each sandbox now syncs from its own subdirectory rather than root, allowing linked UUIDs to work correctly."}
{"id":"CP-5qy","title":"Orchestrator should restart sandbox-exec processes that die","description":"When a sandbox-exec process exits (crashes or otherwise dies), the orchestrator should restart it automatically.\n\nCurrently the orchestrator has check_and_restart() logic but it may not be working correctly for sandbox-exec processes. The sync wrapper process may be running but the actual child process (e.g., bartleby, text-to-telegram) may have died.\n\nNeed to ensure the entire process tree is monitored and restarted on failure.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T19:21:34.651720793Z","created_by":"jes","updated_at":"2026-01-03T07:19:25.650620312Z","closed_at":"2026-01-03T07:19:25.650620312Z","close_reason":"Verified working - sandbox-exec processes restart automatically when killed (tested with bartleby multiple times)"}
{"id":"CP-5tk6","title":"Deduplicate orchestrator process termination logic","description":"## Summary\nProcess termination logic (SIGTERM/SIGKILL process-group handling with timeout) is duplicated across ProcessManager and DiscoveredProcessManager. Extract a shared helper to ensure consistent shutdown behavior and reduce drift.\n\n## Files to modify\n- src/orchestrator/manager.rs (use shared termination helper)\n- src/orchestrator/discovered_manager.rs (use shared termination helper)\n- src/orchestrator/process_utils.rs (new helper for killpg/timeout)\n\n## Implementation steps\n1. Extract a helper that takes a Child handle and performs SIGTERM, wait timeout, then SIGKILL fallback.\n2. Use it in both managers where processes are stopped/removed.\n3. Keep logging consistent and preserve existing timeouts.\n\n## Example\nBefore: repeated killpg + timeout blocks in multiple methods.\nAfter: stop_process(child, name, log_prefix) handles termination everywhere.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:25:22.903761472Z","created_by":"jes","updated_at":"2026-01-10T10:04:48.260562309Z","closed_at":"2026-01-10T10:04:48.260562309Z","close_reason":"Added stop_process_gracefully() helper to process_utils module. Updated ProcessManager (stop_process, shutdown) and DiscoveredProcessManager (remove_process, restart_process, shutdown) to use shared termination logic."}
{"id":"CP-5vk","title":"Blocks acceptance: New files in subdirectories appear at wrong path in sandbox","description":"## Summary\nFiles created in workspace subdirectories appear at the wrong path in sandboxes.\n\n## Repro\n1. Create `workspace/bartleby/test-note.txt` with content 'note'\n2. Check bartleby sandbox\n\n## Expected\nFile appears at `bartleby/test-note.txt` in sandbox\n\n## Actual\nFile appears at root `test-note.txt` in sandbox (wrong path)\n\nAdditionally, the file content is empty (0 bytes) instead of 'note'.\n\n## Log evidence\n```\n[bartleby] Server created new file: test-note.txt -\u003e /tmp/commonplace-sandbox-.../test-note.txt\n```\n\nThe path should be `bartleby/test-note.txt` not `test-note.txt`.\n\n## Blocks\nAcceptance criteria C5, C6","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T02:14:13.431123317Z","created_by":"jes","updated_at":"2026-01-03T02:17:02.39902252Z","closed_at":"2026-01-03T02:17:02.399030281Z"}
{"id":"CP-5we","title":"Beads integration with bidirectional sync","description":"Sync beads JSONL into commonplace as documents. Edits made via commonplace should publish back to beads, enabling bidirectional issue tracking integration.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:56:53.320326707Z","created_by":"jes","updated_at":"2026-01-05T08:44:43.083181209Z","closed_at":"2026-01-05T08:44:43.083181209Z","close_reason":"Working: beads JSONL syncs via commonplace-sync, cbd reads/writes through server, bidirectional flow complete","labels":["future-work"],"dependencies":[{"issue_id":"CP-5we","depends_on_id":"CP-oi3","type":"blocks","created_at":"2025-12-30T18:43:29.742508518Z","created_by":"daemon"}]}
{"id":"CP-5wwk","title":"tracker.shadow() return value ignored - silent failure","description":"In sse.rs:817, tracker.shadow(old_key) is called but its return value (Option\u003cPathBuf\u003e) is ignored.\n\nRace condition: If concurrent GC removes the inode state between the read lock (line 793) and write lock (line 817), shadow() returns None and the shadow hardlink is never tracked - but we just created the hardlink at line 809.\n\nResult: Orphaned shadow hardlink that won't be GC'd and won't have its writes forwarded.\n\nFix: Check return value of shadow() and either:\n1. Log warning if it returns None unexpectedly\n2. Delete the hardlink we just created if tracking failed\n\nRelated: CP-txg0 review","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T08:57:20.721396968Z","created_by":"jes","updated_at":"2026-01-05T23:50:36.65853067Z","closed_at":"2026-01-05T23:50:36.65853067Z","close_reason":"Fixed: now checks shadow() return value and removes orphaned hardlink if tracking failed"}
{"id":"CP-5y0","title":"Sync doesn't detect offline edits on restart","description":"When the sync client is stopped, files are edited, and sync is restarted, the offline edits are NOT synced to the server until a new modification is made.\n\nTest case (O5):\n1. Stop workspace sync\n2. Edit file while sync is down\n3. Restart workspace sync\n4. Server content is NOT updated\n\nExpected: Sync should detect file changes made while it was down and push them on startup.\nActual: File appears 'in sync' until a new modification is made.\n\nLikely cause: Sync client caches last known content/hash and only checks on file change events, not at startup.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T09:49:00.716518671Z","created_by":"jes","updated_at":"2026-01-02T10:57:10.858702165Z","closed_at":"2026-01-02T10:57:10.858702165Z","close_reason":"Closed"}
{"id":"CP-62e1","title":"Server stack overflow on file-tmux-file sync","description":"Server crashes with stack overflow when file-tmux-file syncs tmux pane content. Happens on small (112 byte) edits after larger syncs. Workaround: disable file-tmux-file by emptying workspace/tmux/__processes.json","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-10T20:44:16.490740839Z","created_by":"jes","updated_at":"2026-01-10T21:46:00.632070459Z","closed_at":"2026-01-10T21:46:00.632070459Z","close_reason":"Fixed: stack overflow with 8MB thread stack + async_recursion, file deletion with fetch_subdir_node_id"}
{"id":"CP-6ke7","title":"Add MQTT topic for document forking","description":"HTTP has POST /docs/:id/fork to create a new document forked from an existing one at HEAD or a specific commit. No MQTT equivalent.\n\nAdd to sync protocol or commands port.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:20.607105809Z","created_by":"jes","updated_at":"2026-01-10T10:19:31.702681214Z","comments":[{"id":30,"issue_id":"CP-6ke7","author":"jes","text":"Needs refactor: CommandsHandler only has DocumentStore but fork_document requires DocumentService (needs commit_store for state replay). Must update CommandsHandler to accept Arc\u003cDocumentService\u003e. Deferring to avoid scope creep in wiggum loop.","created_at":"2026-01-10T10:19:43Z"}]}
{"id":"CP-6kk","title":"Remove non-recursive mode from orchestrator","description":"Summary: Remove the non-recursive orchestrator mode so it always scans recursively and watches all processes.json files.\n\nFiles to modify:\n- src/bin/orchestrator.rs (CLI flags and default behavior)\n- src/orchestrator (remove non-recursive code paths)\n- docs/DEVELOPMENT.md or README.md (update usage)\n\nImplementation steps:\n1. Remove the non-recursive flag(s) and associated code paths.\n2. Ensure recursive scanning is always enabled.\n3. Update docs/CLI help to reflect the simplified behavior.\n\nExample:\nBefore: orchestrator --watch-processes (non-recursive)\nAfter: orchestrator always uses recursive mode","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T18:11:15.095687874Z","created_by":"jes","updated_at":"2026-01-03T18:57:19.117322889Z","closed_at":"2026-01-03T18:57:19.117322889Z","close_reason":"Removed non-recursive mode. Orchestrator now always starts server+sync from commonplace.json, then recursively discovers processes. Removed --recursive, --watch-processes, and --use-paths flags."}
{"id":"CP-6kpq","title":"Add .cbd.json config file for repo-to-commonplace path mapping","description":"cbd should discover .cbd.json in repo root (or .beads/) that specifies the commonplace path to the synced issues.jsonl file. Enables cbd to work in any repo without hardcoded paths.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T06:43:34.197110755Z","created_by":"jes","updated_at":"2026-01-05T06:49:51.248409509Z","closed_at":"2026-01-05T06:49:51.248409509Z","close_reason":"Added .cbd.json discovery to cbd - walks up directories looking for .cbd.json or .beads/.cbd.json"}
{"id":"CP-6ngz","title":"P2: Config reload leaves stale process settings in manager","description":"In manager.rs around lines 486-533, reload_config() compares old vs new config but doesn't update internal state for processes that changed but weren't restarted. The ProcessInfo stored in self.processes may have stale configuration.\n\nLocation: src/orchestrator/manager.rs:486-533\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:09:52.679464435Z","created_by":"jes","updated_at":"2026-01-06T01:19:41.791591756Z","closed_at":"2026-01-06T01:19:41.791591756Z","close_reason":"Update unchanged process configs in reload_config"}
{"id":"CP-6o2m","title":"Add MQTT path-based operations","description":"HTTP has /files/*path endpoints that resolve filesystem paths to document IDs. No MQTT equivalent.\n\nOptions:\n- Path resolution service on a system topic\n- Or extend topic format to support path-based addressing\n\nCurrently MQTT topics require knowing the document path with extension upfront.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:24.604689681Z","created_by":"jes","updated_at":"2026-01-09T07:05:24.604689681Z"}
{"id":"CP-6oj","title":"Live HTML viewer with websocket updates","description":"Browser-based viewer that connects via websocket to display commonplace documents in real-time. XHTML documents render as HTML, JSON and XML documents display formatted source. Updates stream live as documents change.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:59:51.301579711Z","created_by":"jes","updated_at":"2026-01-04T09:28:35.297677806Z","closed_at":"2026-01-04T09:28:35.297684906Z","labels":["future-work"],"dependencies":[{"issue_id":"CP-6oj","depends_on_id":"CP-5i1","type":"blocks","created_at":"2025-12-30T18:00:25.637694596Z","created_by":"daemon"}]}
{"id":"CP-6pd","title":"Use cwd to run processes in sandbox directory instead of symlinks/env vars","description":"When conductor/orchestrator spawns processes for directory-attached work, set the working directory (cwd) to the sandbox/workspace directory instead of relying on symlinks or environment variables like BARTLEBY_WORKING_DIR. This allows programs that use relative paths to naturally see the sandbox contents without any special configuration. Simpler and more transparent than the symlink approach.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-31T17:04:16.788917689Z","created_by":"jes","updated_at":"2025-12-31T19:55:32.624440087Z","closed_at":"2025-12-31T19:55:32.624440087Z","close_reason":"Implemented in PR #58. Bartleby now uses Path.cwd() instead of BARTLEBY_WORKING_DIR env var, and orchestrator sets cwd to workspace directory."}
{"id":"CP-6uwd","title":"Deduplicate HTTP error mapping","description":"## Summary\nHTTP error mapping for ServiceError/PathResolveError is implemented separately in api.rs and files.rs. Consolidate error-\u003eStatusCode and IntoResponse handling into shared helpers.\n\n## Files to modify\n- src/api.rs\n- src/files.rs\n- src/http/errors.rs (new shared module)\n\n## Implementation steps\n1. Move ServiceError status mapping and PathResolveError mapping into shared functions.\n2. Update api.rs and files.rs to use shared IntoResponse helpers.\n3. Keep status codes and body formats identical.\n\n## Example\nBefore: api.rs and files.rs each map errors to StatusCode separately.\nAfter: http::errors::to_response(error) used in both.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:01:07.239859851Z","created_by":"jes","updated_at":"2026-01-10T12:11:25.436209897Z","comments":[{"id":33,"issue_id":"CP-6uwd","author":"jes","text":"After investigation: These are different error types (ServiceError, PathResolveError, FileError), not duplicated impls. No simple deduplication opportunity - would require adding axum to services layer.","created_at":"2026-01-10T12:11:25Z"}]}
{"id":"CP-6ux9","title":"Empty UUID map guard incorrectly skips deletion for legitimately empty subdirectories","design":"When a node-backed subdirectory becomes empty on the server (all files deleted), build_uuid_map_recursive returns an empty map. The current safety check in handle_subdir_schema_cleanup treats this as a fetch failure and skips deletions, so stale local files remain. Need to distinguish true fetch errors from legitimately empty schemas.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T22:42:32.229756487Z","created_by":"jes","updated_at":"2026-01-06T23:46:09.52226168Z","closed_at":"2026-01-06T23:46:09.52226168Z","close_reason":"Fixed empty UUID map guard to properly distinguish legitimate empty schemas from fetch failures. Added status tracking to build_uuid_map_recursive, guards for fetch failures, schema entries check, and node-backed directory support. Merged in PR #106."}
{"id":"CP-75fq","title":"UUID coordination between sync clients is broken","description":"When multiple sync clients (sandbox, workspace) create schemas independently, they generate different UUIDs for the same logical paths. After a database restart, UUIDs diverge completely:\n- Sandbox generates UUID A for content.txt\n- Workspace has stale UUID B for content.txt  \n- Server may have empty schemas or yet another UUID\n\nRoot cause: No single source of truth for UUIDs. Each sync client generates UUIDs locally when creating new schema entries.\n\nImpact: File content doesn't sync between sandboxes and workspace because they're syncing to different server documents.\n\nProposed fix: Server should be the source of truth. When a sync client creates a new entry, it should:\n1. Push schema to server with null node_id\n2. Server's reconciler generates the UUID\n3. Sync client pulls the UUID back and updates local schema\n\nThis ensures all clients see the same UUIDs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-09T05:46:42.591775384Z","created_by":"jes","updated_at":"2026-01-09T18:52:32.800073019Z","closed_at":"2026-01-09T18:52:32.800073019Z","close_reason":"Fixed by making server the source of truth for UUIDs. Sync clients now push schemas with None node_ids, server generates UUIDs, and clients fetch back server-assigned UUIDs."}
{"id":"CP-78a","title":"Blocks acceptance: CRDT merge corruption on file edit","description":"## Summary\nFile content becomes corrupted after an edit, indicating CRDT merge failure.\n\n## Repro\n1. Create file `workspace/bartleby/test-note.txt` with content 'note'\n2. Wait for sync\n3. Edit file to contain 'updated note'\n4. Check sandbox\n\n## Expected\nContent: 'updated note'\n\n## Actual\nContent: 'updated updaed note' (corrupted merge of old and new content)\n\n## Analysis\nThe CRDT appears to be merging the old content 'note' with the new 'updated note' incorrectly, resulting in 'updated updaed note'.\n\n## Blocks\nAcceptance criterion E6","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T02:18:08.885726507Z","created_by":"jes","updated_at":"2026-01-03T03:45:07.862314284Z","closed_at":"2026-01-03T03:45:07.862314284Z","close_reason":"Unable to reproduce after multiple attempts. Comment from jes: 'Multiple tests with file creation and editing showed correct CRDT behavior. May have been a transient race condition.' Closing as unreproducible. If issue recurs, reopen with specific reproduction steps.","comments":[{"id":21,"issue_id":"CP-78a","author":"jes","text":"Unable to reproduce. Multiple tests with file creation and editing in text-to-telegram subdirectory showed correct CRDT behavior. 'note' to 'updated note' worked correctly. Rapid edits also converged correctly. May have been a transient race condition.","created_at":"2026-01-03T02:53:03Z"}]}
{"id":"CP-7cjh","title":"Reconciler creates child documents but doesn't update parent schema","description":"When the server's fs reconciler creates new subdirectories (e.g., tmux windows), it creates the child document nodes but fails to add them to the parent directory's schema. This causes other sync clients to never see the new directories.\n\n**Reproduction:**\n1. file-tmux-file creates new tmux window files in its sandbox\n2. Reconciler logs 'Migrated inline subdirectory' and 'Reconciler created document'\n3. But parent schema (e.g., tmux/0/) never gets the new entry\n4. Other syncs (bartleby, workspace) don't see the new window\n\n**Evidence:**\n- Server tmux/0 schema has 7 entries\n- Workspace tmux/0 directory has 10 entries (0-js-evaluator, 2-_tmux_, 4-jes missing from server)\n- Reconciler logs show documents were created but parent schema unchanged\n\n**Expected:** Parent schema should be updated when reconciler creates child documents.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-06T20:03:41.198249125Z","created_by":"jes","updated_at":"2026-01-06T20:45:50.231329326Z","closed_at":"2026-01-06T20:45:50.231329326Z","close_reason":"Added create_yjs_json_merge for additive schema syncs. Merged in PR #103."}
{"id":"CP-7el5","title":"Add --push-only and --pull-only flags to commonplace-sync","description":"Add directional sync modes to commonplace-sync:\n\n**--push-only**: \n- Watches local files for changes and pushes to server\n- Ignores server-side updates (no SSE subscription or pull)\n- Use case: source-of-truth files like .beads/issues.jsonl\n\n**--pull-only**:\n- Subscribes to server updates and writes to local files\n- Ignores local file changes (no file watcher)\n- Use case: read-only mirrors, generated content\n\nCurrent behavior (bidirectional) remains the default.\n\nImplementation:\n- Skip SSE subscription when --push-only\n- Skip file watcher setup when --pull-only\n- Validate flags are mutually exclusive","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T20:00:46.196501081Z","created_by":"jes","updated_at":"2026-01-03T23:04:24.043330229Z","closed_at":"2026-01-03T23:04:24.043330229Z","close_reason":"Implemented --push-only and --pull-only flags for commonplace-sync"}
{"id":"CP-7gx","title":"Sync --use-paths should work without --node when server has fs-root","description":"Currently sync client with --use-paths still requires --node. When using path-based endpoints (/files/*path), the sync should be able to work without specifying a node ID since paths are resolved on the server.\n\nWorkaround: Query /fs-root endpoint (CP-81o) to get the node ID.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-31T23:59:59.108226549Z","created_by":"jes","updated_at":"2026-01-01T11:33:34.815949Z","closed_at":"2026-01-01T11:33:34.815949Z","close_reason":"Fixed in PR #75. Sync with --use-paths can now auto-discover fs-root from server.","comments":[{"id":13,"issue_id":"CP-7gx","author":"jes","text":"Dependency: This requires CP-81o (GET /fs-root endpoint) to be implemented first. The sync client needs to be able to discover the fs-root document ID from the server before it can work without --node.\n\nWill implement CP-81o first, then come back to this.","created_at":"2026-01-01T11:21:19Z"}]}
{"id":"CP-7h3","title":"commonplace-ps should show sandbox child process info, not wrapper","description":"Currently commonplace-ps shows the commonplace-sync wrapper process PID and CWD for sandbox-exec processes. This is misleading because:\n\n- The CWD shows where the orchestrator started (e.g., /home/jes/commonplace)\n- The actual sandboxed app runs as a child process in /tmp/commonplace-sandbox-*\n\nExample current output:\n```\nbartleby  818853 Running  /home/jes/commonplace\n```\n\nBut the actual bartleby process (PID 818883) runs in /tmp/commonplace-sandbox-be791041-...\n\nOptions:\n1. Show child process PID and CWD instead of wrapper\n2. Show both (wrapper PID + child CWD)\n3. Add a column for 'sandbox path' when applicable","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T12:28:21.602293227Z","created_by":"jes","updated_at":"2026-01-03T19:14:44.953532069Z","closed_at":"2026-01-03T19:14:44.953532069Z","close_reason":"Fixed. Now reads child process CWD from /proc/\u003cchild_pid\u003e/cwd to show the actual sandbox directory instead of the wrapper's CWD."}
{"id":"CP-7m0","title":"Flat directory JSON with subdirectory documents","description":"Directory JSON should only contain one level of files. Subdirectories need to be their own separate JSON documents. This enables partial checkouts of large directory trees.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:47:47.262049538Z","created_by":"jes","updated_at":"2025-12-30T21:56:23.60592734Z","closed_at":"2025-12-30T21:56:23.60592734Z","close_reason":"Closed"}
{"id":"CP-7n2","title":"Refactor sync.rs: Extract file watcher logic","description":"Extract file watcher setup and event handling from sync.rs into dedicated module `src/sync/watcher.rs`. This handles notify events, debouncing, and change detection.","status":"closed","priority":1,"issue_type":"chore","created_at":"2025-12-30T00:34:50.932224-08:00","updated_at":"2025-12-30T15:26:25.522185703Z","closed_at":"2025-12-30T15:26:25.522185703Z","close_reason":"Merged in PR #39","dependencies":[{"issue_id":"CP-7n2","depends_on_id":"CP-jwf","type":"blocks","created_at":"2025-12-30T00:36:37.008385-08:00","created_by":"daemon"}]}
{"id":"CP-7ne","title":"Writes to JSONL files are disappearing","description":"Summary: JSONL file writes intermittently vanish; appended lines are missing after a short period, suggesting a sync/atomic-write issue or competing writers.\n\nFiles to modify:\n- src/bin/sync.rs (local file event handling and writeback)\n- src/sync/* (file watcher, coalescing, conflict resolution)\n- any JSONL writers (search for .jsonl writes)\n\nImplementation steps:\n1. Identify affected JSONL files and reproduce (append line, wait, verify line missing).\n2. Check for multiple writers or sandbox processes touching the same JSONL file.\n3. Inspect sync logs around the disappearance for overwrite or rollback events.\n4. Verify whether atomic write behavior or rename-based updates are being misinterpreted as deletes.\n5. Add a regression test that appends to a concrete JSONL file and confirms persistence through sync cycles.\n\nExample:\nBefore: Append a line to a JSONL file (e.g., history/events.jsonl), then wait ~N seconds.\nAfter: New line remains present unless explicitly removed.","notes":"Root cause likely: JSONL files are treated as plain text updates. upload_task only routes application/json to push_json_content; application/x-ndjson uses push_file_content (text), which sends Y.Text updates. On server, JSONL expects Y.Array; mismatched updates can serialize to empty or per-character JSONL. Add JSONL-aware upload path using create_yjs_jsonl_update and route jsonl/ndjson content types to it.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T00:00:47.541630379Z","created_by":"jes","updated_at":"2026-01-03T08:59:59.485881086Z","closed_at":"2026-01-03T08:59:59.485881086Z","close_reason":"Fixed in commit 3539e38 - JSONL uploads now use Y.Array updates instead of treating them as plain text, preventing data loss."}
{"id":"CP-7npd","title":"Deduplicate ps output helpers","description":"## Summary\nProcess status output in bin/ps.rs has local helpers (timestamp formatting, process-running check) that duplicate utilities used elsewhere. Consolidate into shared helpers to keep formatting consistent and avoid drift.\n\n## Files to modify\n- src/bin/ps.rs\n- src/workspace.rs (reuse format_timestamp) or src/orchestrator/status.rs (new helper)\n- src/orchestrator/process_utils.rs (add is_process_running helper)\n\n## Implementation steps\n1. Replace local format_timestamp with workspace::format_timestamp.\n2. Move is_process_running into a shared utility and import it in ps.rs.\n3. Ensure output format remains the same.\n\n## Example\nBefore: ps.rs defines format_timestamp and is_process_running.\nAfter: ps.rs uses shared helpers.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:50:55.741563317Z","created_by":"jes","updated_at":"2026-01-10T09:50:55.741563317Z"}
{"id":"CP-7w41","title":"Deduplicate schema fetch/validate/write logic in dir_sync","description":"## Summary\nSchema fetch/parse/validate/write logic is repeated in multiple dir sync paths (root schema change, subdir cleanup, subdir sync). Consolidate into shared helpers to ensure consistent validation and reduce drift.\n\n## Files to modify\n- src/sync/dir_sync.rs (refactor handle_schema_change / handle_subdir_schema_cleanup / subdir sync helpers)\n- src/sync/schema_io.rs (optional: move shared validation + write helpers here)\n\n## Implementation steps\n1. Identify duplicated sequences: fetch head, parse FsSchema, validate root entries, write .commonplace.json, early-return on invalid content.\n2. Extract a helper (e.g., fetch_and_parse_schema or apply_schema_head) that returns (schema, content) and handles validation.\n3. Use the helper in root and subdir schema handlers to avoid divergent behavior.\n4. Add a small test or logging assertion to ensure invalid/empty schema content is consistently skipped.\n\n## Example\nBefore: handle_schema_change and handle_subdir_schema_cleanup each parse and validate separately.\nAfter: both call apply_schema_head(...) and share the same validity rules.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T09:04:18.81421055Z","created_by":"jes","updated_at":"2026-01-10T09:31:32.438135059Z","closed_at":"2026-01-10T09:31:32.438135059Z","close_reason":"Added fetch_and_validate_schema helper in schema_io.rs. Updated 3 callers in dir_sync.rs to use the shared helper, reducing duplication by ~85 lines."}
{"id":"CP-80u","title":"Fix P2: Skip wiring when an identical edge already exists (PR #8)","description":"From PR #8 Codex review: The router creates wires based on managed_wires, but if a wiring already exists from an external source (e.g., /nodes/:from/wire/:to), this code will still call wire_* and create a second subscription. Consider checking existing registry wirings before adding.","design":"Added `find_existing_wiring(from, to, port)` method to NodeRegistry. Router manager now checks for existing wires before creating new ones and adopts them into managed_wires if found. Known limitation: Adopted wires skip the router's cycle error emission, but this is safe because external wire creation already performs cycle detection at the registry level.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-26T23:24:33.186937-08:00","updated_at":"2025-12-27T00:35:45.522329-08:00","closed_at":"2025-12-27T00:35:45.522329-08:00","close_reason":"Skip duplicate wirings implemented. Added find_existing_wiring() to NodeRegistry. Router now checks for existing wires before creating new ones and skips duplicates without claiming ownership (prevents router interference). Merged in PR #10."}
{"id":"CP-80z2","title":"XML documents using XmlFragment causes /replace to crash","description":"## Root Cause\n\nXML documents use `XmlFragment` internally (via `get_or_insert_xml_fragment`) but the diff module (`src/diff.rs`) creates updates for `Y.Text` type. When applying Text updates to an XmlFragment document, the server crashes.\n\n## Affected Endpoints\n- POST /docs/{id}/replace\n- POST /files/{path}/replace\n\n## Workaround Applied\nChanged XML content type to use `Y.Text` instead of `Y.XmlFragment` in:\n- src/document.rs (create_document, get_or_create_with_id, apply_yjs_update)\n- src/replay.rs (get_content_and_state_at_commit)\n\n## Proper Fix\nEither:\n1. Keep XML using Text (current workaround) - simpler but loses semantic XML editing\n2. Update diff module to detect content type and use appropriate Yjs type (XmlFragment vs Text)\n\n## Testing\nTest in tests/api_tests.rs was updated to use Text-based updates for XML documents.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T05:29:13.957190244Z","created_by":"jes","updated_at":"2026-01-07T19:21:09.471261202Z","closed_at":"2026-01-07T19:21:09.471261202Z","close_reason":"Fixed by implementing compute_xml_diff_update() in src/diff.rs that generates XmlFragment-compatible updates. Also fixed replay.rs to handle XML content type with XmlFragment."}
{"id":"CP-81o","title":"Server should expose fs-root document ID at /fs-root endpoint","description":"When server is started with --fs-root, there's no way for clients to discover the fs-root document ID. Add GET /fs-root endpoint that returns the document ID.\n\nThis would simplify the sync workflow - clients could auto-discover the fs-root instead of needing to pass --node.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-31T23:59:51.564657714Z","created_by":"jes","updated_at":"2026-01-01T11:27:19.342457135Z","closed_at":"2026-01-01T11:27:19.342457135Z","close_reason":"Fixed in PR #74. Added GET /fs-root endpoint that returns the fs-root document ID."}
{"id":"CP-82tm","title":"Deduplicate /docs and /files handler logic","description":"## Summary\nAPI and files endpoints both implement doc info/head/edit/replace response types and logic. Consider a shared HTTP layer module to avoid drift between /docs and /files handlers.\n\n## Files to modify\n- src/api.rs\n- src/files.rs\n- src/http/ (new shared helpers)\n\n## Implementation steps\n1. Extract shared response/request types (already captured in CP-42ll/CP-usfc).\n2. Factor shared handler logic for head/edit/replace into reusable functions that accept a document ID.\n3. Update both /docs and /files handlers to use the shared functions.\n\n## Example\nBefore: /docs and /files each build DocHeadResponse and ReplaceResponse.\nAfter: http::handlers::get_head(doc_id) used by both.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:06:39.06130002Z","created_by":"jes","updated_at":"2026-01-10T10:06:39.06130002Z"}
{"id":"CP-8cd","title":"Make --recursive the default for orchestrator","description":"When running the orchestrator with a server that has --fs-root configured, --recursive mode should be the default behavior.\n\nCurrently requires explicit --recursive flag to discover all processes.json files in the filesystem tree. This should be the default since it's the most common use case.\n\nThe non-recursive mode (--watch-processes) can still be available for watching a specific document.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T19:21:01.334042203Z","created_by":"jes","updated_at":"2026-01-03T06:59:04.700263953Z","closed_at":"2026-01-03T06:59:04.700263953Z","close_reason":"Already implemented - --recursive defaults to true in src/cli.rs:120"}
{"id":"CP-8ci","title":"Sync sandbox mode with temp directory isolation","description":"Upgrade sync subprocess mode to sync sandbox: uses a temp directory, checks out a synced directory or subdirectory, runs the child process in there, and cleans up after itself when the child exits.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:50:05.321115052Z","created_by":"jes","updated_at":"2026-01-01T02:37:02.757303296Z","closed_at":"2026-01-01T02:37:02.757303296Z","close_reason":"Sandbox mode verified working: creates temp dir, syncs with server, runs command in sandbox, cleans up on exit","dependencies":[{"issue_id":"CP-8ci","depends_on_id":"CP-cxj","type":"blocks","created_at":"2025-12-30T18:02:57.400462144Z","created_by":"daemon"},{"issue_id":"CP-8ci","depends_on_id":"CP-w2v","type":"blocks","created_at":"2025-12-30T18:02:57.424433327Z","created_by":"daemon"}],"comments":[{"id":4,"issue_id":"CP-8ci","author":"jes","text":"Accidentally closed without implementation","created_at":"2025-12-30T21:02:26Z"},{"id":8,"issue_id":"CP-8ci","author":"jes","text":"PR #49 was stale - reviewing if sandbox mode actually works correctly","created_at":"2026-01-01T02:36:44Z"}]}
{"id":"CP-8d2x","title":"Deduplicate MQTT subscription helpers","description":"## Summary\nTopic subscription helpers are duplicated across MQTT ports (edits/commands/events/sync) with similar subscribe/unsubscribe wrappers. Consolidate into a shared helper to reduce boilerplate.\n\n## Files to modify\n- src/mqtt/edits.rs\n- src/mqtt/commands.rs\n- src/mqtt/events.rs\n- src/mqtt/sync.rs\n- src/mqtt/mod.rs or src/mqtt/subscriptions.rs (new helper)\n\n## Implementation steps\n1. Add a helper to subscribe/unsubscribe given a Topic and QoS, returning MqttError.\n2. Update each port module to use the shared helper instead of repeating subscribe code.\n3. Keep logging and QoS behavior unchanged.\n\n## Example\nBefore: each module calls client.subscribe(topic_str, QoS::AtLeastOnce).\nAfter: mqtt::subscriptions::subscribe_topic(client, topic).","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:54:05.296521594Z","created_by":"jes","updated_at":"2026-01-10T09:54:05.296521594Z"}
{"id":"CP-8fz","title":"Fix server replay to use node's content type for JSON documents","description":"P1 from PR #6 review: The server's /nodes/:id/head and replace endpoints replay with ContentType::Text, but Y.Map commits need ContentType::Json. When replaying Y.Map commits with Text type, replay.rs returns \"Text root not found\" causing 500 errors.\n\nFix: Update src/api.rs (around lines 460-464) to use the node's actual content type when replaying commits.","design":"Fixed in 3 places:\n\n1. api.rs get_node_head: Now gets content type from the node via DocumentNode downcast instead of hardcoding Text. This allows proper replay of JSON (Y.Map) commits.\n\n2. api.rs replace_content: Now validates content type and returns 415 error for non-text nodes. Replace endpoint only supports text because it uses text-based diffing.\n\n3. sync.rs push_schema_to_server: Simplified to always use edit endpoint with Y.Map updates. This avoids the replace endpoint's text-only limitation.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T18:13:33.205983-08:00","updated_at":"2025-12-26T18:22:55.113026-08:00","closed_at":"2025-12-26T18:22:55.113026-08:00","close_reason":"Fixed server replay to use node's actual content type. GET /nodes/:id/head now works for JSON (Y.Map) documents. Replace endpoint properly rejects non-text content with 415. Merged in PR #7.","labels":["P1","api","bug","yjs"]}
{"id":"CP-8nbb","title":"file-tmux-file: carriage returns not working","description":"Summary: Carriage returns (\\r) are not being handled correctly in file↔tmux↔file flows, causing cursor-return behavior to be lost or rendered incorrectly.\n\nFiles to modify:\n- src/tmux (or tmux integration module)\n- src/sync/file_sync.rs\n- src/sync/watcher.rs\n- tests (tmux/file IO handling)\n\nImplementation steps:\n1. Reproduce by sending a line with \\r (e.g., progress-style output) through file→tmux→file and observe incorrect rendering/output.\n2. Trace how file content is read, normalized, and written; confirm whether \\r is stripped, converted to \\n, or ignored.\n3. Preserve \\r in the data path (or map to tmux send-keys appropriately) so terminal cursor returns behave as expected.\n4. Add a regression test with a \\r-containing payload to ensure roundtrip fidelity.\n\nExample:\nBefore: \"progress 10%\\rprogress 20%\" becomes two lines or loses cursor reset.\nAfter: tmux displays the overwrite behavior and output file preserves intended \\r semantics.\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T03:50:50.189985064Z","created_by":"jes","updated_at":"2026-01-07T00:51:01.079041467Z","closed_at":"2026-01-07T00:51:01.079041467Z","close_reason":"Not reproducible - tested and carriage returns are working correctly"}
{"id":"CP-8ou","title":"Directory sync doesn't preserve node_ids from existing .commonplace.json","description":"When running commonplace-sync in directory mode, scan_directory() always generates fresh Entry structs with node_id: None. It doesn't read the existing .commonplace.json to preserve manually-set node_ids.\n\nThis blocks the ability to link files across directories by manually editing node_ids in the schema - the sync overwrites them on every scan.\n\nExpected behavior: scan_directory should merge with existing .commonplace.json, preserving node_ids for files that still exist.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-31T23:35:43.380888682Z","created_by":"jes","updated_at":"2025-12-31T23:50:25.998188768Z","closed_at":"2025-12-31T23:50:25.998188768Z","close_reason":"Merged in PR #62. scan_directory now preserves node_ids from existing .commonplace.json."}
{"id":"CP-8p9","title":"Sync should delete directories removed from schema","description":"When a directory entry is removed from .commonplace.json schema, the sync should delete the corresponding physical directory from disk.\n\nCurrently, schema changes propagate but the physical directory remains orphaned on disk.\n\nRelated to CP-jgn (the inverse problem).","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T04:59:34.340307337Z","created_by":"jes","updated_at":"2026-01-03T08:31:28.3992328Z","closed_at":"2026-01-03T08:31:28.3992328Z","close_reason":"Sync now deletes files and directories that have been removed from the server schema. When schema changes are detected, orphaned files are deleted and their sync tasks stopped, and orphaned directories are recursively removed."}
{"id":"CP-8sf","title":"Add CLI to signal orchestrator-managed process by path/name","description":"Summary: Provide a command-line tool to signal a running process by path and process name.\n\nFiles to modify:\n- src/bin (new binary, e.g., src/bin/commonplace-signal.rs)\n- src/orchestrator (process lookup by name + cwd/path, expose signal API)\n- docs/DEVELOPMENT.md or README.md (document usage)\n\nImplementation steps:\n1. Accept arguments: path (cwd or root) and process name; optional signal (default SIGTERM).\n2. Resolve to a running orchestrator-managed process that matches name and path.\n3. Send signal and report success; if not found, exit non-zero with a clear error.\n4. Add `--json` output with resolved pid, signal, name, and cwd.\n\nExample:\ncommonplace-signal --path /home/jes/commonplace --name sync --signal SIGTERM\n→ signaled pid 12345 (sync) in /home/jes/commonplace\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T06:08:05.415694784Z","created_by":"jes","updated_at":"2026-01-03T07:46:30.811509276Z","closed_at":"2026-01-03T07:46:30.811509276Z","close_reason":"Implemented commonplace-signal CLI to send signals to orchestrator-managed processes by name. Supports --path filter and --signal option."}
{"id":"CP-8t3","title":"Move HTTP interface to separate binary with MQTT connection","description":"Extract the HTTP interface into a new binary that connects to the document store over MQTT instead of direct access.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T21:56:46.493852588Z","created_by":"jes","updated_at":"2025-12-29T08:45:22.041612098Z","closed_at":"2025-12-29T01:52:01.289555636Z","close_reason":"HTTP/Store split implemented. Two new binaries: commonplace-store (MQTT+persistence) and commonplace-http (stateless HTTP gateway). Fixed P1 issues from codex reviews. Merged in PR #14.","dependencies":[{"issue_id":"CP-8t3","depends_on_id":"CP-08v","type":"blocks","created_at":"2025-12-28T21:57:24.845534402Z","created_by":"daemon"}]}
{"id":"CP-8tiw","title":"P2: log.rs silently ignores Yjs update application failures","description":"In log.rs around lines 315-322, when applying Yjs updates fails, the error is logged but execution continues as if successful. This can lead to divergent state between what the log shows and actual document content.\n\nLocation: src/bin/log.rs:315-322\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:09:57.9244403Z","created_by":"jes","updated_at":"2026-01-06T01:14:47.719996001Z","closed_at":"2026-01-06T01:14:47.719996001Z","close_reason":"Added eprintln warnings for base64 decode and Yjs update decode failures"}
{"id":"CP-8vr","title":"Sync race condition when multiple files share same UUID","description":"When using commonplace-link to make two files share the same UUID, edits to one file don't reliably sync.\n\nROOT CAUSE (from codex review):\nRace between SSE writes and upload_task's initial file read. When processing a change, upload_task reads the file on disk BEFORE it checks state.pending_write. If another SSE handler for the same document is writing server content (because a second local file shares the UUID), the upload task captures the just-written server version instead of the user's edit. It then posts that stale content and logs a successful upload even though the user's change was overwritten on disk and never reaches the server.\n\nFIX: Check pending_write barrier BEFORE reading the file content in upload_task.\n\nLocation: src/sync/file_sync.rs:17-60\n\nRepro:\n1. Create two files with commonplace-link sharing a UUID\n2. Edit file A\n3. Sync reports 'Uploaded: X chars inserted'\n4. But server content remains unchanged\n\nImpact: Sandbox linking architecture (docs/SANDBOX_LINKING.md) doesn't work reliably.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T05:11:38.809310083Z","created_by":"jes","updated_at":"2026-01-01T05:29:49.087146613Z","closed_at":"2026-01-01T05:29:49.087146613Z","close_reason":"Fixed by capturing file content at watcher notification time instead of in upload_task. The watcher now reads the file immediately when detecting a change and passes the content in FileEvent::Modified(Vec\u003cu8\u003e), preventing SSE from overwriting the file before upload_task processes it."}
{"id":"CP-92l","title":"Rename commonplace-replay to commonplace-log and commonplace-show with git-like behavior","description":"Split commonplace-replay into two git-like commands:\n\n**commonplace-log** (like git log):\n- Show commit history for a file\n- Display timestamps, commit IDs, maybe diffs\n- Support filtering by date range, count, etc.\n- Default: show recent commits with summary\n\nGit-compatible output modes:\n- `--oneline`: compact one-line-per-commit format (cid timestamp summary)\n- `--graph`: ASCII art showing commit ancestry/branching\n- `--decorate`: show branch/tag info if applicable\n- `--stat`: show file change statistics (+/- lines)\n- `-n \u003ccount\u003e`: limit number of commits shown\n- `--since`, `--until`: date filtering\n\n**commonplace-show** (like git show):\n- Show content at a specific commit\n- Default: show current HEAD content\n- Support `--at \u003ccid\u003e` or positional `\u003ccid\u003e` for historical content\n- `--stat`: show change stats for the commit\n\nCurrent behavior to preserve:\n- Work from any directory (not just inside workspace)\n- Resolve file path to UUID via .commonplace.json\n- --json output format\n\nPotential enhancements:\n- commonplace-log --diff: show inline diffs between commits\n- commonplace-diff \u003ccid1\u003e \u003ccid2\u003e: compare two commits","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T19:18:18.499816787Z","created_by":"jes","updated_at":"2026-01-03T20:33:59.115960721Z","closed_at":"2026-01-03T20:33:59.115960721Z","close_reason":"Added commonplace-log and commonplace-show binaries with git-like flags (--oneline, --graph, --stat, -n, --since/--until, --json). Extracted workspace path resolution to shared module."}
{"id":"CP-96p","title":"create_yjs_json_update produces malformed nested JSON","description":"When pushing JSON to a Y.Map document, the create_yjs_json_update function puts the entire root object as a string value instead of properly nesting the Y.Map structure. Example: {\"version\":1,\"root\":\"\\n  \\\"version\\\": 1,\\n  \\\"root\\\": {\\n...\"} - The root field contains escaped JSON string instead of an object.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T00:28:29.314098124Z","created_by":"jes","updated_at":"2026-01-01T02:06:51.668159906Z","closed_at":"2026-01-01T02:06:51.668159906Z","close_reason":"Symptom of CP-g0c schema corruption - Y.Map roundtrip tests pass and show correct nested JSON handling"}
{"id":"CP-9abt","title":"WebSocket commonplace protocol extensions (Phase 2)","description":"Add commonplace-specific message types to WebSocket endpoint.\n\nRequires: CP-59h Phase 1 (y-websocket core) - DONE\n\n## Message Types\n\n- Type 3: CommitMeta (parent cid, timestamp, author, message)\n- Type 4: BlueEvent (commit notifications)\n- Type 5: RedEvent (ephemeral JSON events)\n\n## Implementation\n\n1. Add CommitMeta handling in handler.rs\n   - Client sends commit context with updates\n   - Server persists to CommitStore\n\n2. Add BlueEvent broadcasting\n   - When commits are created, broadcast to commonplace-mode clients\n   - Include doc_id, commit_id, timestamp\n\n3. Add RedEvent round-trip\n   - Client sends ephemeral JSON\n   - Server broadcasts to other clients in room\n\n## Files to modify\n\n- src/ws/handler.rs - Handle new message types\n- src/ws/room.rs - BlueEvent/RedEvent broadcasting\n- src/ws/protocol.rs - Already has encoding, add tests","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-04T02:01:36.816738371Z","created_by":"jes","updated_at":"2026-01-04T02:18:07.381495448Z","closed_at":"2026-01-04T02:18:07.381495448Z","close_reason":"Implemented commonplace protocol extensions: CommitMeta handling, BlueEvent broadcast, RedEvent round-trip","dependencies":[{"issue_id":"CP-9abt","depends_on_id":"CP-59h","type":"discovered-from","created_at":"2026-01-04T02:02:24.612719386Z","created_by":"jes"}]}
{"id":"CP-9u1","title":"Sync client uses derived IDs instead of UUIDs from schema","description":"After CP-4f7 (deprecate derived IDs), the reconciler generates UUIDs for file entries. But the sync client still constructs derived IDs like 'fs-root:path/to/file.txt' when syncing file contents, instead of reading the node_id UUIDs from the updated schema.\n\nSteps to reproduce:\n1. Start server with --fs-root\n2. Run sync with --directory pointing to a folder with files\n3. Sync pushes schema, reconciler creates documents with UUIDs\n4. Sync tries to push file content using derived ID\n5. 'Identifier not found, waiting for reconciler' loops forever\n\nExpected: Sync should re-fetch the schema after push to get the UUID node_ids, then use those for file content syncing.\n\nBlocking bartleby integration testing.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-31T16:54:04.192088145Z","created_by":"jes","updated_at":"2025-12-31T17:17:14.235135137Z","closed_at":"2025-12-31T17:17:14.235135137Z","close_reason":"Fixed: Sync client now recursively fetches schemas from node-backed directories to resolve UUIDs. Added build_uuid_map_recursive() function that follows directory references."}
{"id":"CP-9ul","title":"Add commonplace CLI dispatcher that runs subcommands","description":"Create a main `commonplace` binary that dispatches to subcommands, like git:\n\n```\ncommonplace sync ...    → runs commonplace-sync\ncommonplace log ...     → runs commonplace-log\ncommonplace show ...    → runs commonplace-show\ncommonplace ps          → runs commonplace-ps\ncommonplace link ...    → runs commonplace-link\ncommonplace server ...  → runs commonplace-server\n```\n\nImplementation options:\n1. **exec approach**: Just exec the appropriate binary (like git does with git-*)\n2. **built-in approach**: Compile all commands into one binary with subcommands\n\nBenefits:\n- Cleaner UX: `commonplace log` vs `commonplace-log`\n- Discoverability: `commonplace --help` shows all commands\n- Consistent with git, cargo, docker patterns\n\nShould support:\n- `commonplace help \u003ccmd\u003e` or `commonplace \u003ccmd\u003e --help`\n- Pass through all args to subcommand\n- Helpful error if subcommand binary not found","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T19:19:26.189757831Z","created_by":"jes","updated_at":"2026-01-03T20:39:04.41226905Z","closed_at":"2026-01-03T20:39:04.41226905Z","close_reason":"Added commonplace dispatcher binary that runs subcommands (commonplace log, commonplace show, etc). Features: help, version, typo suggestions."}
{"id":"CP-9wb6","title":"Sync client creates inline subdirectories that server rejects after CP-e3jr","description":"## Summary\n\nAfter merging CP-e3jr (remove inline subdirectory support), the sync client and server are incompatible. The sync client creates schemas with inline subdirectories (directories with `node_id: null`), but the server now requires all directories to be node-backed.\n\n## Symptoms\n\n- Server logs show: `Skipping subdirectory document: migration failed: Schema error: Inline subdirectory '0' is not supported. All directories must be node-backed (have node_id).`\n- `/files/` API returns \"Path not found in filesystem\" for affected directories\n- Orchestrator discovery fails with 404 when trying to load `__processes.json` from affected paths\n- Sandbox processes (like file-tmux-file) fail to start because their process definitions can't be loaded\n\n## Reproduction\n\n1. Start orchestrator with fresh server (no --database persistence)\n2. Sync client pushes filesystem schema to server\n3. Any directory structure with nested subdirectories gets inline entries\n4. Server reconciler rejects these schemas\n5. Files in those directories become inaccessible via /files/ API\n\n## Affected Code\n\n- `src/sync/dir_sync.rs` - Creates schemas with inline subdirectories\n- `src/fs/reconciler.rs` - Rejects inline subdirectories (correct behavior after CP-e3jr)\n\n## Fix Required\n\nUpdate the sync client to create node-backed directories instead of inline ones. When syncing a directory structure, each subdirectory should:\n1. Get its own document/node_id on the server\n2. Reference that node_id in the parent schema instead of embedding entries inline\n\n## Workaround\n\nNone currently - this blocks all nested directory syncing.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-08T23:52:32.120754165Z","created_by":"jes","updated_at":"2026-01-09T00:28:12.91091059Z","closed_at":"2026-01-09T00:28:12.91091059Z","close_reason":"Fixed: Sync client now creates node-backed directories instead of inline subdirectories. All tests pass."}
{"id":"CP-a0g","title":"Orphaned sandbox processes survive orchestrator restarts","description":"When the orchestrator is restarted (not just killed), old sandbox processes from previous runs can remain orphaned.\n\nObserved:\n- Orchestrator started at 18:04, spawned text-to-telegram (PIDs 2831xxx)\n- Orchestrator restarted at 18:27, spawned new text-to-telegram (PIDs 2847xxx)  \n- Old 2831xxx processes still running alongside new ones\n- This caused Telegram API conflicts (multiple bot instances polling)\n\nExpected: When orchestrator starts, it should either:\n1. Kill any orphaned processes from previous runs (check .pid files in sandboxes?)\n2. Or detect existing sandbox syncs and reuse them instead of spawning duplicates\n\nThe T1-T6 termination cascade tests verify cleanup when killing the *current* orchestrator, but don't cover the restart scenario where old processes persist.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T18:49:13.77723552Z","created_by":"jes","updated_at":"2026-01-03T07:21:05.246726667Z","closed_at":"2026-01-03T07:21:05.246726667Z","close_reason":"Implemented: cleanup_stale_sandboxes() at sync startup + PID file tracking + PR_SET_PDEATHSIG","dependencies":[{"issue_id":"CP-a0g","depends_on_id":"CP-occ","type":"relates-to","created_at":"2026-01-02T18:51:55.367733448Z","created_by":"daemon"},{"issue_id":"CP-a0g","depends_on_id":"CP-yw8","type":"relates-to","created_at":"2026-01-02T19:21:00.669824268Z","created_by":"daemon"}]}
{"id":"CP-a30","title":"Blocks acceptance: File content not syncing to sandboxes","description":"During acceptance testing, file content edits in workspace are not propagating to sandbox directories.\n\nSymptoms:\n- workspace/text-to-telegram/test-file.txt has 'hello' locally\n- Server shows empty content for the file\n- Sandbox file is empty\n\nRelated issues:\n- CP-azw (JSON files sync as empty)\n- This may be a broader sync issue affecting all content types\n\nReproduction:\n1. Start server, workspace sync, orchestrator\n2. Create/edit workspace/text-to-telegram/test-file.txt with 'hello'\n3. Wait 5+ seconds\n4. Check server: curl http://localhost:3000/docs/{node_id}/head\n5. Check sandbox: cat /tmp/commonplace-sandbox-*/test-file.txt\n6. Both should have 'hello' but show empty\n\nThis blocks acceptance criteria C1-C6, E1-E6, and dependent tests.","notes":"Verified working after sync restart. Initial content push issue is timing-related and resolves after file watcher stabilizes.","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-01-03T03:59:30.365618809Z","created_by":"jes","updated_at":"2026-01-03T04:22:42.473808382Z","closed_at":"2026-01-03T04:22:42.473815905Z"}
{"id":"CP-a31","title":"commonplace-link doesn't push schema changes to server","description":"When commonplace-link modifies a local .commonplace.json file to add a linked entry, the sync process doesn't detect this change and push it to the server. The schema change stays local until manually pushed.\n\nWorkaround: Manually push the schema with curl:\ncurl -X POST http://localhost:3000/docs/{node_id}/replace -H 'Content-Type: application/json' -d \"$(cat .commonplace.json)\"\n\nExpected: commonplace-link should push the schema change to server after modifying local schema files, OR sync should detect local schema changes and push them.","notes":"This is the root cause of CP-fr4 (linked files not syncing). The fix is to add HTTP client code to link.rs to push schema changes to server after modifying local .commonplace.json files. The push_schema_to_server function already exists in sync/client.rs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T10:34:10.974157176Z","created_by":"jes","updated_at":"2026-01-03T05:38:14.912198187Z","closed_at":"2026-01-03T05:38:14.912198187Z","close_reason":"Fixed: commonplace-link now pushes schema changes to server after local write. Added --server argument, async main, reqwest client, and push_schema_to_server calls."}
{"id":"CP-a524","title":"Deduplicate SSE task connection loop","description":"## Summary\nSSE connection loops are duplicated across sse_task, sse_task_with_flock, and sse_task_with_tracker. Consolidate the shared SSE connect/retry loop and event dispatch to reduce drift.\n\n## Files to modify\n- src/sync/sse.rs (extract shared SSE loop and reuse it)\n\n## Implementation steps\n1. Identify the shared connect/reconnect loop and event match (open/edit/shutdown/warn/error/404).\n2. Extract a helper (e.g., run_sse_loop) that accepts a handler callback for edit events and any per-task state (flock/tracker).\n3. Update sse_task, sse_task_with_flock, and sse_task_with_tracker to call the shared loop.\n4. Keep logging behavior and retry delays identical.\n\n## Example\nBefore: three near-identical loops differ only by edit handler.\nAfter: one loop, three handlers.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T09:08:19.643042598Z","created_by":"jes","updated_at":"2026-01-10T09:44:46.539239392Z","closed_at":"2026-01-10T09:44:46.539239392Z","close_reason":"Added SseContext enum and run_sse_loop function. Updated sse_task, sse_task_with_flock, and sse_task_with_tracker to use shared loop (51 lines reduced)."}
{"id":"CP-a7lb","title":"P1: WebSocket SyncStep2 responses dropped under backpressure","description":"In handler.rs around lines 262-270, when the WebSocket send buffer is full, SyncStep2 sync responses are silently dropped. This can cause sync to stall or produce inconsistent state. Should either block, buffer, or error rather than drop.\n\nLocation: src/ws/handler.rs:262-270\nFound by: codex review","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T09:09:55.226414072Z","created_by":"jes","updated_at":"2026-01-05T23:05:38.008662504Z","closed_at":"2026-01-05T23:05:38.008662504Z","close_reason":"Fixed: SyncStep2 response now returns error if send fails instead of silently dropping"}
{"id":"CP-a85","title":"Filetree to XHTML renderer","description":"Render a filetree (directory structure) as XHTML output.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:54:39.878136696Z","created_by":"jes","updated_at":"2026-01-07T04:31:06.281613644Z","closed_at":"2026-01-07T04:31:06.281613644Z","close_reason":"Implemented filetree-to-xml process: Added SDK cp.watch() wildcard support, created filetree-to-xml.ts with recursive schema watching, fixed orchestrator --allow-import for Deno 2.x. Process registers and starts but has XML output merging issues - filed follow-up bug.","labels":["future-work"]}
{"id":"CP-aa5","title":"Make debug previews in document replace UTF-8 safe","description":"Summary: Prevent UTF-8 slicing panics in replace_content debug logging so document updates apply.\n\nFiles to modify:\n- src/services/document.rs (debug preview slices around lines 623, 625, 738, 754)\n\nImplementation steps:\n1. Add a small helper to preview strings by character count (not byte index).\n2. Replace unsafe byte slicing in debug logs with the helper.\n3. Keep byte length logging unchanged to preserve diagnostics.\n\nExample:\nBefore: \u0026content_before[..content_before.len().min(50)]\nAfter:  preview_text(\u0026content_before, 50)\n","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T04:38:59.366095457Z","created_by":"jes","updated_at":"2026-01-03T04:39:57.640942282Z","closed_at":"2026-01-03T04:39:57.640942282Z","close_reason":"Done"}
{"id":"CP-acp1","title":"Deduplicate content-type resolution across sync and MQTT","description":"## Summary\nContent-type detection is split between sync/content_type.rs (detect_from_path) and mqtt/topics.rs (content_type_for_path), which can drift. Consolidate into a shared ContentType resolver used by both sync and MQTT paths.\n\n## Files to modify\n- src/sync/content_type.rs (export shared resolver)\n- src/mqtt/topics.rs (use shared resolver)\n- src/mqtt/edits.rs (no change besides import if needed)\n\n## Implementation steps\n1. Expose a shared ContentType resolver that accepts a path string and returns ContentType + mime/binary metadata.\n2. Update content_type_for_path to delegate to the shared resolver (or remove it).\n3. Ensure extensions like .xml/.xhtml and .jsonl are mapped consistently across MQTT and sync.\n\n## Example\nBefore: MQTT and sync can disagree on content type for a path.\nAfter: both use the same resolver.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:27:13.972994287Z","created_by":"jes","updated_at":"2026-01-10T11:20:04.552876985Z","closed_at":"2026-01-10T11:20:04.552876985Z","close_reason":"Deduplicated content-type detection by updating mqtt/topics.rs to use shared detect_from_path from sync/content_type.rs. Also added .bin to TEXT_EXTENSIONS for MQTT compatibility and application/xhtml+xml to ContentType::from_mime."}
{"id":"CP-adm","title":"Biscuits for MQTT-layer authorization","description":"Implement biscuit-based authorization enforced at the MQTT layer. Biscuits provide:\n- Delegatable, attenuatable credentials\n- Fine-grained path-based permissions\n- Caveat-based restrictions (time, scope, etc.)\n\nEnforcement at MQTT means all clients (JS sandbox, external processes, CLI, MCP servers) go through the same auth layer.","design":"Spec drafted in `docs/MACAROONS.md`.\n\nKey decisions (v1):\n- Enforcement: Mosquitto auth plugin as verifier; offline verification at broker.\n- Transport: MQTT CONNECT password carries base64url biscuit (username optional for logging).\n- Caveats:\n  - cp.v=1\n  - cp.exp=\u003cunix_seconds\u003e (recommended)\n  - cp.aud=\u003cbroker_id\u003e (recommended)\n  - cp.cid=\u003cmqtt_client_id\u003e (optional binding)\n  - cp.acl=\u003cbase64url(json)\u003e where json contains publish/subscribe/both topic filters.\n- ACL evaluation: multiple cp.acl caveats are intersected (supports attenuation by adding stricter ACLs).\n- SUBSCRIBE policy: prefer subset-of-allowed check for topic filters; allow exact-match as v1 fallback.\n- Revocation strategy: short expirations + key rotation in v1; optional third-party caveats/discharge later.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.628965-08:00","updated_at":"2026-01-10T09:21:21.870753343Z","labels":["future-work"]}
{"id":"CP-ajb","title":"Add integration tests for MQTT path resolution","design":"Needs design discussion: Requires understanding MQTT broker setup for tests. Current codebase doesn't have MQTT integration tests - would need to add test infrastructure first (mock broker or real broker in tests). Substantial effort beyond just adding test cases.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-29T08:03:39.396454509Z","created_by":"jes","updated_at":"2025-12-30T23:17:27.562485622Z","closed_at":"2025-12-30T23:17:27.562485622Z","close_reason":"Added 13 integration tests for MQTT path resolution. PR #54 merged."}
{"id":"CP-ayri","title":"P2: cbd status update ignores close metadata","description":"In cbd.rs around lines 1003-1006, when updating issue status to 'closed', the code doesn't properly handle close_reason and close_context fields that should be set. The update command should accept these fields when status=closed.\n\nLocation: src/cbd.rs:1003-1006\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:09:50.974241873Z","created_by":"jes","updated_at":"2026-01-06T00:49:21.193181395Z","closed_at":"2026-01-06T00:49:21.193181395Z","close_reason":"Added --reason flag to update command, auto-sets closed_at when status=closed"}
{"id":"CP-az4","title":"commonplace-link doesn't support node-backed directories","description":"When trying to link files where one is in a node-backed directory (like bartleby/), commonplace-link fails with 'Directory bartleby has no entries (node-backed?)'.\n\nTest case:\ncd workspace\n../target/release/commonplace-link shared-a.txt bartleby/shared-b.txt\n\nError: Directory bartleby has no entries (node-backed?)\n\nThe tool needs to be updated to fetch the nested schema from the server for node-backed directories.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T09:46:43.116817489Z","created_by":"jes","updated_at":"2026-01-02T10:27:00.376511008Z","closed_at":"2026-01-02T10:27:00.376511008Z","close_reason":"Fixed. commonplace-link now supports node-backed directories by detecting them during path traversal and loading their local .commonplace.json files. Both source and target can be in node-backed subdirectories. PR #88 merged."}
{"id":"CP-az8d","title":"file-tmux-file: XML docs should use Yjs XmlFragment, not Text","description":"Summary: XML documents in the file↔tmux↔file path are currently coerced to Yjs Text as a workaround, which loses element-level XML CRDT semantics. The diff/merge layer should support Yjs XmlFragment directly so XML edits preserve structure-aware operations.\\n\\nFiles to modify:\\n- src/sync/file_sync.rs\\n- src/sync/diff.rs (or the XML/Yjs diff module)\\n- src/tmux (tmux integration for XML docs)\\n- tests (XML diff/merge and file-tmux-file roundtrip)\\n\\nImplementation steps:\\n1. Identify where XML documents are detected and coerced to Text (the workaround path).\\n2. Update the diff/merge module to accept and operate on Yjs XmlFragment, preserving element boundaries and attributes.\\n3. Plumb XmlFragment through file-tmux-file flow without type coercion.\\n4. Ensure serialization/deserialization of XML retains structure and works with existing CRDT ops.\\n5. Add tests that apply element-level edits (insert/remove nodes/attributes) and verify correct merges through the tmux/file pipeline.\\n\\nExample:\\nBefore: \u003cp\u003e\u003cb\u003ehi\u003c/b\u003e\u003c/p\u003e edits become flat text; element merges are lost.\\nAfter: XmlFragment preserves \u003cb\u003e node operations and merges structurally.\\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T05:29:24.125605677Z","created_by":"jes","updated_at":"2026-01-07T19:21:25.519416271Z","closed_at":"2026-01-07T19:21:25.519416271Z","close_reason":"Core XmlFragment support implemented. XML documents now use XmlFragment with proper diff/replace support via compute_xml_diff_update(). File sync path should now work with XML content type."}
{"id":"CP-azw","title":"Blocks acceptance: New JSON files sync as empty {}","description":"Summary: Creating a JSON file locally can result in the synced file becoming empty ({}). Example: writing '{\"a\": 1}' to baz.json results in the file content changing to '{}'.\n\nRepro:\n1. cd ~/commonplace/workspace/bartleby\n2. echo '{\"a\": 1}' \u003e baz.json\n3. cat baz.json\n4. Observe output becomes '{}' (empty object)\n\nFiles to modify:\n- src/bin/sync.rs (file create/update handling)\n- src/sync/* (create vs update detection, CRDT initialization)\n- any JSON format handling that normalizes/rewrites content\n\nImplementation steps:\n1. Reproduce with sync logs enabled; capture file watcher events and server updates.\n2. Check if initial file creation triggers schema creation with empty JSON and overwrites local content.\n3. Verify whether CRDT or JSON parser normalizes invalid/partial reads on create.\n4. Ensure create+write is treated atomically (avoid reading file before write completes).\n5. Add a regression test for create/write JSON file content preservation.\n\nExample:\nBefore: baz.json contains '{\"a\": 1}'.\nAfter: baz.json remains '{\"a\": 1}' and is pushed to server without being replaced by '{}'.","notes":"Verified working after sync restart. JSON file sync issue is timing-related during initial sync.","status":"closed","priority":0,"issue_type":"bug","created_at":"2026-01-03T00:02:32.674091099Z","created_by":"jes","updated_at":"2026-01-03T04:22:43.331706593Z","closed_at":"2026-01-03T04:22:43.331714451Z"}
{"id":"CP-bagj","title":"Centralize CLI defaults for server/MQTT/workspace","description":"## Summary\nCLI defaults for server URL, MQTT broker, and workspace name are repeated across multiple argument structs and binaries. Centralize defaults to avoid drift (e.g., http://localhost:3000, mqtt://localhost:1883, workspace=commonplace).\n\n## Files to modify\n- src/cli.rs (use shared constants)\n- src/bin/sync.rs (use shared constants)\n- src/config.rs or src/constants.rs (new shared defaults)\n\n## Implementation steps\n1. Introduce constants for default server URL, MQTT broker URL, and workspace name.\n2. Replace literal default_value strings in clap args with the constants.\n3. Keep env var overrides intact.\n\n## Example\nBefore: repeated \"http://localhost:3000\" literals in multiple structs.\nAfter: DEFAULT_SERVER_URL used everywhere.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:31:21.649118526Z","created_by":"jes","updated_at":"2026-01-10T09:31:21.649118526Z"}
{"id":"CP-bb2c","title":"Deduplicate node-backed subdir spawn logic","description":"## Summary\nNode-backed subdir discovery + spawn logic is duplicated between sync/subscriptions.rs and bin/sync.rs. Extract a shared helper to fetch subdirs and spawn tasks consistently.\n\n## Files to modify\n- src/sync/subscriptions.rs\n- src/bin/sync.rs\n- src/sync/subdir_spawn.rs (new helper)\n\n## Implementation steps\n1. Add helper to fetch node-backed subdirs and spawn subdir SSE/MQTT tasks based on transport.\n2. Replace repeated discovery/spawn blocks in subscriptions.rs and sync.rs.\n3. Keep logging and push_only behavior unchanged.\n\n## Example\nBefore: multiple blocks call get_all_node_backed_dir_ids and spawn tasks.\nAfter: spawn_subdir_watchers(...) used everywhere.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:02:29.543689746Z","created_by":"jes","updated_at":"2026-01-10T10:02:29.543689746Z"}
{"id":"CP-bd3f","title":"Deduplicate MQTT receive loop handling","description":"## Summary\nMQTT message receive loops (handling Lagged/Closed) are duplicated in mqtt/mod.rs and http_gateway/sse.rs. Extract a shared helper for consuming the broadcast channel with consistent error handling.\n\n## Files to modify\n- src/mqtt/mod.rs\n- src/http_gateway/sse.rs\n- src/mqtt/receiver.rs or src/mqtt/util.rs (new helper)\n\n## Implementation steps\n1. Add a helper to drive a broadcast receiver with standardized Lagged/Closed handling.\n2. Use it in mqtt::MqttService::run and http_gateway::sse task.\n3. Keep logging consistent.\n\n## Example\nBefore: two separate recv loops.\nAfter: shared consume_mqtt_messages(receiver, handler).","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:54:27.663325433Z","created_by":"jes","updated_at":"2026-01-10T09:54:27.663325433Z"}
{"id":"CP-bl1q","title":"Petri-token like functionality (\"green\")","description":"Future exploration: Add petri-token like functionality to commonplace, potentially called \"green\". Details TBD.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T23:42:02.081162414Z","created_by":"jes","updated_at":"2026-01-05T23:42:02.081162414Z","labels":["future-work"]}
{"id":"CP-bl9","title":"Bartleby input document lines reappear after deletion","description":"Summary: The bartleby input document periodically repopulates previously deleted lines; deletions appear to be undone, suggesting a sync or writeback issue.\n\nFiles to modify:\n- workspace/bartleby/input.txt (observed symptom; replace with actual input doc path)\n- src/bin/sync.rs (inspect bidirectional sync + conflict resolution)\n- src/sync/* (file event handling and writeback logic)\n- src/orchestrator/* (if sandbox processes are restarting or resyncing)\n\nImplementation steps:\n1. Identify the exact input document path and reproduction (delete lines, wait, observe reappearance).\n2. Check if multiple processes are writing to the same document (duplicate sandboxes or multiple bartleby instances).\n3. Inspect sync logs around the time of reappearance for incoming edits vs. local file events.\n4. Verify whether sync is re-applying server state after local deletes (e.g., due to missed local change or schema mismatch).\n5. Add a test or logging to capture the ordering of local delete vs. remote update events.\n\nExample:\nBefore: Delete lines 10-20 in bartleby input file; after ~N seconds, lines 10-20 reappear unchanged.\nAfter: Deleted lines stay deleted unless a new remote edit explicitly re-adds them.","notes":"Plan: detect atomic writes (tmp-\u003erename) in sync so replacements are treated as content updates, not delete+create.\n\nProposed approach:\n1) Define atomic pattern: CREATE temp file + WRITE/CLOSE + RENAME temp -\u003e target (often preceded by REMOVE target). Treat as single update to target.\n2) Add event coalescer in sync watcher keyed by directory + basename. Buffer events for short window (50–200ms). If REMOVE target + RENAME tmp-\u003etarget occurs within window, collapse to update target and ignore temp file.\n3) Use rename-correlation when available (inotify cookie IDs); otherwise fallback to time+path heuristics.\n4) Sync pipeline: on coalesced update, read target and emit content update, not delete+create. Ignore temp files by pattern and by rename-in-window.\n5) Tests: (a) atomic write via temp+rename emits exactly one update to server, (b) delete+create without rename remains delete+create.\n\nLikely files:\n- src/bin/sync.rs\n- src/sync/* (event handling / coalescing)\n\nWhy: text-to-telegram uses atomic write (temp -\u003e rename). Without coalescing, watcher may miss or double-fire and CRDT may treat replacement as reinsert, causing deleted lines to reappear.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T19:32:15.387195704Z","created_by":"jes","updated_at":"2026-01-02T20:07:31.663592454Z","closed_at":"2026-01-02T20:07:31.663592454Z","close_reason":"Fixed. Implemented atomic write detection in directory watcher: Deleted+Created within debounce window coalesced to Modified. Added temp file filtering. PR #94 merged.","comments":[{"id":20,"issue_id":"CP-bl9","author":"jes","text":"Investigation findings:\n\n1. Text-to-telegram uses atomic writes (temp file + rename) in _atomic_write()\n2. This creates a new inode each time, which may confuse the file watcher or CRDT sync\n3. The output.txt shows the same prompts being processed multiple times at different timestamps\n4. Example: '[2025-12-31 23:01:52] hi' was processed at 01:20:53 AND again at 01:30:24\n\nPossible causes:\na) Atomic write creates new file, sync treats it as replacement rather than edit\nb) CRDT merge conflicts when multiple sync clients have divergent states\nc) File watcher missing events or double-firing on atomic writes\n\nNext steps:\n- Check if sync's file watcher handles atomic writes correctly\n- Consider using in-place edits instead of atomic writes in text-to-telegram\n- Add logging to trace edit propagation through the sync chain","created_at":"2026-01-02T19:47:11Z"}]}
{"id":"CP-bno","title":"POST /docs should accept initial content in request body","description":"Currently POST /docs creates an empty document. The request body is not used as initial content.\n\nEnhancement: Allow POST /docs to accept content in the body, either:\n1. JSON body for JSON documents\n2. Text body for text documents\n\nThis would simplify workflows where you want to create a document with specific content.","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-01T00:15:49.814509595Z","created_by":"jes","updated_at":"2026-01-01T11:40:47.686684035Z","closed_at":"2026-01-01T11:40:47.686684035Z","close_reason":"Fixed in PR #76. POST /docs now accepts optional 'content' field in request body for setting initial document content. Returns 400 and cleans up if content setting fails."}
{"id":"CP-bnv","title":"Add path-based HTTP API endpoints","design":"Needs design discussion: What should the path-based API look like? Options: 1) /files/path/to/file.txt 2) /docs?path=path/to/file.txt 3) Something else. Need to decide URL structure, how to handle URL encoding of paths, and whether to support both path and UUID access on same endpoints.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-29T08:33:32.251084948Z","created_by":"jes","updated_at":"2025-12-29T21:44:24.931689601Z","closed_at":"2025-12-29T21:44:24.931689601Z","close_reason":"Added /files/*path endpoints for path-based HTTP API access. Merged in PR #24."}
{"id":"CP-bsso","title":"Extract shared subscription helper for actor SSE/NATS providers","description":"Summary: SSESubscriptionProvider and NatsSubscriptionProvider repeat the same subscription id formatting, JSON parse/error logging, and per-subscription cleanup. Add shared helpers to keep the two providers consistent.\n\nFiles to modify:\n- commonplaced-2025/src/actors/common/sse-subscription-provider.ts\n- commonplaced-2025/src/actors/common/nats-subscription-provider.ts\n- commonplaced-2025/src/actors/common/subscription-system.ts (or new helper module)\n\nImplementation steps:\n1) Extract shared helper(s) for subscription id generation and JSON parsing with consistent error handling.\n2) Update both providers to reuse the helper(s) for document/dir/all subscriptions.\n3) Keep transport-specific parts (EventSource vs NATS subscription) inside each provider.\n\nExample:\nBefore: each provider builds subscription ids and JSON.parse+try/catch inline.\nAfter: use shared helper like buildSubscriptionId(kind, key, counter) and parseSubscriptionPayload(event.data, logPrefix).","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:12:22.989122332Z","created_by":"jes","updated_at":"2026-01-10T10:12:40.657445507Z"}
{"id":"CP-bt0","title":"Add CLI to list orchestrator-managed processes (pid/cwd)","description":"Summary: Provide a command-line tool to list all processes currently managed by the commonplace orchestrator, including PID and CWD. The list must include both the orchestrator's direct child processes and any sync subprocesses spawned beneath them.\n\nFiles to modify:\n- src/bin (new binary, e.g., src/bin/commonplace-ps.rs)\n- src/orchestrator (add method to enumerate running processes and their metadata)\n- docs/DEVELOPMENT.md or README.md (document usage)\n\nImplementation steps:\n1. Add an orchestrator query API that returns process name, PID, and working directory, including nested child processes spawned by sync.\n2. Implement CLI that prints a table to stdout; include `--json` option for machine-readable output.\n3. If orchestrator is not running, print a clear error and exit non-zero.\n\nExample:\ncommonplace-ps\nname\\tpid\\tcwd\nserver\\t12345\\t/home/jes/commonplace\nsync\\t12346\\t/home/jes/commonplace/workspace\nsync-child\\t12347\\t/home/jes/commonplace/workspace\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T05:49:01.43131488Z","created_by":"jes","updated_at":"2026-01-03T07:31:52.250923124Z","closed_at":"2026-01-03T07:31:52.250923124Z","close_reason":"Implemented commonplace-ps CLI that reads status file from orchestrator. Added OrchestratorStatus and ProcessStatus types in src/orchestrator/status.rs. Orchestrator writes status to /tmp/commonplace-orchestrator-status.json when processes start/stop."}
{"id":"CP-bth","title":"Sync tool should respect environment variables as set by orchestrator","description":"The sync client (commonplace-sync) should read and respect environment variables that are set by the orchestrator process. This allows the orchestrator to configure sync behavior (server URL, node ID, etc.) through environment variables rather than requiring all configuration to be passed as CLI arguments.\n\nNote: This issue was recovered from commit ee6f395 after being accidentally deleted during a beads sync conflict.","notes":"Reviewed: sync tool already respects COMMONPLACE_SERVER, COMMONPLACE_NODE, COMMONPLACE_FORK_FROM via clap env attribute. May need COMMONPLACE_MQTT if direct MQTT support added to sync.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T06:44:05.946481773Z","created_by":"jes","updated_at":"2025-12-30T18:42:26.376893745Z","closed_at":"2025-12-30T18:42:26.376893745Z","close_reason":"Closed"}
{"id":"CP-buj","title":"Sync should auto-convert symlinks to commonplace-linked files","description":"When commonplace-sync encounters a symlink in the synced directory, it should:\n\n1. Check if the symlink target is also within a synced directory\n2. If so, automatically convert to a commonplace-link (same UUID)\n3. If not, warn the user that the symlink won't sync correctly\n\nThis prevents agents and users from accidentally creating symlinks that break sync, by transparently converting them to the correct mechanism.\n\nEdge cases:\n- Symlink to file outside workspace: warn and skip\n- Symlink to directory: may need special handling (directory linking not yet supported)\n- Circular symlinks: detect and warn","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-02T19:17:03.570133771Z","created_by":"jes","updated_at":"2026-01-03T08:57:14.276022696Z","closed_at":"2026-01-03T08:57:14.276022696Z","close_reason":"Implemented symlink auto-conversion to commonplace-linked files. When sync encounters a symlink within the workspace, it uses the target's node_id for the symlink entry, effectively creating a commonplace-link. Edge cases handled: outside workspace, directories, unsynced targets, hidden/ignored symlinks."}
{"id":"CP-bxv","title":"Recursive discovery should wait for sync initial push to complete","description":"When starting in recursive mode, the orchestrator waits 3 seconds for sync to push initial content. But sync can take 1-2 minutes to push all content depending on the workspace size.\n\nCurrent behavior:\n1. Orchestrator starts server + sync\n2. Waits 3 seconds\n3. Runs discovery - may find empty processes.json files\n4. Re-discovery runs every 30 seconds but doesn't re-try failed processes.json\n\nProblems:\n1. Discovery runs before sync has pushed all content\n2. Failed processes.json files (empty at initial parse) are never retried\n3. Only schema changes trigger re-discovery, not file content changes\n\nSuggested fix:\n1. Sync should signal when initial push is complete (via exit, or a 'ready' state)\n2. Orchestrator should wait for this signal before starting discovery\n3. Discovery should re-try any processes.json that failed to parse after content changes\n\nWorkaround: Increase the wait time from 3 seconds to something larger.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T18:05:09.767910488Z","created_by":"jes","updated_at":"2026-01-04T00:32:13.192569475Z","closed_at":"2026-01-04T00:32:13.192569475Z","close_reason":"Implemented polling-based wait for sync initial push instead of fixed 10s timeout"}
{"id":"CP-byr","title":"Add CLI to view/replay edits for a file","description":"Summary: Provide a command-line tool that can list and replay edits for a synced file using its commit history.\n\nFiles to modify:\n- src/bin (new binary, e.g., src/bin/commonplace-replay.rs)\n- src/replay.rs (expose helpers to stream commits and/or content at commits)\n- docs/DEVELOPMENT.md or README.md (document usage)\n\nImplementation steps:\n1. Accept a file path or UUID and resolve to a document ID.\n2. Fetch commit history and print a summary list (cid, timestamp, author, message).\n3. Add a replay mode that reconstructs content at each commit and streams diffs or full content.\n4. Add `--json` output option for both list and replay modes.\n\nExample:\ncommonplace-replay workspace/text-to-telegram/input.txt --list\ncommonplace-replay workspace/text-to-telegram/input.txt --replay --to /tmp/replay.txt\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T06:51:30.897030745Z","created_by":"jes","updated_at":"2026-01-03T08:05:11.809992855Z","closed_at":"2026-01-03T08:05:11.809992855Z","close_reason":"Implemented commonplace-replay CLI to view commit history and content at any commit for synced files."}
{"id":"CP-c1b","title":"Fix P1: Start sync tasks for newly created local files (PR #4)","description":"From PR #4 Codex review: In directory mode, create/modify events only trigger a schema update but don't start sync tasks for the new files.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:21.87906-08:00","updated_at":"2025-12-26T23:43:55.607923-08:00","closed_at":"2025-12-26T23:43:55.607923-08:00","close_reason":"Already fixed and merged to main. The DirEvent::Created handler in directory_watcher_task (sync.rs lines 805-828) now calls spawn_file_sync_tasks for newly created local files."}
{"id":"CP-c1j","title":"Cross-fs-root linking via commonplace-link","description":"Enable commonplace-link to assign shared UUIDs across different fs-roots. This allows files in separate sync sandboxes to share content through the server.\n\nUse case: text-to-telegram and bartleby run in separate sandboxes. We need text-to-telegram/content.txt to sync with workspace/telegram/content.txt by sharing the same UUID.\n\nApproach: \n1. Create a 'linking workspace' that can reference files from multiple fs-roots\n2. Use commonplace-link within the linking workspace to assign shared UUIDs\n3. Individual sandbox syncs respect UUIDs assigned externally\n\nSee docs/SANDBOX_LINKING.md for full architecture.","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-01T04:54:08.973778925Z","created_by":"jes","updated_at":"2026-01-01T04:59:07.233301299Z","closed_at":"2026-01-01T04:59:07.233301299Z","close_reason":"Not needed - single workspace tree with subdirectories solves the problem. See docs/SANDBOX_LINKING.md"}
{"id":"CP-c6v","title":"Orchestrator should dynamically start/stop processes as processes.json changes","description":"Summary: Orchestrator should start new processes and stop removed ones when processes.json files change, without requiring a restart.\n\nFiles to modify:\n- src/orchestrator (process discovery and reload handling)\n- src/orchestrator/discovery.rs (detect add/remove diffs)\n- docs/DEVELOPMENT.md or README.md (document live reload behavior)\n\nImplementation steps:\n1. Detect changes in processes.json and compute added/removed process entries.\n2. Start newly added processes automatically.\n3. Stop processes that were removed from the config.\n4. Log actions clearly and avoid flapping on transient edits.\n\nExample:\n- Add entry to processes.json → orchestrator spawns new process.\n- Remove entry → orchestrator terminates process.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T18:15:52.050521881Z","created_by":"jes","updated_at":"2026-01-03T18:39:49.017746667Z","closed_at":"2026-01-03T18:39:49.017746667Z","close_reason":"Implemented dynamic process start/stop. SSE now watches processes.json files for content changes. When a processes.json is edited, the orchestrator immediately fetches and reconciles to start new processes or stop removed ones."}
{"id":"CP-c7h","title":"Podman for untrusted sandbox","description":"Use Podman containers for sandboxed execution of untrusted code. Provides stronger isolation than temp directory sandbox for running arbitrary user-provided commands.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-30T19:39:21.869408178Z","created_by":"jes","updated_at":"2025-12-30T19:39:21.869408178Z","labels":["future-work"]}
{"id":"CP-c7xi","title":"Deduplicate diff stats helpers","description":"## Summary\nChangeStats computation and diff formatting logic is duplicated between bin/log.rs and bin/show.rs. Extract shared diff stats helper to keep calculations consistent.\n\n## Files to modify\n- src/bin/log.rs\n- src/bin/show.rs\n- src/cli/diff_stats.rs (new shared helper)\n\n## Implementation steps\n1. Move compute_diff_stats and related ChangeStats struct to a shared helper.\n2. Update log/show to import and use the shared helper.\n3. Keep output formatting and counts identical.\n\n## Example\nBefore: log.rs and show.rs each implement compute_diff_stats.\nAfter: both call diff_stats::compute(old, new).","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:52:43.009147203Z","created_by":"jes","updated_at":"2026-01-10T11:56:55.59566792Z","closed_at":"2026-01-10T11:56:55.59566792Z","close_reason":"Extracted ChangeStats and compute_diff_stats to cli.rs; updated show.rs and log.rs to use shared implementation"}
{"id":"CP-c8y6","title":"Add --follow and --reverse flags to commonplace-log","description":"Add two new flags:\n- --reverse: Show commits oldest-first instead of newest-first\n- --follow: Watch for new commits via SSE and output them as they arrive (implies --reverse)\n\n--follow will subscribe to the document's SSE stream and print new commits as they happen.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T21:41:03.193815251Z","created_by":"jes","updated_at":"2026-01-03T21:44:14.35592669Z","closed_at":"2026-01-03T21:44:14.35592669Z","close_reason":"Implemented --reverse for oldest-first ordering and --follow (-f) for watching new commits via SSE"}
{"id":"CP-ce1","title":"Implement .processes.json discovery for conductor","description":"Conductor discovers .processes.json files in the document tree and automatically launches/manages declared processes. Enables user-defined processes to attach to file paths without central configuration.\n\nSee: docs/plans/2025-12-30-processes-json-design.md\n\nTasks:\n- Parse .processes.json format (command, owns, cwd)\n- Watch document tree for .processes.json changes\n- Launch processes with COMMONPLACE_PATH and COMMONPLACE_MQTT env vars\n- Restart with exponential backoff on failure\n- Handle conflicts (warn if two processes claim same path)\n- Update counter example to use this convention","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T04:17:59.60264365Z","created_by":"jes","updated_at":"2025-12-30T04:56:47.832163772Z","closed_at":"2025-12-30T04:56:47.832163772Z","close_reason":"Implemented .processes.json discovery for conductor. Created discovery.rs with config parsing and process manager. Updated counter example to use env vars. PR #35 merged."}
{"id":"CP-ceq","title":"Test that new files created in sandbox get synced as new documents","description":"When a sandboxed process creates a new file that doesn't exist in the schema, the sync should:\n1. Detect the new file\n2. Create a new document on the server with a new UUID\n3. Add the file to the local schema\n4. Push the updated schema to the server\n\nNeed integration tests to verify this works correctly in sandbox mode.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T19:25:43.482719737Z","created_by":"jes","updated_at":"2026-01-03T07:16:33.1481434Z","closed_at":"2026-01-03T07:16:33.1481434Z","close_reason":"Verified in acceptance testing session (2026-01-03) - sandbox file creation syncs correctly with new UUIDs"}
{"id":"CP-cgi","title":"Fix P1: Use JSON replay when pushing Y.Map schema updates (PR #6)","description":"From PR #6 Codex review: The change switches the initial fs-root schema write to a Y.Map update but needs to use JSON replay instead for proper handling.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:21.54461-08:00","updated_at":"2025-12-26T23:39:41.921295-08:00","closed_at":"2025-12-26T23:39:41.921295-08:00","close_reason":"Already fixed in PR #7 (CP-8fz: Fix server replay to use node's content type). The replay.rs now handles ContentType::Json by initializing a map root and serializing via map.to_json(). Merged to main."}
{"id":"CP-ckq","title":"CRDT merge fails for concurrent edits from linked files","description":"When two files are linked to the same server document (via shared node_id), concurrent edits from both syncs produce incorrect merges.\n\n**Reproduction:**\n1. Set up file linking: content.txt ↔ prompts.txt (same node_id)\n2. Write 'test message' to content.txt\n3. Wait for sync to propagate to prompts.txt\n4. Clear BOTH files simultaneously\n5. Observe: server HEAD still has content, files get content restored\n\n**Expected:** Server HEAD should be empty, files should stay empty\n\n**Actual:** Server HEAD shows 15 bytes after both syncs uploaded 'delete 15 chars'\n\n**Log evidence:**\n- Uploaded: 0 chars inserted, 15 deleted (cid: bb01f73e)\n- Uploaded: 0 chars inserted, 15 deleted (cid: 502649a6)  \n- Wrote server content: 15 bytes at 502649a6\n\n**Root cause analysis:**\nThe sync protocol wasn't designed for multiple sync clients connected to the same document. Each sync maintains its own last_written_cid and parent tracking. When both upload with the same parent:\n1. First upload creates commit A (parent P)\n2. Second upload creates commit B (parent P, but HEAD is now A)\n3. Server enters merge path for B\n4. Merge computation produces wrong result\n\nThis is likely related to CP-f20 (race condition fix) but is a deeper issue with how CRDT updates are computed and applied in the merge path.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T09:36:52.554169483Z","created_by":"jes","updated_at":"2026-01-01T10:12:15.57286668Z","closed_at":"2026-01-01T10:12:15.57286668Z","close_reason":"Fixed in PR #72. Text diffs now use server's actual Yjs state via get_yjs_state() + compute_diff_update_with_base().","comments":[{"id":9,"issue_id":"CP-ckq","author":"jes","text":"## Root Cause Identified (Debug Session 2026-01-01)\n\nThrough debug logging, identified the actual root cause:\n\n### The Bug\nIn src/diff.rs, compute_diff_update creates a **fresh YDoc** with hardcoded client IDs:\n- base_doc = Doc::with_client_id(1)\n- target_doc = Doc::with_client_id(2)\n\n### Why This Fails\n1. Text diffs for the fast path use compute_diff_update(\u0026doc.content, new_content)\n2. This creates updates referencing character positions using client IDs 1 and 2\n3. The server's actual YDoc has characters inserted by **different** client IDs\n4. Yjs uses (client_id, clock) pairs to identify characters\n5. When the update tries to delete, it can't find matching characters\n6. **Result:** Delete operations become no-ops\n\n### Evidence from Logs\nAPPLY: doc content BEFORE update = 'test message' (13 bytes)\nAPPLY: doc content AFTER update = 'test message' (13 bytes)\nThe delete was applied but content didn't change!\n\n### The Fix\nText types need to use the server's actual Yjs state, like JSON types already do.\n\nChange in src/services/document.rs lines 635 and 651:\nFROM: diff::compute_diff_update(\u0026doc.content, new_content)\nTO: Use get_yjs_state(id) + compute_diff_update_with_base()","created_at":"2026-01-01T09:59:53Z"}]}
{"id":"CP-ckt","title":"Fix P1: URL-encode derived node IDs for nested files (PR #4)","description":"From PR #4 Codex review: When syncing directories, file.relative_path will include / characters which need to be URL-encoded for node IDs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:22.662002-08:00","updated_at":"2025-12-26T23:42:34.654703-08:00","closed_at":"2025-12-26T23:42:34.654703-08:00","close_reason":"Already fixed in PR #4 (043d0c2). The encode_node_id function was added to URL-encode node IDs containing slashes. All node ID usages in sync.rs now use this function. Merged to main."}
{"id":"CP-cofe","title":"P2: log.rs diff algorithm ignores duplicate lines","description":"In log.rs around lines 504-552, the set-based diff algorithm treats duplicate lines as identical, potentially producing incorrect diffs when the same line appears multiple times in different positions.\n\nLocation: src/bin/log.rs:504-552\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:10:00.576583994Z","created_by":"jes","updated_at":"2026-01-06T01:11:16.67287576Z","closed_at":"2026-01-06T01:11:16.67287576Z","close_reason":"Replaced HashSet-based diff with similar crate's TextDiff::from_lines for proper duplicate line handling"}
{"id":"CP-cr47","title":"New inode not tracked when commit_id is None","description":"In sse.rs:860-863, when head.cid is None (empty document), the new inode is never tracked:\n\n```rust\nif let Some(cid) = commit_id {\n    tracker.track(new_key, cid, file_path.clone());\n}\n```\n\nImpact: Subsequent atomic writes to this file won't create shadow hardlinks because the inode isn't in the tracker.\n\nOptions:\n1. Track with empty string as commit_id\n2. Track with a sentinel value like 'genesis'\n3. Document this as intentional (no shadow needed for empty docs?)\n\nRelated: CP-txg0 review","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-05T08:57:30.50056508Z","created_by":"jes","updated_at":"2026-01-06T01:22:42.432022866Z","closed_at":"2026-01-06T01:22:42.432022866Z","close_reason":"Track with empty string commit_id for genesis documents"}
{"id":"CP-cv9","title":"Validate at_commit CID belongs to requested document (P2 from codex review)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T06:23:26.603794724Z","created_by":"jes","updated_at":"2025-12-30T23:01:36.990246638Z","closed_at":"2025-12-30T23:01:36.990246638Z","close_reason":"Validated at_commit CID in get_head, fork_document, and replace_content. PR #52 merged.","labels":["api","security"],"comments":[{"id":2,"issue_id":"CP-cv9","author":"jes","text":"From codex review on PR #36 (src/api.rs:410):\n\nThe ?at_commit path replays commits solely by CID without verifying that the CID is part of the requested document's history. CommitReplayer::get_content_and_state_at_commit ignores the provided doc id, so a caller who knows a CID from another document can fetch that other document's state through /docs/:id/head?at_commit=... (or get misinterpreted content if content types differ).\n\nFix: Check that target_cid is reachable from the document head before replaying.","created_at":"2025-12-30T06:23:35Z"}]}
{"id":"CP-cxj","title":"Sync tool skips files with unknown extensions","description":"Ensure sync tool skips files with extensions it doesn't understand rather than failing or corrupting them.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T17:54:27.688193893Z","created_by":"jes","updated_at":"2025-12-30T18:09:02.091283573Z","closed_at":"2025-12-30T18:09:02.091283573Z","close_reason":"Already implemented - is_allowed_extension() function filters files by ALLOWED_EXTENSIONS constant, used in directory scanning and file event handlers"}
{"id":"CP-d24i","title":"Deduplicate workspace schema traversal helpers","description":"## Summary\nWorkspace schema traversal and path resolution logic is duplicated across CLI commands (link, uuid, replay) and workspace.rs. Extract shared helpers to reduce drift and keep schema traversal consistent.\n\n## Files to modify\n- src/workspace.rs (expose shared helpers)\n- src/bin/link.rs\n- src/bin/uuid.rs\n- src/bin/replay.rs\n\n## Implementation steps\n1. Move common helpers (find_workspace_root, find_schema_path, traverse_schema_path) into workspace.rs.\n2. Update CLI commands to call workspace.rs helpers instead of reimplementing traversal.\n3. Add tests in workspace.rs to cover root/subdir schema traversal.\n\n## Example\nBefore: each CLI command re-reads root schema and walks nested schemas.\nAfter: shared workspace::resolve_schema_path(...) used everywhere.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:30:16.044734439Z","created_by":"jes","updated_at":"2026-01-10T09:30:16.044734439Z"}
{"id":"CP-da9","title":"commonplace-ps showing stale/incorrect PIDs","description":"commonplace-ps displays PIDs that don't correspond to actual running processes.\n\nExample:\n```\njes@commonplace:~/commonplace/workspace/bartleby$ ../../target/release/commonplace-ps\nOrchestrator PID: 229603 (started at 2026-01-03 11:46:59)\n\nNAME                      PID STATE      CWD\n--------------------------------------------------------------------------------\nbartleby               230639 Running    -\nfile-tmux-file         229612 Running    -\ntext-to-telegram       793203 Running    -\n\njes@commonplace:~/commonplace/workspace/bartleby$ ps 792682\n    PID TTY      STAT   TIME COMMAND\n```\n\nThe PIDs shown don't match actual running processes. The orchestrator status file may be out of sync with reality, or PIDs are being read/stored incorrectly.","notes":"Additional issue: commonplace-ps should indicate when a process is rapidly restarting (crash loop). Currently it just shows 'Running' with stale PIDs, hiding the fact that the process is unstable.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T11:47:30.802260724Z","created_by":"jes","updated_at":"2026-01-03T12:23:55.78355672Z","closed_at":"2026-01-03T12:23:55.78355672Z","close_reason":"commonplace-ps now validates PIDs and shows Dead state for non-running processes"}
{"id":"CP-daj","title":"New files created after sync starts aren't added to schema - blocks acceptance","description":"When creating new files in the workspace while sync is running, the files aren't being added to the .commonplace.json schema and aren't synced to the server.\n\nTest case:\n1. Start server, sync, orchestrator\n2. Wait for initial sync to complete\n3. Create a new file: echo 'test' \u003e workspace/root-test-file.txt\n4. Wait 30 seconds\n5. Check workspace/.commonplace.json - new file not listed\n6. Check server schema - new file not present\n\nExpected: New files should be detected by file watcher, added to schema, and synced to server.\nActual: Files exist on disk but are never added to schema.\n\nNote: Files that existed before sync started ARE synced correctly. The bug affects only files created AFTER sync is running.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T08:22:05.305521305Z","created_by":"jes","updated_at":"2026-01-02T09:17:14.732591508Z","closed_at":"2026-01-02T09:17:14.732591508Z","close_reason":"Fixed. Two issues were causing new files to not be detected: 1) Path mismatch between absolute watcher paths and relative directory paths - fixed by canonicalizing both paths before strip_prefix. 2) Event debounce overwriting Created with Modified - fixed by preserving Created events in the debounce HashMap."}
{"id":"CP-deuz","title":"Migrate client.rs to use fetch_head helper (5 call sites)","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.430125659Z","created_by":"jes","updated_at":"2026-01-10T18:32:00.210341851Z","closed_at":"2026-01-10T18:32:00.210341851Z","close_reason":"Migrated 3 call sites (push_schema_to_server, delete_schema_entry, push_file_content). Left 2 retry-loop call sites (push_json_content_internal, push_jsonl_content) as they have complex retry logic interleaved with HEAD fetching."}
{"id":"CP-dgu","title":"Sync tool: persist state locally for offline change detection","description":"The sync tool currently has no local state persistence. On restart, it fetches HEAD from the server and may overwrite local changes made while sync was stopped.\n\n## Current Behavior\n- SyncState is purely in-memory (last_written_cid, last_written_content)\n- On startup, fetches server HEAD and writes to local file\n- --initial-sync flag offers limited control (skip/local/server) but no conflict detection\n- Local changes made while offline could be silently lost\n\n## Proposed Solution\nPersist sync state to a local file (e.g., .commonplace-sync.json or alongside each synced file):\n- Last synced CID\n- Last synced content hash\n- Timestamp of last sync\n\nOn restart:\n1. Load persisted state\n2. Compare local file hash to persisted hash → detect local modifications\n3. Fetch server HEAD → detect server modifications  \n4. If both modified: conflict (prompt user or use strategy flag)\n5. If only local modified: push to server\n6. If only server modified: pull to local\n7. If neither modified: resume normal sync\n\n## Files to modify\n- src/bin/sync.rs - Add state persistence and conflict detection\n- Possibly src/sync/ modules for shared types","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T05:08:27.003489287Z","created_by":"jes","updated_at":"2025-12-30T06:26:58.408628459Z","closed_at":"2025-12-30T06:26:58.408628459Z","close_reason":"Implemented sync state persistence. PR #36 merged. State file tracks last_synced_cid and file hashes. Detects offline changes on restart. CRDT merge logic deferred to follow-up.","comments":[{"id":1,"issue_id":"CP-dgu","author":"jes","text":"Design complete: docs/plans/2025-01-02-sync-state-persistence-design.md\n\nKey design decisions:\n- State file: .{basename}.commonplace-sync.json beside synced target (not inside)\n- Uses CRDT merge instead of manual conflict resolution\n- Stores last_synced_cid + file hashes, not content\n\nBLOCKING: Need to add ?at_commit query param to /head endpoint first (infrastructure exists via CommitReplayer, just not exposed)","created_at":"2025-12-30T05:45:25Z"}]}
{"id":"CP-dlu","title":"commonplace-ps not showing CWD of processes","description":"commonplace-ps shows '-' for all process CWDs instead of actual working directories.\n\nExample output:\n```\nNAME                      PID STATE      CWD                                     \n--------------------------------------------------------------------------------\nbartleby               230639 Running    -                                       \nfile-tmux-file         229612 Running    -                                       \ntext-to-telegram       775886 Running    -\n```\n\nExpected: Should show the actual working directory (sandbox temp dir or configured cwd) for each process.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T11:41:18.810220474Z","created_by":"jes","updated_at":"2026-01-03T12:23:54.790461891Z","closed_at":"2026-01-03T12:23:54.790461891Z","close_reason":"CWD now read from /proc/\u003cpid\u003e/cwd for processes without explicit cwd config"}
{"id":"CP-dnc","title":"Make sync robust when a process uses atomic-writes to edit a file","description":"The file watcher in src/sync/watcher.rs uses is_modify() to detect file changes. While this should catch rename events (since Modify(Name(To)) is a Modify variant), there are potential edge cases:\n\n1. Directory watcher (lines 179-209) explicitly handles rename modes - robust\n2. File watcher (lines 70-86) only checks is_modify() - less explicit\n\nInvestigate:\n- What happens if editor does delete+create instead of atomic rename?\n- Platform-specific inotify behavior (path vs inode watching)\n- Does watcher need to re-attach after file replacement?\n\nConsider:\n- Add explicit rename handling to file_watcher_task\n- Add integration tests with atomic write patterns\n- Test with vim, emacs, VSCode atomic saves","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T22:52:32.17908978Z","created_by":"jes","updated_at":"2025-12-31T23:04:38.726997837Z","closed_at":"2025-12-31T23:04:38.726997837Z","close_reason":"Watches parent directory for atomic writes, explicit rename handling, 4 integration tests"}
{"id":"CP-dpi7","title":"commonplace-ps shows parent CWD instead of sandbox CWD","description":"Summary: commonplace-ps reports the orchestrator's working directory for sandboxed processes; it should report the sandbox temp directory (e.g., /tmp/commonplace-sandbox-...).\n\nExample:\n$ commonplace ps\nOrchestrator PID: 1253246 (started at 2026-01-04 01:01:18)\n\nNAME                      PID STATE      CWD\n--------------------------------------------------------------------------------\nbartleby              1253324 Running    /home/jes/commonplace\nfile-tmux-file        1253325 Running    /home/jes/commonplace\nserver                1253251 Running    /home/jes/commonplace\nsync                  1253278 Running    /home/jes/commonplace\ntext-to-telegram      1253329 Running    /home/jes/commonplace\n\nExpected: sandboxed processes should show their sandbox temp directory (e.g., /tmp/commonplace-sandbox-...).\n\nFiles to modify:\n- src/orchestrator/manager.rs and/or src/orchestrator/discovered_manager.rs (CWD resolution)\n- src/bin/ps.rs (display logic if needed)\n\nImplementation steps:\n1. When process is sandboxed (commonplace-sync --sandbox), resolve CWD from the child/grandchild process that owns the sandbox.\n2. Prefer /proc/\u003cpid\u003e/cwd of the innermost sandboxed process rather than the orchestrator parent.\n3. Add a test or fixture to validate sandbox CWD reporting.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-04T09:28:17.618969624Z","created_by":"jes","updated_at":"2026-01-04T09:45:40.553451454Z","closed_at":"2026-01-04T09:45:40.553451454Z","close_reason":"Closed"}
{"id":"CP-dscp","title":"XML should use Yjs XmlFragment, not Text (replace/diff support)","description":"Summary: XML documents should be first-class via Yjs XmlFragment; the current workaround (CP-80z2) demotes XML to Text, which loses element-level CRDT semantics (structural merges/attributes). The diff/replace logic must handle XmlFragment correctly so XML gets proper collaborative editing.\\n\\nFiles to modify:\\n- src/sync/diff.rs (or XML/Yjs diff/replace module)\\n- src/sync/file_sync.rs\\n- src/tmux (XML document handling)\\n- tests (XML element-level merge coverage)\\n\\nImplementation steps:\\n1. Locate the workaround that converts XML docs to Text (CP-80z2) and identify the missing XmlFragment support path.\\n2. Implement diff/replace handling for Yjs XmlFragment, preserving element boundaries/attributes and structural edits.\\n3. Plumb XmlFragment through file-tmux-file without type coercion.\\n4. Ensure serialize/deserialize roundtrips maintain XML structure and CRDT ops.\\n5. Add regression tests that perform element-level edits and verify merges remain structural.\\n\\nExample:\\nBefore: \u003cp\u003e\u003cb\u003ehi\u003c/b\u003e\u003c/p\u003e becomes flat text; element merges are lost.\\nAfter: XmlFragment preserves \u003cb\u003e node operations and merges structurally.\\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T18:41:06.869049706Z","created_by":"jes","updated_at":"2026-01-07T19:21:07.594744668Z","closed_at":"2026-01-07T19:21:07.594744668Z","close_reason":"Implemented XmlFragment support in diff.rs with compute_xml_diff_update(). XML documents now use XmlFragment internally and the diff/replace operations generate proper XmlFragment updates instead of Text updates."}
{"id":"CP-dt3","title":"[blocks acceptance] File deletion doesn't propagate to sandboxes","description":"When a file is deleted from the workspace, the deletion doesn't propagate to sandbox sync clients.\n\nTest case:\n1. Create file in workspace/text-to-telegram/test-file.txt\n2. Verify file appears in sandbox (works)\n3. Delete workspace/text-to-telegram/test-file.txt\n4. Wait 5 seconds\n5. File still exists in sandbox (should be deleted)\n\nThe sync likely needs to handle schema removals and trigger file deletion in the synced directories.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T09:44:34.285517457Z","created_by":"jes","updated_at":"2026-01-02T10:01:03.072528079Z","closed_at":"2026-01-02T10:01:03.072528079Z","close_reason":"Fixed. Two issues: (1) handle_file_deleted was always pushing to fs_root_id - fixed to use find_owning_document to get correct owning document. (2) handle_schema_change wasn't handling deletions - added code to detect files in known_paths but not in schema_paths and delete the local files, stopping their sync tasks. Verified D1-D4 acceptance tests now pass."}
{"id":"CP-du5","title":"Sandbox path config inverted - blocks acceptance","description":"The processes.json files are in wrong locations, causing sandbox paths to be inverted from acceptance criteria:\n\nCurrent:\n- workspace/processes.json has text-to-telegram → gets full workspace (wrong)\n- workspace/bartleby/processes.json has bartleby → gets only bartleby/ (wrong)\n\nExpected (per docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md):\n- Bartleby should have full workspace tree → config at workspace/processes.json\n- Text-to-telegram should only have its subdir → config at workspace/text-to-telegram/processes.json\n\nFix: Swap the config locations.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T06:07:22.908171066Z","created_by":"jes","updated_at":"2026-01-02T08:14:16.959288554Z","closed_at":"2026-01-02T08:14:16.959288554Z","close_reason":"Fixed - processes.json in correct locations"}
{"id":"CP-dv7","title":"Directory sync enters feedback loop writing schemas repeatedly","description":"When running commonplace-sync --directory, the sync enters a feedback loop writing schemas repeatedly.\n\n**Root Cause Analysis:**\nThe semantic JSON comparison fix (PR #80) wasn't sufficient. The loop continues because:\n1. SSE events trigger handle_schema_change() which calls write_nested_schemas()\n2. write_nested_schemas() fetches and writes subdirectory schemas\n3. Even with deduplication, the content may differ due to:\n   - CRDT evolution in Yjs documents\n   - Different key ordering in JSON serialization\n   - scan_directory() producing different output than server content\n\n**Additional fixes needed:**\n1. Add deduplication to write_nested_schemas_recursive()\n2. Consider debouncing SSE events\n3. Or prevent the file watcher from triggering on schema files entirely\n4. Compare normalized schemas (sorted keys, consistent formatting)","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T22:07:42.612446584Z","created_by":"jes","updated_at":"2026-01-02T00:15:29.878363029Z","closed_at":"2026-01-02T00:15:29.878363029Z","close_reason":"Closed"}
{"id":"CP-dyku","title":"Deduplicate root document initialization across storage backends","description":"Summary: MemoryStorage, LevelStorage, and SqliteStorage all duplicate the same NULL_UUID/root document initialization (patching $ and $.entries); extract a shared helper to keep behavior consistent.\n\nFiles to modify:\n- commonplaced-2025/src/documents/storage/memoryStorage.ts\n- commonplaced-2025/src/documents/storage/levelStorage.ts\n- commonplaced-2025/src/documents/storage/sqliteStorage.ts\n- commonplaced-2025/src/documents/storage.ts (or new shared helper module)\n\nImplementation steps:\n1) Add a helper like ensureRootDocumentInitialized(store, docId) that patches $ and $.entries if missing.\n2) Update each storage backend initialize() to call the helper instead of duplicating the logic.\n3) Ensure helper handles view() failures safely and preserves existing entries.\n\nExample:\nBefore: three storage backends copy the same try/catch patch(\"$\") + patch(\"$.entries\") logic.\nAfter: initialize() calls ensureRootDocumentInitialized(rootDoc).","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:19:18.036489858Z","created_by":"jes","updated_at":"2026-01-10T10:19:34.840904267Z"}
{"id":"CP-e375","title":"Deduplicate http_gateway error mapping","description":"## Summary\nhttp_gateway/api.rs repeats StatusCode+String error tuples for JSON encoding/publish. Extract a shared helper for mapping errors to (StatusCode, String) to reduce duplication.\n\n## Files to modify\n- src/http_gateway/api.rs\n- src/http_gateway/errors.rs (new helper)\n\n## Implementation steps\n1. Add helper functions like bad_request_json_err and internal_err.\n2. Replace repeated map_err(|e| (StatusCode::BAD_REQUEST, e.to_string())) blocks.\n3. Keep status codes identical.\n\n## Example\nBefore: multiple map_err closures in each handler.\nAfter: http_gateway::errors::bad_request(e) used everywhere.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:05:13.821683127Z","created_by":"jes","updated_at":"2026-01-10T11:45:14.675464836Z","closed_at":"2026-01-10T11:45:14.675464836Z","close_reason":"Added bad_request() and internal_error() helpers to deduplicate error mapping in http_gateway handlers."}
{"id":"CP-e3h","title":"Sync doesn't push local file edits to server","description":"When editing files in a synced directory, changes are not being pushed to the server.\n\nTest case:\n1. Start sync with: commonplace-sync --server http://localhost:3000 --directory ./workspace --path \"\"\n2. Edit a file in workspace/bartleby/processes.json\n3. Wait for sync\n4. Check server content - unchanged\n\nExpected: Local edits should be detected by file watcher and pushed to server.\nActual: Server content remains unchanged after local edit.\n\nThe sync process is running and watching the directory, but edits aren't triggering pushes. This may be related to subdirectory schemas with node_id (node-backed directories) not properly watching nested files.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T04:49:53.510418943Z","created_by":"jes","updated_at":"2026-01-02T07:48:20.848924736Z","closed_at":"2026-01-02T07:48:20.848924736Z","close_reason":"Works as expected. Testing confirmed: 1) Directory mode file edits sync correctly to server, 2) Sandbox mode file edits sync correctly to server. The UUID resolution improvements from CP-ee1 fix (migrate_subdirectory_document) ensured files in node-backed subdirectories get correct UUIDs, enabling per-file watchers to push edits to the correct server document."}
{"id":"CP-e3jr","title":"Remove support for inline subdirectories","description":"Inline subdirectories are deprecated. migrate_inline_subdirectories() in reconciler.rs handles migration to node-backed style. Once all schemas are migrated, remove:\n\n1. Inline subdirectory parsing/handling code\n2. The migration code itself (or keep for safety)\n3. Any fallback paths that still check for inline entries\n\nFiles to audit:\n- src/fs/reconciler.rs (migration code)\n- src/path.rs (inline handling)\n- src/files.rs (entries.is_null checks)\n- src/sync/uuid_map.rs (inline entry handling)","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-06T00:55:16.282048143Z","created_by":"jes","updated_at":"2026-01-08T18:36:06.571193856Z","closed_at":"2026-01-08T18:36:06.571193856Z","close_reason":"Removed inline subdirectory support. All directories must now be node-backed. Root entries still traversed correctly.","labels":["future-work"]}
{"id":"CP-e3km","title":"Red broadcasts should use gold ordering metadata","description":"Summary: Route red-port broadcasts through the gold event-ordering mechanism so subscribers can deterministically order red events across transports.\n\nFiles to modify:\n- src/ws/protocol.rs\n- src/ws/room.rs\n- src/mqtt/events.rs\n- docs/ARCHITECTURE.md\n\nImplementation steps:\n1. Define the gold ordering token/sequence for red events (e.g., monotonic sequence or gold CID) once gold infra is available.\n2. Extend red event envelopes to include the ordering field for both WebSocket and MQTT paths.\n3. Update broadcast paths to mint/attach the ordering token at publish time.\n4. Update any red-event consumers to surface the ordering metadata and document the semantics.\n5. Add tests that emit multiple red events and assert ordering metadata increases/links correctly.\n\nExample:\nBefore: red event payload is unordered; clients may see events in different orders.\nAfter: red events include `order: \u003cgold_seq\u003e` so clients can sort consistently.\n","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-06T01:50:48.734309682Z","created_by":"jes","updated_at":"2026-01-06T01:50:48.734309682Z","dependencies":[{"issue_id":"CP-e3km","depends_on_id":"CP-g99w","type":"blocks","created_at":"2026-01-06T01:50:48.738801222Z","created_by":"jes"}]}
{"id":"CP-e5ag","title":"Deduplicate HeadResponse fetch logic","description":"## Summary\nFetching HeadResponse (build_head_url + GET + JSON parse + status checks) is duplicated throughout sync modules (file_sync, dir_sync, sse, uuid_map, schema_io). Extract a shared helper to standardize error handling and reduce boilerplate.\n\n## Files to modify\n- src/sync/ (file_sync.rs, dir_sync.rs, sse.rs, uuid_map.rs, schema_io.rs, client.rs)\n- src/sync/head.rs or src/sync/http.rs (new helper)\n\n## Implementation steps\n1. Add a helper like fetch_head(client, server, identifier, use_paths) -\u003e Result\u003cHeadResponse, SyncError\u003e.\n2. Replace repeated GET/parse patterns with the helper, preserving existing fallback behavior.\n3. Ensure empty schema \"{}\" handling stays consistent.\n\n## Example\nBefore: dozens of open-coded GET+parse sequences.\nAfter: shared fetch_head called everywhere.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:27:54.041268931Z","created_by":"jes","updated_at":"2026-01-10T18:16:12.221612938Z","comments":[{"id":31,"issue_id":"CP-e5ag","author":"jes","text":"Large refactor: 20+ call sites across 7 files. Error handling patterns vary - some use Result, some use match with status checks, some use if let Ok. Would need careful migration to avoid breaking edge cases. Deferring for focused session.","created_at":"2026-01-10T10:32:01Z"},{"id":38,"issue_id":"CP-e5ag","author":"jes","text":"Added fetch_head helper in client.rs. Full migration of 20+ call sites deferred - each site has different error handling patterns (?, match, if let). Helper is exported and ready for incremental adoption.","created_at":"2026-01-10T18:14:41Z"}]}
{"id":"CP-e7ez","title":"File deletion doesn't propagate to sandbox","description":"When a file is deleted from workspace, the schema is updated but the physical file in sandbox directories is not deleted. D2/D4 tests fail.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-10T20:49:11.155834457Z","created_by":"jes","updated_at":"2026-01-10T21:46:00.668780263Z","closed_at":"2026-01-10T21:46:00.668780263Z","close_reason":"Fixed: stack overflow with 8MB thread stack + async_recursion, file deletion with fetch_subdir_node_id"}
{"id":"CP-ebk","title":"Directory sync uses wrong /files path for node-backed subdirs","description":"Summary: In directory sync with --use-paths, new files in node-backed subdirectories are pushed to /files/\u003cbasename\u003e instead of /files/\u003csubdir\u003e/\u003cbasename\u003e, causing 404s and wrong SSE subscriptions.\n\nEvidence:\n- Creating workspace/bartleby/bingo.jsonl attempts /files/bingo.jsonl (404) while /files/bartleby/bingo.jsonl exists.\n- Logs show: 'Identifier bingo.jsonl not found, waiting for reconciler' and SSE connects to /sse/files/bingo.jsonl.\n\nFiles to modify:\n- src/sync/dir_sync.rs (handle_file_created identifier selection)\n\nImplementation steps:\n1. In handle_file_created, when use_paths=true, set identifier to full fs-root relative path (relative_path), not owning_doc.relative_path.\n2. Ensure SSE and upload targets use the same full path.\n3. Add a regression test: create file in node-backed subdir with --use-paths, verify requests go to /files/subdir/file.\n\nExample:\nBefore: workspace/bartleby/bingo.jsonl -\u003e /files/bingo.jsonl (404)\nAfter: workspace/bartleby/bingo.jsonl -\u003e /files/bartleby/bingo.jsonl","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T00:30:42.278407464Z","created_by":"jes","updated_at":"2026-01-03T00:34:51.774580189Z","closed_at":"2026-01-03T00:34:51.774580189Z","close_reason":"Fixed identifier to use full fs-root relative path when --use-paths is enabled"}
{"id":"CP-ee1","title":"File creation in node-backed subdirs not synced - blocks C1-C6 acceptance","description":"When creating a file in a node-backed subdirectory (e.g., workspace/text-to-telegram/test-file.txt), the sync client fails to propagate the file to the server.\n\nRoot cause: handle_file_created pushes the schema to fs_root_id (parent document), but the file belongs to the subdirectory's own schema document.\n\nExpected behavior: When file is created in path under a node-backed directory, push schema update to that subdirectory's document.\n\nCurrent behavior: Schema pushed to root document, subdirectory's schema unchanged, file never synced.\n\nBlocks acceptance criteria C1-C6.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T06:39:29.364037755Z","created_by":"jes","updated_at":"2026-01-02T07:41:39.756898109Z","closed_at":"2026-01-02T07:41:39.756898109Z","close_reason":"Fixed by: 1) Adding migrate_subdirectory_document() method that generates UUIDs for entries in node-backed subdirectory schemas; 2) Using last_valid_node_schemas cache to preserve previously generated UUIDs across multiple schema pushes; 3) Merging existing UUIDs before migration to prevent race conditions."}
{"id":"CP-ef0e","title":"Add MQTT topic for ancestry check","description":"HTTP has GET /docs/:id/is-ancestor to check if one commit is an ancestor of another. No MQTT equivalent.\n\nAdd to sync protocol as a new message type:\n- Request: {type: 'is_ancestor', req: '...', commit: '...', ancestor: '...'}\n- Response: {type: 'is_ancestor_response', req: '...', result: true/false}","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:22.078305825Z","created_by":"jes","updated_at":"2026-01-10T10:53:44.13626298Z","closed_at":"2026-01-10T10:53:44.13626298Z","close_reason":"Added IsAncestor/IsAncestorResponse to MQTT sync protocol. SyncHandler now handles is_ancestor requests using store.is_ancestor()."}
{"id":"CP-egb","title":"Sandboxed JS evaluator with document subscriptions and output","description":"Run a JS file in a sandbox with the ability to:\n- Declare dependencies on other commonplace documents\n- Subscribe to commits on those dependencies\n- Create a read-only output document\n- Listen for commands on the output document's path\n- Emit events as that document\n\nThe JS file acts as a reactive transform: inputs → computation → output document.","design":"Plan (v1)\n\nGoal\n- Run a JS file as a reactive transform: subscribe to dependency documents (read-only), own output document(s) (write), receive commands on the output path, and emit events.\n\nContext / Existing Docs\n- MQTT topic model and ports: docs/MQTT.md\n- External-process participation via MQTT: examples/python-client/README.md\n- MQTT auth direction (future): docs/MACAROONS.md (CP-adm)\n\nArchitecture\n- The evaluator is an external process that connects to the MQTT broker with a stable client_id.\n- For each dependency path:\n  - Subscribe to `{path}/edits`.\n  - Use `{path}/sync/{client_id}` to catch up (head + ancestors/pull) and reconstruct state.\n  - Maintain a local Yjs doc for the dependency so the script sees converged content.\n- For each owned output path:\n  - Subscribe to `{path}/commands/#`.\n  - Publish edits to `{path}/edits` (with parent commit context) and publish events to `{path}/events/\u003cname\u003e`.\n\nRuntime Choice\n- Preferred v1: Deno-based runner (permissions + TS path later).\n- Fallback v1: Node-based runner.\n- Future v2: embed a JS runtime in Rust (deno_core or quickjs) to avoid external runtime dependency.\n\nScript API (initial)\n- `cp.doc(path, { type })` returns a handle with:\n  - `get()` → current content (string for text, object/array for JSON)\n  - `set(value, { message })` → publish update\n  - `onChange(cb)` → callback on new converged content\n- `cp.onCommand(verb, handler)` registers a handler for `{output}/commands/{verb}`.\n- `cp.emit(event, payload)` publishes to `{output}/events/{event}`.\n\nConfiguration\n- Sidecar manifest next to script (v1): `\u003cscript\u003e.commonplace.json` describing:\n  - `deps`: list of doc paths + content types\n  - `outputs`: list of owned doc paths + content types\n  - `commands`: allowed verbs\n  - broker/server settings + client_id\n- Future: allow the manifest to live in a commonplace doc.\n\nOrchestrator Integration\n- Run via `processes.json` using `sandbox-exec`, so orchestrator launches:\n  - `commonplace-sync --sandbox --exec \"\u003crunner\u003e \u003cscript\u003e\"`\n- This gives a temp working directory with synced files and reduces accidental access to the host checkout.\n\nSandbox / Resource Limits\n- Deno runner flags (target shape): `--no-prompt`, narrow `--allow-net` to broker, allow only required env vars, deny subprocess.\n- Future: Podman-based isolation (CP-c7h) for stronger untrusted-code boundaries.\n\nMilestones\n1. Minimal runner: 1 dependency (text), 1 output (text), subscribe + publish edits.\n2. Add commands and events plumbing (magenta/red ports).\n3. Add JSON support using Y.Map/Y.Array semantics.\n4. Add manifest parsing and multi-dep/multi-output wiring.\n5. Observability: log per-doc head/last-applied CID and last command/event.\n6. Add an example transform + regression test harness.\n\nOpen Questions\n- Choose v1 runtime (Deno vs Node) based on deploy constraints.\n- Decide whether output ownership is enforced (deny non-owner publishes) or convention-only.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.433243-08:00","updated_at":"2026-01-10T02:50:06.741821959Z","closed_at":"2026-01-10T02:50:06.741821959Z","close_reason":"Completed","labels":["future-work"],"dependencies":[{"issue_id":"CP-egb","depends_on_id":"CP-nno","type":"blocks","created_at":"2025-12-28T22:50:33.552787-08:00","created_by":"daemon"}]}
{"id":"CP-eiyx","title":"Add MQTT wildcard subscriptions for directory trees","description":"**This is the key gap that blocks CP-o7h0 (sync refactor to MQTT).**\n\nCurrent MQTT topics are per-file: {workspace}/{path.ext}/{port}\nThe topic parser requires an extension, so you can't subscribe to:\n- workspace/docs/# (all files under docs/)\n- workspace/+/+.txt/edits (all .txt files)\n\nFor sync to use MQTT instead of SSE, we need to watch entire directory trees with one subscription.\n\nOptions:\n1. Change topic structure to support directory wildcards\n2. Add a separate 'schema' port that broadcasts when any file in a subtree changes\n3. Use directory node_ids in topics instead of paths\n\nThis is P1 because it's the architectural blocker for the MQTT sync refactor.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-09T07:05:34.837963327Z","created_by":"jes","updated_at":"2026-01-09T07:09:07.377581874Z","closed_at":"2026-01-09T07:09:07.377581874Z","close_reason":"Wrong approach. The real fix is CP-m4q7: change topic structure to {workspace}/{port}/{path} so wildcards work naturally without extension requirements."}
{"id":"CP-ejh","title":"Refactor api.rs: Extract document handlers to service layer","description":"api.rs is 918 lines. Extract document CRUD operations into a service layer module (`src/services/document.rs`), separating business logic from HTTP handler concerns.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-30T00:34:51.124071-08:00","updated_at":"2025-12-30T16:17:43.597788755Z","closed_at":"2025-12-30T16:17:43.597788755Z","close_reason":"Closed"}
{"id":"CP-eoy","title":"commonplace-link: support linking anywhere within a checked out file tree","description":"Currently commonplace-link only works for files in the same directory. For use cases like linking bartleby's workspace files to text-to-telegram files, we need cross-directory linking within a synced tree.\n\nProposed behavior:\n- commonplace-link source target (where both are in the same synced workspace tree)\n- Updates both files' schemas to share the same UUID\n- When either file is edited, both sync to the same document\n\nUse case: bartleby/prompts.txt linked to telegram/content.txt so telegram messages become bartleby prompts, and bartleby/output.txt linked to telegram/input.txt so bartleby responses go to telegram.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-31T23:23:03.804284034Z","created_by":"jes","updated_at":"2026-01-01T01:58:24.737984163Z","closed_at":"2026-01-01T01:58:24.737984163Z","close_reason":"Implemented cross-directory linking support. Commit 2c99666."}
{"id":"CP-eyi","title":"Cron-style recurring scheduled events","description":"Fire events on a recurring schedule (like crontab). Use cases:\n- Periodic document refresh triggers\n- Scheduled backup/export jobs\n- Regular polling of external sources\n- Time-based workflow triggers\n\nCould be defined as a special node type or as a configuration on documents.","design":"OPEN QUESTIONS for implementation:\n1. Where are schedules stored - in a special document, node metadata, or separate config?\n2. What format for cron expressions (standard cron or extended)?\n3. What event type is fired on schedule trigger?\n4. How to handle timezone - UTC only or configurable?\n5. Should missed events (server was down) be replayed or skipped?\n6. Is this a separate service or integrated into commonplace-store?","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-28T23:53:18.565001-08:00","updated_at":"2025-12-30T01:44:47.450426047Z","labels":["future-work"]}
{"id":"CP-f20","title":"Bidirectional CRDT sync fails for producer-consumer file patterns","description":"When two files are linked with shared UUIDs for bidirectional sync, deletions on one side get restored instead of propagating.\n\nExample: bartleby appends responses to output.txt, which syncs to text-to-telegram/input.txt. text-to-telegram pops (removes) lines after sending to Telegram. The sync should propagate this deletion back to output.txt, but instead the deleted lines get restored from output.txt.\n\nExpected behavior: Deletion in input.txt → Yjs delete operation → syncs to server → propagates to output.txt (line removed from both)\n\nActual behavior: Deletion in input.txt gets overwritten by sync restoring content from output.txt\n\nPossible causes:\n1. File watcher not detecting text-to-telegram's atomic write (_atomic_write via tempfile rename)\n2. Diff algorithm not generating proper delete operations\n3. Race condition: output.txt re-syncs before input.txt deletion propagates\n4. Sync direction priority issue - maybe 'initial-sync local' affects ongoing sync behavior\n\nImpact: Causes spam loops when using file linking for message passing between applications.","notes":"Theory: SSE server edits are skipped when local content differs from last_written_content, but the sync client only refreshes HEAD when an echo is detected. In producer-consumer, consumer deletes lines → SSE to producer; producer has local pending changes (append or atomic write) so handle_server_edit sets needs_head_refresh and returns. upload_task clears needs_head_refresh when processing the local change but only calls refresh_from_head on echo; on a real local upload it never refreshes. That means the producer uploads its stale content (with deleted lines) and overwrites the deletion. See src/sync/sse.rs:229-292 (skip server edit on local changes sets needs_head_refresh) and src/sync/file_sync.rs:120-210 (needs_head_refresh cleared but only used on echo path). Fix likely: after any successful upload where needs_head_refresh was set, refresh from HEAD (or merge) so server edits aren't lost.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T07:47:10.941526886Z","created_by":"jes","updated_at":"2026-01-01T08:15:49.177770661Z","closed_at":"2026-01-01T08:15:49.177770661Z","close_reason":"Fixed in PR #71. Re-check needs_head_refresh after upload to catch SSE events that arrived during upload."}
{"id":"CP-f4j7","title":"Deduplicate fs-root discovery helper","description":"## Summary\nfs-root discovery via GET /fs-root is duplicated in sync.rs, cbd.rs, and orchestrator.rs. Extract a shared helper to keep error handling and parsing consistent.\n\n## Files to modify\n- src/bin/sync.rs\n- src/bin/orchestrator.rs\n- src/cbd.rs\n- src/sync/paths.rs or src/client.rs (new shared helper)\n\n## Implementation steps\n1. Add a shared discover_fs_root(client, server) helper returning fs-root ID.\n2. Replace local implementations with the shared helper.\n3. Keep error messages consistent (including handling of 503 when fs-root not configured).\n\n## Example\nBefore: multiple GET /fs-root implementations.\nAfter: shared helper used everywhere.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:33:19.305226639Z","created_by":"jes","updated_at":"2026-01-10T09:33:19.305226639Z"}
{"id":"CP-fd60","title":"Orchestrator should watch commonplace.json and reload on changes","description":"The orchestrator reads commonplace.json at startup but doesn't watch it. Should use inotify/fswatch to detect changes and dynamically add/remove/restart processes. This is the external unix file, distinct from processes.json within workspace.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T06:43:59.281537076Z","created_by":"jes","updated_at":"2026-01-05T08:51:13.610952713Z","closed_at":"2026-01-05T08:51:13.610952713Z","close_reason":"Implemented config file watching with inotify, automatic reload on changes, added stop_process and reload_config methods to ProcessManager"}
{"id":"CP-fed5","title":"Deduplicate file_states map handling","description":"## Summary\nFileSyncState map access patterns (read/write/insert/remove) are repeated in dir_sync.rs and file_events.rs. Extract helper functions to manage file_states updates consistently.\n\n## Files to modify\n- src/sync/dir_sync.rs\n- src/sync/file_events.rs\n- src/sync/file_state.rs (new helper)\n\n## Implementation steps\n1. Add helpers for insert_file_state, remove_file_state, list_file_states_by_prefix.\n2. Replace direct RwLock access blocks with helper calls.\n3. Keep logging and behavior unchanged.\n\n## Example\nBefore: multiple blocks manually lock file_states and insert/remove entries.\nAfter: file_state::insert(file_states, state) used everywhere.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:56:19.641961402Z","created_by":"jes","updated_at":"2026-01-10T09:56:19.641961402Z"}
{"id":"CP-fh9h","title":"Smart .commonplace.json sync - detect user edits vs our writes","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-10T02:22:08.676072007Z","created_by":"jes","updated_at":"2026-01-10T03:02:46.693058925Z","closed_at":"2026-01-10T03:02:46.693058925Z","close_reason":"Implemented written_schemas tracking to distinguish user edits from sync client writes to .commonplace.json files","comments":[{"id":24,"issue_id":"CP-fh9h","author":"jes","text":"Currently .commonplace.json files are completely ignored by the watcher to prevent feedback loops. But this prevents user edits (like linking files) from being pushed to the server.\n\nSolution: Track what we write to schema files (like we do with last_written_content for regular files). When a .commonplace.json change is detected, compare with what we last wrote - if different, it's a user edit and should be pushed to server.","created_at":"2026-01-10T02:22:24Z"}]}
{"id":"CP-fi7t","title":"Potential race: exec may start before initial sync completes","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-10T22:51:00.210129245Z","created_by":"jes","updated_at":"2026-01-10T22:51:00.210129245Z","comments":[{"id":40,"issue_id":"CP-fi7t","author":"jes","text":"Observed behavior: bartleby sync process is stuck doing initial file sync for minutes (syncing tmux files). The exec never starts because initial sync takes too long.\n\nThe tmux directory has many files and the sync is processing them one by one before starting the exec. This is a different bug - initial sync should be faster or exec should start earlier.","created_at":"2026-01-10T22:56:53Z"}]}
{"id":"CP-fmv6","title":"Migrate schema_io.rs to use fetch_head helper (2 call sites)","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.383060176Z","created_by":"jes","updated_at":"2026-01-10T18:26:41.014507024Z","closed_at":"2026-01-10T18:26:41.014507024Z","close_reason":"Migrated 2 call sites to use fetch_head helper"}
{"id":"CP-fr4","title":"UUID-linked files in subdirectories not syncing - blocks acceptance","description":"Files linked via commonplace-link (sharing the same UUID) are not syncing content between each other.\n\nExample:\n- workspace/bartleby/output.txt and workspace/text-to-telegram/input.txt share UUID 12504d60-2b58-43c8-b461-a42215a3954d\n- output.txt contains 'sync test 1767417213'\n- input.txt contains 'hi'\n- They should have identical content but don't converge\n\nThe server shows old content while local files have newer content. The workspace sync appears to not be pushing changes from files in subdirectories that have linked UUIDs.\n\nThis blocks the sandbox linking architecture described in docs/SANDBOX_LINKING.md.","notes":"Evidence in /tmp/sync.log: sync resolved bartleby/output.txt and text-to-telegram/input.txt to derived IDs (no UUID found). Lines 2026-01-03T05:17:39/05:17:41 show 'No UUID found for bartleby/output.txt, using derived ID ...' and 'No UUID found for text-to-telegram/input.txt, using derived ID ...'. This means the link mapping isn't in schema after restart, so files aren't actually linked.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T05:18:16.153599126Z","created_by":"jes","updated_at":"2026-01-03T05:38:15.705702651Z","closed_at":"2026-01-03T05:38:15.705702651Z","close_reason":"Fixed by CP-a31: commonplace-link now pushes schemas to server, sync picks up correct UUIDs, linked files sync correctly."}
{"id":"CP-fs3x","title":"Subdirectory SSE events don't trigger file syncing for remote changes","design":"When files are added/changed in a node-backed subdirectory on the server, the subdir_sse_task receives an edit event but only does cleanup (orphan removal), not file syncing. This was already broken before CP-j4wg (it was using wrong parameters). A dedicated mechanism is needed to pull remote file changes in subdirectories. Options: 1) Add periodic resync for subdirs, 2) Modify handle_schema_change to accept path prefix, 3) Implement dedicated subdir file sync.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T22:21:43.09159333Z","created_by":"jes","updated_at":"2026-01-07T00:15:39.150829137Z","closed_at":"2026-01-07T00:15:39.150829137Z","close_reason":"Added handle_subdir_new_files() to sync NEW files from subdirectory schemas when SSE edit events are received. Fixed locally-deleted directory detection to use root-level synced dirs. Fixed identifier to use path when use_paths=true. Merged in PR #107."}
{"id":"CP-g0c","title":"Sync corrupts .commonplace.json schema - overwrites to {} or {version:1}","description":"When running commonplace-sync with --directory and --node, the local .commonplace.json schema gets corrupted. It starts with a proper schema (version, root, entries with files) but gets repeatedly overwritten to just {} or {\"version\":1}. The fs-root document on the server ends up with content: {} instead of the full schema. This breaks directory sync completely.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T01:49:36.833268107Z","created_by":"jes","updated_at":"2026-01-01T01:54:58.343768111Z","closed_at":"2026-01-01T01:54:58.343768111Z","close_reason":"Fixed by validating schema before writing to local file. Commit 759ddfb."}
{"id":"CP-g3zv","title":"cbd: document usage and intentional bd differences","description":"Summary: Document cbd usage, config discovery, and which bd features are intentionally omitted in JSONL-only mode.\n\nFiles to modify:\n- docs/DEVELOPMENT.md or README.md\n\nImplementation steps:\n1. Add a cbd section with command examples for list/ready/show/create/update/close/dep.\n2. Document config discovery via .cbd.json and .beads/.cbd.json, plus env vars COMMONPLACE_SERVER and CBD_PATH.\n3. Call out intentional differences from bd (no sqlite/daemon flags, no db sync modes) and why.\n4. Add a short JSON output example for list and show.\n\nExample:\ncommonplace-bd --path beads/commonplace-issues.jsonl ready --json\n","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-05T07:02:56.617452765Z","created_by":"jes","updated_at":"2026-01-05T08:42:57.869975828Z","closed_at":"2026-01-05T08:42:57.869975828Z","close_reason":"Added cbd documentation to docs/DEVELOPMENT.md with command examples, config discovery, JSON output, and bd comparison","dependencies":[{"issue_id":"CP-g3zv","depends_on_id":"CP-vyd6","type":"blocks","created_at":"2026-01-05T07:02:56.624024151Z","created_by":"jes"},{"issue_id":"CP-g3zv","depends_on_id":"CP-2gv7","type":"blocks","created_at":"2026-01-05T07:02:56.627915987Z","created_by":"jes"},{"issue_id":"CP-g3zv","depends_on_id":"CP-2ee8","type":"blocks","created_at":"2026-01-05T07:02:56.631383702Z","created_by":"jes"},{"issue_id":"CP-g3zv","depends_on_id":"CP-1j5m","type":"discovered-from","created_at":"2026-01-05T07:02:56.637827079Z","created_by":"jes"}]}
{"id":"CP-g99w","title":"Block-chain like functionality (\"gold\")","description":"Future exploration: Add blockchain-like functionality to commonplace, potentially called \"gold\". Details TBD.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T23:41:42.881279591Z","created_by":"jes","updated_at":"2026-01-05T23:41:42.881279591Z","labels":["future-work"]}
{"id":"CP-g9e8","title":"Add MQTT topic for document deletion","description":"HTTP has DELETE /docs/:id but there's no MQTT equivalent.\n\nAdd a commands topic verb for deletion:\n- Topic: {workspace}/{path}/commands/delete\n- Or add to sync protocol as a new message type\n\nThis allows MQTT clients to delete documents without HTTP.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:03.416287569Z","created_by":"jes","updated_at":"2026-01-09T08:22:05.417169284Z","closed_at":"2026-01-09T08:22:05.417169284Z","close_reason":"Added MQTT delete-document command: topic {workspace}/commands/delete-document with DeleteDocumentRequest/Response message types"}
{"id":"CP-gcrf","title":"Migrate file_sync.rs to use fetch_head helper (6 call sites)","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.18169264Z","created_by":"jes","updated_at":"2026-01-10T18:18:17.18169264Z"}
{"id":"CP-ggju","title":"Deduplicate URL path encoding helpers across TS clients","description":"Summary: TS clients build /files and /dir URLs with inline encodeURIComponent calls in multiple files; add a shared helper to ensure consistent path encoding.\n\nFiles to modify:\n- commonplaced-2025/src/actors/common/api-client.ts\n- commonplaced-2025/src/actors/common/sse-subscription-provider.ts\n- commonplaced-2025/src/system-actor/system-actor.ts\n- commonplaced-2025/src/server/operations.ts (if using shared helper there too)\n- commonplaced-2025/src/server/http-utils.ts or new utility module\n\nImplementation steps:\n1) Add shared URL helper(s) like encodeDocId(docId) and encodePath(path) that preserve slash segments where needed.\n2) Replace inline encodeURIComponent usages in TS clients with the helper(s).\n3) Update any tests (if present) or add small unit test coverage for special characters.\n\nExample:\nBefore: build URLs inline, e.g. base + '/files/' + encodeURIComponent(path).\nAfter: base + '/files/' + encodePath(path) using a shared helper.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:13:31.323465539Z","created_by":"jes","updated_at":"2026-01-10T10:14:09.50551349Z"}
{"id":"CP-gis9","title":"SDK cp.watch does not honor + wildcard","description":"The cp.watch API advertises MQTT + and # wildcards, but the underlying subscribe function only handles exact topics or # prefixes and never matches +. Any watcher registered with a + (e.g., cp.watch(\"workspace/+/edits\", …)) will never fire. Consider expanding the subscribe matching logic to handle single-level + wildcards or drop the claim in the watch API.","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-07T20:25:51.760641211Z","created_by":"jes","updated_at":"2026-01-07T20:57:15.210360363Z","closed_at":"2026-01-07T20:57:15.210360363Z","close_reason":"Fixed by adding topicMatches() function that properly handles both + and # MQTT wildcards."}
{"id":"CP-gl6m","title":"Make commonplace-log behave like git log (newest first, show diffs by default)","description":"Summary: Update commonplace-log output to mirror git log behavior.\n\nAlready done:\n- Newest commits first (fixed in CP-92l)\n\nTo implement:\n1. **Pager support**: Pipe output through pager (less/PAGER) for interactive sessions\n2. **Show diffs by default**: Like git log -p, show content changes between commits\n3. **--no-patch/-s**: Flag to suppress diff output (summary only)\n4. **Colorized output**: Commit IDs, dates, diff hunks\n\nFiles to modify:\n- src/bin/log.rs (pager, diff output)\n\nExample:\n```\ncommonplace log workspace/file.txt\n→ newest first, colorized, with diffs, through pager\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T20:42:14.214758441Z","created_by":"jes","updated_at":"2026-01-03T20:56:06.677094668Z","closed_at":"2026-01-03T20:56:06.677094668Z","close_reason":"Added pager support (less -R by default for TTY), colored diff output by default, --no-patch and --no-pager flags, -p and -u flag support"}
{"id":"CP-gt4b","title":"Share decorator parsing helpers between trigger-parser and dataflow-parser","description":"Summary: trigger-parser.ts and dataflow-parser.ts both parse JavaScript with Babel and traverse class methods/decorators; extract shared parsing helpers to reduce duplicated AST handling and error reporting.\n\nFiles to modify:\n- commonplaced-2025/src/actors/common/trigger-parser.ts\n- commonplaced-2025/src/actors/common/dataflow-parser.ts\n- commonplaced-2025/src/actors/common (new shared parser helper module)\n\nImplementation steps:\n1) Identify shared parsing setup (Babel parse config, class method traversal, decorator extraction).\n2) Create a helper that yields decorated method info (method name, decorator nodes, source location).\n3) Update both parsers to use the shared helper and keep their decorator-specific mapping logic.\n\nExample:\nBefore: both parsers manually call parse() and traverse() and loop decorators.\nAfter: shared helper returns a list of decorated methods, and each parser only maps decorators into trigger/dataflow definitions.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:15:10.611575204Z","created_by":"jes","updated_at":"2026-01-10T10:15:27.760414169Z"}
{"id":"CP-hbnl","title":"file-tmux-file sync failing with 404s on derived document IDs","description":"The file-tmux-file sync client is getting 404 errors when trying to commit files. It's using derived document IDs like 'uuid:0/content.txt' that the server doesn't recognize. Example from logs: 'Connecting to SSE: http://localhost:3000/sse/docs/234524b7-0c54-451c-bb6f-3fb01329ec61%3A0%2Fcontent.txt' (URL-decoded: uuid:0/content.txt). This prevents changes in the file-tmux-file sandbox from syncing to workspace/tmux/0.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-09T17:04:20.3672504Z","created_by":"jes","updated_at":"2026-01-09T17:55:00.332462812Z","closed_at":"2026-01-09T17:55:00.332462812Z","close_reason":"Fixed by adding build_uuid_map_and_write_schemas function that writes nested schema files during uuid_map build, ensuring find_owning_document can correctly resolve deep nested directories"}
{"id":"CP-hi7p","title":"Deduplicate workspace path normalization in CLI","description":"## Summary\nWorkspace-relative path normalization is duplicated across CLI binaries (link/uuid/replay) and workspace.rs. Consolidate to use workspace::normalize_path and keep error handling consistent.\n\n## Files to modify\n- src/bin/link.rs\n- src/bin/uuid.rs\n- src/bin/replay.rs\n- src/workspace.rs (reuse normalize_path, possibly expose helper for CLI)\n\n## Implementation steps\n1. Remove local normalize_path implementations in CLI binaries.\n2. Use workspace::normalize_path for resolving CLI paths to workspace-relative strings.\n3. Adjust error mapping to surface WorkspaceError cleanly.\n\n## Example\nBefore: each CLI binary duplicates normalize_path.\nAfter: shared helper in workspace.rs used everywhere.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:35:14.053331247Z","created_by":"jes","updated_at":"2026-01-10T12:03:52.566578205Z","closed_at":"2026-01-10T12:03:52.566578205Z","close_reason":"Removed local normalize_path from link.rs, uuid.rs, replay.rs; now use workspace::normalize_path"}
{"id":"CP-hivq","title":"P2: URL-encode evaluate script URL","description":"The script URL is assembled by concatenating document_path and script without URL encoding. Spaces or special chars in paths will break the URL.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T06:34:05.307301794Z","created_by":"jes","updated_at":"2026-01-07T01:10:34.118655827Z","closed_at":"2026-01-07T01:10:34.118655827Z","close_reason":"Added encode_url_path() helper to URL-encode path segments in evaluate script URLs"}
{"id":"CP-hm9","title":"MCP server: Keep MQTT event loop alive until publish is delivered","design":"From Codex review on PR #31: The event loop is capped at 2 seconds, so if broker connection takes longer, the publish() call only enqueues the packet and the loop stops before it can be sent/acked. fire_command returns success even if command was not delivered. Consider keeping a persistent MQTT client or waiting for ConnAck/PubAck before reporting success. File: src/bin/mcp.rs:73","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-30T02:22:36.905947547Z","created_by":"jes","updated_at":"2026-01-03T08:25:57.6795406Z","closed_at":"2026-01-03T08:25:57.6795406Z","close_reason":"MCP server now properly waits for ConnAck and PubAck before reporting success. Fixed the race condition where fire_command could return success before the message was delivered."}
{"id":"CP-hmkf","title":"Deduplicate scan_directory + schema_to_json usage","description":"## Summary\nRepeated scan_directory + schema_to_json sequences appear across file_events, file_sync, and dir_sync when pushing schema updates. Extract a shared helper to generate schema JSON from a directory with consistent error handling.\n\n## Files to modify\n- src/sync/file_events.rs\n- src/sync/file_sync.rs\n- src/sync/dir_sync.rs\n- src/sync/directory.rs (add helper like scan_directory_json)\n\n## Implementation steps\n1. Add scan_directory_json(path, options) -\u003e Result\u003cString, ScanError\u003e that wraps scan_directory + schema_to_json.\n2. Replace repeated scan+serialize blocks with the helper.\n3. Keep logging/behavior unchanged.\n\n## Example\nBefore: 6+ places call scan_directory then schema_to_json.\nAfter: call scan_directory_json(...).","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:55:26.958755205Z","created_by":"jes","updated_at":"2026-01-10T12:27:49.415767072Z","closed_at":"2026-01-10T12:27:49.415767072Z","close_reason":"Added scan_directory_to_json helper to directory.rs. Updated file_events.rs (3 call sites) and file_sync.rs (1 call site) to use the new helper, reducing nesting. dir_sync.rs unchanged since it needs the FsSchema object."}
{"id":"CP-hp5b","title":"Deduplicate base64 helpers","description":"## Summary\nBase64 helpers are duplicated: sync/yjs.rs defines base64_encode/base64_decode while core code uses crate::b64. Consolidate to a single base64 implementation to avoid divergent behavior and error types.\n\n## Files to modify\n- src/sync/yjs.rs (use crate::b64)\n- src/b64.rs (extend if needed, or re-export)\n- src/sync/mod.rs (update exports)\n\n## Implementation steps\n1. Replace sync/yjs.rs base64_encode/base64_decode with crate::b64::encode/decode.\n2. Update call sites/tests to use the unified helper and error type.\n3. Keep behavior compatible with existing update encoding (padding/whitespace handling).\n\n## Example\nBefore: two independent base64 implementations.\nAfter: sync and core share crate::b64.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:29:19.422051216Z","created_by":"jes","updated_at":"2026-01-10T10:31:02.136585335Z","closed_at":"2026-01-10T10:31:02.136585335Z","close_reason":"Replaced sync/yjs.rs base64_encode/base64_decode with crate::b64::encode/decode. Single base64 implementation now shared across codebase."}
{"id":"CP-hqet","title":"Hardcoded client_id=2 in diff.rs causes CRDT corruption on concurrent edits","description":"## Summary\nAll diff computations in diff.rs use hardcoded `Doc::with_client_id(2)`. When two concurrent replace requests arrive, both generate Yjs updates claiming to be from client_id=2 with potentially overlapping/conflicting clocks. Yjs expects unique client IDs - when it sees two updates from the 'same' client with conflicting timelines, it interleaves characters incorrectly.\n\n## Reproduction\n1. Two sync clients (bartleby, text-to-telegram) share a linked file\n2. Both make edits simultaneously (append vs delete-first-line)\n3. Both syncs send replace requests to server within same time window\n4. Server computes two diffs, both with client_id=2\n5. Updates merge incorrectly, producing garbled text like 'All onttu firesnare g still'\n\n## Root Cause\nIn diff.rs, all these functions use hardcoded client_id:\n- compute_diff_update: `Doc::with_client_id(2)`\n- compute_diff_update_with_base: `Doc::with_client_id(2)`\n- compute_xml_diff_update: `Doc::with_client_id(2)`\n- compute_xml_diff_update_with_base: `Doc::with_client_id(2)`\n\n## Fix\nUse unique client IDs for each diff computation:\n- `Doc::new()` generates random client ID\n- Or use `Doc::with_client_id(rand::random::\u003cu64\u003e())`\n\n## Files\n- src/diff.rs (4 functions need fix)","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T17:05:14.709705766Z","created_by":"jes","updated_at":"2026-01-10T17:15:23.342922108Z","closed_at":"2026-01-10T17:15:23.342922108Z","close_reason":"Fixed by using Doc::with_client_id(1) for base_doc (predictable char IDs) and Doc::new() for target_doc (unique random ID prevents concurrent corruption). Tests pass."}
{"id":"CP-htfn","title":"Extract schema I/O into schema_io.rs","description":"Move write_schema_file, write_nested_schemas, write_nested_schemas_recursive from dir_sync.rs into a new schema_io.rs module. Pure extraction, no behavior change.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T07:53:58.305996983Z","created_by":"jes","updated_at":"2026-01-10T08:19:49.175888269Z","closed_at":"2026-01-10T08:19:49.175888269Z","close_reason":"Extracted write_schema_file, write_nested_schemas, write_nested_schemas_recursive and SCHEMA_FILENAME to new schema_io.rs module. Reduces dir_sync.rs by ~220 lines."}
{"id":"CP-hxw6","title":"Rename processes.json to __processes.json for visibility","description":"The processes.json file within workspace directories looks too mundane. Rename to __processes.json to make it stand out as a special system file, similar to __pycache__ or __init__.py conventions.","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-05T06:44:09.264086032Z","created_by":"jes","updated_at":"2026-01-05T07:05:56.304854344Z","closed_at":"2026-01-05T07:05:56.304854344Z","close_reason":"Renamed processes.json to __processes.json throughout codebase: code, comments, log messages, and workspace files. Build, clippy, and tests pass."}
{"id":"CP-i0v","title":"File link UUIDs get overwritten when orchestrator restarts","description":"When the orchestrator restarts with a fresh database, the server's fs reconciler generates new UUIDs for entries instead of using the UUIDs from the local .commonplace.json schema.\n\n**Expected:** The linked UUIDs (e.g., aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa) should be preserved from the local schema.\n\n**Actual:** Reconciler creates new UUIDs, destroying file links.\n\n**Log evidence:**\n- Reconciler created document: bartleby/prompts.txt -\u003e afd6bda9-2d8c-479b-a64d-0d4a03b90def\n- Should have used: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa (from local schema)\n\n**Root cause hypothesis:**\nThe server's fs reconciler runs before sync can push the local schema. With --initial-sync local, the sync should push first, but there's a race condition or ordering issue.\n\n**Workaround:** None currently - manual schema restoration required after each restart.","notes":"Theory: On restart with fresh DB, sync scans the directory and only preserves UUIDs if it can parse local .commonplace.json. If that file is missing/unreadable at startup (or path mismatch), scan_directory generates schema without node_id fields. That schema is pushed first (initial-sync local), so the server reconciler assigns fresh UUIDs. Immediately after, sync builds uuid_map from the server schema and uses those IDs for file sync, effectively overwriting the intended link UUIDs. Root cause is startup ordering/availability of .commonplace.json (or parse failure) rather than reconciler ignoring node_id.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T10:33:03.4426906Z","created_by":"jes","updated_at":"2026-01-01T11:18:07.089985344Z","closed_at":"2026-01-01T11:18:07.089985344Z","close_reason":"Fixed in PR #73. Nested schemas for node-backed directories are now persisted locally and restored on startup with --initial-sync local.","comments":[{"id":10,"issue_id":"CP-i0v","author":"jes","text":"Root cause found: The reconciler's migrate_inline_subdirectories() generates new UUIDs for inline subdirectories (line 521 in reconciler.rs) when migrating them to node-backed format. \n\nThe fix: The reconciler should NOT migrate inline directories to node-backed format. The inline format with explicit node_ids is the correct representation for file linking. Migration should only generate UUIDs for entries that don't have them, not restructure the schema.\n\nAlternatively, we could make migration opt-in or disable it entirely when the schema has inline entries with node_ids.","created_at":"2026-01-01T10:53:03Z"}]}
{"id":"CP-i7c1","title":"P2: Config reload ignores dependency order","description":"In manager.rs around lines 535-556, when reloading config and starting new processes, the code doesn't respect the depends_on order. New processes may start before their dependencies are ready.\n\nLocation: src/orchestrator/manager.rs:535-556\nFound by: codex review","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-05T09:09:53.825787284Z","created_by":"jes","updated_at":"2026-01-06T01:19:43.299495213Z","closed_at":"2026-01-06T01:19:43.299495213Z","close_reason":"Use startup_order() for new/changed processes in reload_config"}
{"id":"CP-ia7","title":"CRDT merge corrupts JSON version field","description":"When pushing schema updates to an fs-root document that already has content, the CRDT merge can corrupt the JSON. Specifically, the 'version' field which should be integer 1 becomes string 've'. This appears to be a character-level diff/merge issue where {\"version\": 1} gets partially merged and produces garbage.\n\nThe corruption causes reconciler to fail with 'JSON parse error: invalid type: string \"ve\", expected u32'.\n\nWorkaround: only push to empty fs-root documents.\n\nReproduction:\n1. Start server with --fs-root and --database\n2. Run sync with --initial-sync local (pushes schema)\n3. Modify local .commonplace.json\n4. Run sync again with --initial-sync local\n5. Server schema now has corrupted version field","notes":"Diagnosis: DocumentStore::set_content uses diff::compute_diff_update (Y.Text char diff) regardless of content type. This is called by fs reconciler when migrating inline subdirectories and when updating fs-root schema. For JSON docs (ContentType::Json/JsonArray/Jsonl), applying a text-based Yjs update corrupts the underlying Y.Map/Y.Array state. That corrupted Yjs state then merges with later schema edits and can turn numeric fields into junk strings (e.g. version=\"ve\").\\n\\nLikely fix: make set_content content-type aware. For JSON/JSONL/JsonArray, generate a Yjs update with create_yjs_json_update/create_yjs_jsonl_update (using current ydoc state as base) instead of text diff; keep text diff only for Text/Xml. Alternatively route reconciliation updates through DocumentService::replace_content so JSON merge logic is used.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T06:13:37.424739978Z","created_by":"jes","updated_at":"2026-01-01T06:56:57.43341078Z","closed_at":"2026-01-01T06:56:57.43341078Z","close_reason":"Fixed in PR #70. set_content now uses content-type aware Yjs updates."}
{"id":"CP-id2","title":"Orchestrator spawns processes outside sandbox that become orphaned","description":"The orchestrator spawned text-to-telegram processes (PIDs 229628, 229633) at 07:45 that ran outside the sandbox and survived as orphans.\n\nThese orphaned processes then blocked new sandbox instances from starting (text-to-telegram detects duplicate instance and exits).\n\nTimeline:\n- 07:45: Orchestrator started (PID 229603)\n- 07:45: file-tmux-file started (229612) - in sandbox, works\n- 07:45: bartleby started (230639) - in sandbox, works  \n- 07:45: text-to-telegram started (229628, 229633) - NOT in sandbox, orphaned\n- 11:49: New text-to-telegram attempts keep failing because orphans block them\n\nThe orphaned processes had no parent commonplace-sync wrapper, suggesting they were spawned directly rather than through the sandbox mechanism.\n\nRelated: CP-a0g (orphaned processes survive restarts)","notes":"Root cause identified: commonplace.json contains OLD format config with 'command' field for text-to-telegram (spawns directly), while workspace/text-to-telegram/processes.json has NEW 'sandbox-exec' format.\n\nThe orchestrator likely loads BOTH configs, causing duplicate spawns:\n1. From commonplace.json: spawns via 'command' field → runs directly → orphans\n2. From recursive discovery: finds processes.json → spawns in sandbox → correct\n\nSolution: Either remove old text-to-telegram entry from commonplace.json, or fix orchestrator to not load both.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T11:51:26.841850296Z","created_by":"jes","updated_at":"2026-01-03T18:28:45.016071733Z","closed_at":"2026-01-03T18:28:45.016071733Z","close_reason":"Fixed by config cleanup. commonplace.json no longer contains application process definitions (bartleby, text-to-telegram). These are now only discovered via recursive mode from workspace/processes.json files, preventing the duplicate spawn issue."}
{"id":"CP-ie8","title":"Orchestrator fails to parse processes.json in subdirectories","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T11:06:13.174709745Z","created_by":"jes","updated_at":"2026-01-03T07:17:49.644809268Z","closed_at":"2026-01-03T07:17:49.644809268Z","close_reason":"Verified working - text-to-telegram runs from workspace/text-to-telegram/processes.json (subdirectory)"}
{"id":"CP-iisd","title":"Deduplicate MQTT command response publishing","description":"## Summary\nMQTT commands handler repeats response publishing logic (serialize + publish to {workspace}/responses) across create/delete/get-content/get-info. Extract a shared helper to reduce duplication.\n\n## Files to modify\n- src/mqtt/commands.rs\n- src/mqtt/mod.rs or src/mqtt/responses.rs (new helper)\n\n## Implementation steps\n1. Add helper to publish a response struct to the responses topic with QoS1.\n2. Replace repeated payload + publish blocks in each handler.\n3. Keep response topic and QoS unchanged.\n\n## Example\nBefore: each handler builds payload and publishes to \"{workspace}/responses\".\nAfter: publish_response(\u0026self.client, \u0026self.workspace, \u0026response).","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:59:00.894974823Z","created_by":"jes","updated_at":"2026-01-10T11:33:30.389297392Z","closed_at":"2026-01-10T11:33:30.389297392Z","close_reason":"Added publish_response\u003cT\u003e() helper method to deduplicate response publishing across all four command handlers (create-document, delete-document, get-content, get-info)."}
{"id":"CP-ik96","title":"CI integration: UUID-linked files sync bidirectionally","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T00:37:15.836990788Z","created_by":"jes","updated_at":"2026-01-11T00:37:15.836990788Z","comments":[{"id":46,"issue_id":"CP-ik96","author":"jes","text":"Covers acceptance criteria M1-M10 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nIncoming message flow (M1-M5):\n- Append to text-to-telegram/content.txt\n- Verify reaches bartleby/prompts.txt via UUID link\n- Bartleby consumes prompt\n- Verify deletion propagates via UUID link\n\nOutgoing response flow (M6-M10):\n- Bartleby appends to output.txt\n- Verify reaches text-to-telegram/input.txt via UUID link\n- Text-to-telegram consumes message\n- Verify deletion propagates via UUID link\n\nTests bidirectional sync of UUID-linked files across sandboxes.","created_at":"2026-01-11T00:41:15Z"}]}
{"id":"CP-iu6u","title":"Deduplicate Content-Location docId parsing in TS clients","description":"Summary: Multiple TS modules parse the Content-Location header to extract a /docs/\u003cuuid\u003e value; consolidate into a shared helper for consistent parsing and error handling.\n\nFiles to modify:\n- commonplaced-2025/src/actors/common/api-client.ts\n- commonplaced-2025/src/actors/common/actor.ts\n- commonplaced-2025/src/sync/filesystem/checkout.ts\n- commonplaced-2025/src/server/http-utils.ts (or a new shared utility module)\n\nImplementation steps:\n1) Add a helper like extractDocIdFromContentLocation(headerValue) that returns the UUID or throws a descriptive error.\n2) Replace inline regex parsing in api-client, actor, and checkout with the helper.\n3) Ensure the helper can handle optional/missing header cases (return undefined vs throw) depending on caller needs.\n\nExample:\nBefore: inline regex /\\/docs\\/([a-f0-9-]+)/ in multiple files.\nAfter: const docId = extractDocIdFromContentLocation(contentLocation, { required: true }).","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:16:44.86604776Z","created_by":"jes","updated_at":"2026-01-10T10:17:03.39486338Z"}
{"id":"CP-iwsr","title":"Deduplicate content-type diff selection","description":"## Summary\nDocumentService has repeated content-type branching (is_jsonl/is_xml_type) for diff computation. Extract a shared helper to select the correct diff/update path per ContentType.\n\n## Files to modify\n- src/services/document.rs\n- src/services/document_utils.rs (new helper)\n\n## Implementation steps\n1. Add helper compute_diff_for_content_type(content_type, old, new, base_state) returning DiffResult.\n2. Replace repeated is_jsonl/is_xml_type branches with the helper.\n3. Keep behavior identical for JSONL/XML/TEXT.\n\n## Example\nBefore: multiple branches compute_json_diff/compute_xml_diff_update/compute_diff_update.\nAfter: one helper does the dispatch.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:03:30.474419929Z","created_by":"jes","updated_at":"2026-01-10T10:03:30.474419929Z","comments":[{"id":35,"issue_id":"CP-iwsr","author":"jes","text":"Complex refactor: The 3 content types have different requirements - JSON needs base_state as b64 string, XML/Text need it as bytes. Also, merge path uses replayer while non-merge path uses doc_store. Would need to unify these interfaces first before extracting a helper.","created_at":"2026-01-10T12:16:07Z"}]}
{"id":"CP-j4wg","title":"Deleted files sometimes reappear after sync","description":"Summary: In some sync flows, deleting a file does not stick; the file can reappear after a subsequent sync event, likely due to stale watcher events or missing tombstone handling between local and remote state.\\n\\nFiles to modify:\\n- src/sync/dir_sync.rs\\n- src/sync/file_sync.rs\\n- src/sync/state.rs\\n- src/sync/watcher.rs\\n- src/sync/sse.rs\\n- tests (add regression coverage for delete propagation)\\n\\nImplementation steps:\\n1. Reproduce by deleting a synced file while the system is running, then trigger a sync/refresh to observe the file reappearing.\\n2. Trace delete propagation from local watcher to upload, and from server updates back to local, paying attention to event ordering and dedupe.\\n3. Ensure deletions are represented as tombstones/commits in state and are not overwritten by stale updates.\\n4. Update sync logic to ignore or drop resurrecting updates when a newer deletion is known locally or remotely.\\n5. Add a regression test that deletes a file and asserts it stays deleted after subsequent sync cycles.\\n\\nExample:\\nBefore: delete note.txt -\u003e file reappears after sync.\\nAfter: delete note.txt -\u003e file remains deleted across sync cycles.\\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T18:21:34.140006526Z","created_by":"jes","updated_at":"2026-01-06T22:50:49.483718498Z","closed_at":"2026-01-06T22:50:49.483718498Z","close_reason":"Fixed: Added handle_subdir_schema_cleanup() to properly handle subdirectory SSE events with correct node_id and path parameters. Includes file deletion support and safety guard against empty UUID maps. Merged in PR #105."}
{"id":"CP-jbw5","title":"SDK uses wrong Yjs type getter - getText() for all files instead of type-specific getter","description":"The SDK always uses doc.getText('content') regardless of file type, but the server stores different content types using different Yjs types:\n\n- .txt/.md → Y.Text (getText works)\n- .jsonl → Y.Array (need getArray, getText returns empty)\n- .json → Y.Map (need getMap, getText returns empty)\n\nCurrent behavior:\n- SDK calls getText('content') for all files\n- Works for text files\n- Returns empty for JSONL/JSON files (wrong Yjs type)\n\nFix needed:\n- SDK already detects content type from file extension\n- Use contentType to call the right Yjs getter:\n  - 'text' → getText('content')\n  - 'jsonl' → getArray('content')  \n  - 'json' → getMap('content')\n- This affects DocHandleImpl and OutputHandleImpl\n\nCurrent workaround: SDK uses head.content directly instead of Yjs state.\n\nImpact: Real-time CRDT sync (via MQTT edits) won't work for JSONL/JSON files until this is fixed.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T02:05:16.775333687Z","created_by":"jes","updated_at":"2026-01-07T02:09:12.143454172Z","closed_at":"2026-01-07T02:09:12.143454172Z","close_reason":"Fixed SDK to use correct Yjs type getter (getText/getArray/getMap) based on content type. JSONL files now properly read from Y.Array, JSON from Y.Map. Real-time CRDT sync works for all file types."}
{"id":"CP-jfv0","title":"Migrate sse.rs to use fetch_head helper (4 call sites)","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.296466677Z","created_by":"jes","updated_at":"2026-01-10T18:42:14.338414505Z","closed_at":"2026-01-10T18:42:14.338414505Z","close_reason":"Migrated 4 call sites to use fetch_head helper"}
{"id":"CP-jgn","title":"Deleting directory on disk should remove it from parent .commonplace.json","description":"When a directory is deleted from the filesystem in a synced tree, the sync should detect this and remove the entry from the parent folder's .commonplace.json schema.\n\nCurrently, the directory is recreated by sync because it still exists in the schema. Users have to manually edit .commonplace.json to permanently delete directories.\n\nImplementation note: May need to track additional local state about whether directories were previously checked out, to distinguish between 'deleted locally' vs 'not yet synced from server'.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T04:58:36.700774372Z","created_by":"jes","updated_at":"2026-01-03T08:43:49.841959198Z","closed_at":"2026-01-03T08:43:49.841959198Z","close_reason":"Implemented directory sync state tracking. Directories that are synced are now recorded in .commonplace-synced-dirs.json. When a directory exists in schema but not on disk, sync checks if it was previously synced - if so, it's treated as a local deletion and not recreated."}
{"id":"CP-jn3g","title":"commonplace-ps should show base processes from commonplace.json","description":"commonplace-ps only shows processes discovered from processes.json files (via DiscoveredProcessManager). It doesn't show base processes started from commonplace.json (via ProcessManager).\n\nExample - these are running but not shown:\n- server\n- sync  \n- beads-sync\n\nOnly discovered sandbox processes appear:\n- bartleby\n- file-tmux-file\n- text-to-telegram\n\nFix: ProcessManager should also write to the status file, or merge both managers' status.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T19:51:37.579636924Z","created_by":"jes","updated_at":"2026-01-04T00:11:42.427240514Z","closed_at":"2026-01-04T00:11:42.427240514Z","close_reason":"Added merge_and_write() to OrchestratorStatus that preserves processes from both ProcessManager (base) and DiscoveredProcessManager (discovered). Both managers now use this method, and commonplace-ps shows all processes. Commit: fb043c7"}
{"id":"CP-jnf","title":"Support JSON files as Yjs map/array types in directory sync","description":"Summary: Allow synced JSON files to be stored as Yjs map/array types (not plain text) so JSON structure is preserved in CRDT.\n\nFiles to modify:\n- src/document.rs (content type handling / default doc type)\n- src/diff.rs (JSON diff generation if needed)\n- src/bin/sync.rs (JSON file handling, initial sync)\n- src/sync/content_type.rs (content type mapping, potentially new metadata)\n- docs/FILESYSTEM.md (clarify JSON type behavior)\n- docs/WIRING_DIAGRAM_FILES.md (if wiring files need special handling)\n\nImplementation steps:\n1. Identify how JSON documents are represented in yrs (map/array) vs text and how current commits are applied.\n2. Decide on encoding for JSON files in sync (map vs array based on top-level JSON).\n3. Update sync upload/replace logic to send JSON updates rather than text edits for JSON files.\n4. Ensure server-side content retrieval returns serialized JSON for map/array docs.\n5. Add tests/fixtures for JSON map/array round-trips through sync.\n\nExample:\n- Before: file data.json stored as plain text in Yjs text.\n- After: data.json stored as Yjs map or array (based on top-level JSON), serialized to JSON text on read/write.\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T13:28:50.035996-08:00","updated_at":"2025-12-28T14:00:59.660408-08:00","closed_at":"2025-12-28T14:00:59.660408-08:00","close_reason":"Implemented JSON map/array sync support"}
{"id":"CP-joe","title":"Blocks acceptance: Bartleby sandbox syncs wrong path (bartleby/ instead of full workspace)","description":"P4 acceptance criteria fails: The bartleby sandbox should sync the entire workspace/ tree with bartleby/ as a subdirectory. Currently it only syncs workspace/bartleby/ as its root. Expected: Bartleby sandbox has files like text-to-telegram/, tmux/, content.txt at root, with bartleby/ as subdirectory. Actual: Bartleby sandbox has output.txt, prompts.txt at root (these are workspace/bartleby/ files).","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T03:23:53.597170323Z","created_by":"jes","updated_at":"2026-01-03T03:32:36.664155073Z","closed_at":"2026-01-03T03:32:36.664155073Z","close_reason":"Fixed by moving bartleby from workspace/bartleby/processes.json to workspace/processes.json. Processes sync at the directory containing their processes.json, so bartleby must be defined at root level to sync the entire workspace tree. Added clarifying section to acceptance criteria doc."}
{"id":"CP-jpx","title":"Sync client fails when files have pre-set node_ids but reconciler not running","description":"When .commonplace.json has pre-set node_ids (for file linking), sync correctly reads them but then waits forever for a reconciler to create the documents. The sync should either:\n1. Create documents directly via POST /docs if they don't exist\n2. Or clearly require --fs-root on the server\n\nCurrently blocks bartleby-telegram file linking use case.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-31T23:55:18.052642482Z","created_by":"jes","updated_at":"2026-01-01T00:04:03.65171956Z","closed_at":"2026-01-01T00:04:03.65171956Z","close_reason":"Not a bug - works when server has --fs-root enabled. Documented correct workflow in docs/FILESYSTEM.md."}
{"id":"CP-jqqy","title":"Deduplicate /files path encoding helper","description":"## Summary\nURL path encoding for /files/* is implemented locally in orchestrator/discovered_manager.rs. Consolidate with existing sync/urls helpers to avoid drift and duplicate encoding logic.\n\n## Files to modify\n- src/orchestrator/discovered_manager.rs\n- src/sync/urls.rs (expose encode_path or a path-segment encoding helper)\n\n## Implementation steps\n1. Export a shared helper for encoding file paths (segment-wise encoding).\n2. Replace local encode_url_path with the shared helper.\n3. Ensure behavior matches existing expectations for preserving '/' separators.\n\n## Example\nBefore: discovered_manager defines encode_url_path.\nAfter: uses sync::urls::encode_path (or new shared helper).","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:54:56.715189009Z","created_by":"jes","updated_at":"2026-01-10T11:36:21.541836937Z","closed_at":"2026-01-10T11:36:21.541836937Z","close_reason":"Replaced local encode_url_path with shared sync::encode_path - identical implementations."}
{"id":"CP-jr39","title":"Named workspaces with MQTT topic namespacing","description":"Introduce the concept of named workspaces where all MQTT commands are nested within that workspace name.\n\n- Each workspace has a name (default: \"commonplace\")\n- All MQTT topics are prefixed with the workspace name\n- Allows multiple independent commonplace instances to share an MQTT broker\n- Example: `commonplace/docs/{id}/blue` instead of `docs/{id}/blue`\n\nThis enables multi-tenant deployments and cleaner topic organization.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-08T19:41:18.67045053Z","created_by":"jes","updated_at":"2026-01-08T21:06:27.381058731Z","closed_at":"2026-01-08T21:06:27.381058731Z","close_reason":"Implemented workspace namespacing for MQTT topics"}
{"id":"CP-jrc","title":"Sync overwrites local schema with server content, losing commonplace-link changes","description":"When sync starts and server already has content, it fetches the server schema and overwrites the local .commonplace.json file. This destroys any changes made by commonplace-link.\n\nRoot cause: dir_sync.rs:783-791 unconditionally writes server schema to local file when strategy=skip (server has content).\n\nRepro:\n1. Run sync to establish initial schema\n2. Run commonplace-link to create shared UUIDs  \n3. Run sync again\n4. Local schema is overwritten with server's old schema, losing the link\n\nExpected: Sync should either push local schema changes to server, or merge local+server schemas.\n\nLocation: src/sync/dir_sync.rs:783-791","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T05:46:00.069867795Z","created_by":"jes","updated_at":"2026-01-01T05:52:29.616008208Z","closed_at":"2026-01-01T05:52:29.616008208Z","close_reason":"Fixed by preserving local schema when it exists. Merged in PR #68."}
{"id":"CP-jv08","title":"Deduplicate orchestrator process spawn/stdout handling","description":"## Summary\nOrchestrator process spawning is duplicated between ProcessManager and DiscoveredProcessManager (stdout/stderr capture, process-group setup, env wiring). Extract a shared helper so both managers use the same spawn/logging behavior and future fixes apply once.\n\n## Files to modify\n- src/orchestrator/manager.rs (use shared spawn helper)\n- src/orchestrator/discovered_manager.rs (use shared spawn helper)\n- src/orchestrator/mod.rs or src/orchestrator/spawn.rs (new shared helper)\n\n## Implementation steps\n1. Add a helper (e.g., spawn_managed_process) that accepts a prepared Command, process name, and optional env/cwd tweaks.\n2. Move stdout/stderr piping + line-prefix logging + process-group setup + kill_on_drop into the helper.\n3. Update ProcessManager::spawn_process and DiscoveredProcessManager::spawn_process to call the helper instead of duplicating the logic.\n4. Keep behavior identical (same prefixes, same env defaults) and add a small test or log-level check if needed.\n\n## Example\nBefore: both managers manually set Stdio::piped, spawn tasks to read lines, and set pre_exec.\nAfter: both managers call spawn_managed_process(name, cmd) and rely on shared stdout/stderr handling.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T09:04:00.056672411Z","created_by":"jes","updated_at":"2026-01-10T09:22:07.256291384Z","closed_at":"2026-01-10T09:22:07.256291384Z","close_reason":"Extracted shared spawn logic into spawn.rs. Both managers now use spawn_managed_process(). Net 31 lines removed."}
{"id":"CP-jw6c","title":"Expand CRDT ancestry checking to SSE handlers","description":"The initial fix (CP-pk4h) only covers sync_single_file for initial_sync=server. Need to also add default content/ancestry checking to SSE handlers that write server content to local files. This prevents the race where SSE overwrites good local content with default {} from server.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T06:29:05.927719516Z","created_by":"jes","updated_at":"2026-01-10T07:26:17.472292658Z","closed_at":"2026-01-10T07:26:17.472292658Z","close_reason":"Implemented proper CRDT ancestry checking in SSE handlers. Uses placeholder doc_id since commits are global across documents. Added URL encoding for CIDs.","comments":[{"id":28,"issue_id":"CP-jw6c","author":"jes","text":"Implemented default content check in all 3 SSE handlers. Known limitation: blocks intentional server clears. The proper fix is ancestry-based checking. See code comments.","created_at":"2026-01-10T06:44:02Z"}]}
{"id":"CP-jwf","title":"Refactor sync.rs: Extract SyncState into separate module","description":"sync.rs is 2,407 lines - the largest file in the codebase. Extract `SyncState` struct and its associated methods (echo detection, write tracking, hash tracking) into `src/sync/state.rs`.","status":"closed","priority":1,"issue_type":"chore","created_at":"2025-12-30T00:34:50.838547-08:00","updated_at":"2025-12-30T08:54:16.77161781Z","closed_at":"2025-12-30T08:54:16.77161781Z","close_reason":"Extracted SyncState and PendingWrite to src/sync/state.rs. Added documentation for echo detection and write barrier. Merged in PR #38."}
{"id":"CP-k51z","title":"CI integration: offline editing syncs on reconnect","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T00:37:11.748359322Z","created_by":"jes","updated_at":"2026-01-11T00:37:11.748359322Z","comments":[{"id":43,"issue_id":"CP-k51z","author":"jes","text":"Covers acceptance criteria O1-O6 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nO1: Stop workspace sync process\nO2: Edit file locally while sync is down\nO3: Verify server content has NOT changed\nO4: Wait for orchestrator to restart sync\nO5: Verify server content now includes local edit\nO6: Verify sandbox also received the edit\n\nTests the offline editing behavior where local changes are pushed on sync restart.","created_at":"2026-01-11T00:41:11Z"}]}
{"id":"CP-k5bq","title":"Directory sync doesn't subscribe to file content SSE - breaks UUID linking","status":"in_progress","priority":1,"issue_type":"bug","created_at":"2026-01-10T22:43:05.886583415Z","created_by":"jes","updated_at":"2026-01-10T22:44:53.75582024Z","comments":[{"id":39,"issue_id":"CP-k5bq","author":"jes","text":"Two possible fixes:\n1. MQTT: Use wildcard subscription (workspace/edits/#) to receive all edits\n2. SSE: Add wildcard support to SSE endpoint (server-side change)\n\nBoth approaches would allow the sync to receive edits for any document in the schema without spawning individual connections.","created_at":"2026-01-10T22:45:04Z"}]}
{"id":"CP-klb","title":"Add commonplace-uuid CLI to resolve a synced path to its UUID","description":"Summary: Provide a command-line tool that accepts a synced file path and prints the UUID it is linked to.\n\nFiles to modify:\n- src/bin (new binary, e.g., src/bin/commonplace-uuid.rs)\n- src/sync/dir_sync.rs or src/sync/state_file.rs (reuse path→UUID resolution logic)\n- README.md or docs/DEVELOPMENT.md (document usage)\n\nImplementation steps:\n1. Implement CLI that takes a path (relative or absolute) and resolves the owning document + UUID (same logic as sync uses).\n2. If file is linked, print the UUID to stdout and exit 0.\n3. If not linked, print a clear error and exit non-zero.\n4. Add a `--json` option that prints `{ \"path\": \"...\", \"uuid\": \"...\" }`.\n\nExample:\ncommonplace-uuid workspace/text-to-telegram/input.txt\n→ 12504d60-2b58-43c8-b461-a42215a3954d\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T05:46:09.496176753Z","created_by":"jes","updated_at":"2026-01-03T07:08:46.657641137Z","closed_at":"2026-01-03T07:08:46.657641137Z","close_reason":"Added commonplace-uuid CLI in src/bin/uuid.rs with --json support"}
{"id":"CP-koj9","title":"Extract file event handlers into file_events.rs","description":"Move handle_file_created, handle_file_modified, handle_file_deleted from dir_sync.rs into a new file_events.rs module. Pure extraction, no behavior change.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T07:54:00.115708068Z","created_by":"jes","updated_at":"2026-01-10T08:32:04.333631614Z","closed_at":"2026-01-10T08:32:04.333631614Z","close_reason":"Extracted ~500 lines from dir_sync.rs into file_events.rs: OwningDocument, find_owning_document, handle_file_created, handle_file_modified, handle_file_deleted"}
{"id":"CP-ksf","title":"CLI tool for firing commands to commonplace paths","description":"A command-line tool that sends commands to a specified commonplace document path. Allows scripting and manual interaction with commonplace documents via their red port (events/commands).","design":"Needs design discussion: High-level feature request without implementation details. Requires specification of API design, data model, and integration points before implementation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.268771-08:00","updated_at":"2025-12-29T23:50:06.348786945Z","closed_at":"2025-12-29T23:50:06.348786945Z","close_reason":"commonplace-cmd CLI implemented. Merged in PR #28."}
{"id":"CP-ksrw","title":"Deduplicate initial sync metadata fetch","description":"## Summary\nRepeated calls to build_uuid_map_recursive and get_all_node_backed_dir_ids in sync.rs are duplicated across different code paths. Extract a shared helper to fetch initial sync metadata (uuid map + subdirs) once.\n\n## Files to modify\n- src/bin/sync.rs\n- src/sync/uuid_map.rs (optional helper aggregator)\n\n## Implementation steps\n1. Add a helper that returns (uuid_map, node_backed_subdirs) for a fs-root.\n2. Replace repeated calls in sync.rs with the helper.\n3. Ensure error handling/logging remains unchanged.\n\n## Example\nBefore: multiple blocks call build_uuid_map_recursive and get_all_node_backed_dir_ids separately.\nAfter: sync_metadata = fetch_sync_metadata(...).","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:58:06.082693128Z","created_by":"jes","updated_at":"2026-01-10T09:58:06.082693128Z"}
{"id":"CP-l5b","title":"Orchestrator leaves orphaned processes when killed","description":"When orchestrator or commonplace-sync is killed (SIGTERM/SIGKILL), spawned child processes (e.g., bartleby.py, text_to_telegram) are left orphaned. Need proper process cleanup - either kill children on exit or use process groups.","notes":"Investigation: orchestrator only listens for Ctrl+C (SIGINT) and never registers SIGTERM handlers in src/bin/orchestrator.rs. So SIGTERM exits immediately without running manager.shutdown(), leaving children alive. For SIGKILL no cleanup is possible. Also shutdown only sends SIGTERM to child PID, not process group; grandchildren can survive.\n\nFix advice: \n1) Add SIGTERM handler (tokio::signal::unix::signal(SignalKind::terminate())) and route it to the same shutdown path as Ctrl+C. \n2) Spawn children into their own process group (Command::before_exec setpgid(0,0)) and on shutdown send SIGTERM to -pid (killpg) before waiting; fall back to SIGKILL. \n3) Consider PR_SET_PDEATHSIG (libc::prctl) so children get SIGTERM if orchestrator dies abruptly, which covers SIGKILL of parent. \n4) Optionally set kill_on_drop(true) for extra safety when handles are dropped.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-01T19:42:53.377806426Z","created_by":"jes","updated_at":"2026-01-02T08:02:20.485194609Z","closed_at":"2026-01-02T08:02:20.485194609Z","close_reason":"Verified working: When orchestrator killed with SIGTERM, all child processes (sandbox syncs, bartleby, text-to-telegram) are properly terminated and sandbox directories are cleaned up. T1-T6 acceptance tests pass. The SIGTERM handler, process groups (setpgid), kill_on_drop, and PDEATHSIG are all working as documented in the issue comments.","comments":[{"id":16,"issue_id":"CP-l5b","author":"jes","text":"Diagnosis: In current code, orchestrator already handles SIGTERM and cleans up process trees. src/bin/orchestrator.rs registers SIGTERM via tokio::signal::unix::signal(SignalKind::terminate()) and routes it to shutdown. src/orchestrator/manager.rs and src/orchestrator/discovered_manager.rs set process groups in pre_exec (setpgid), request PR_SET_PDEATHSIG on Linux, enable kill_on_drop, and on shutdown call killpg with SIGTERM then SIGKILL. This matches the proposed fix in the issue notes. If orphaning is still happening, likely running an older binary or non-unix build (non-unix path only handles Ctrl+C). SIGKILL of parent still cannot be intercepted, but PDEATHSIG should cover abrupt parent death on Linux.","created_at":"2026-01-02T07:09:34Z"},{"id":17,"issue_id":"CP-l5b","author":"jes","text":"Follow-up: further testing should record specifics of what was tried (exact binary/commit, how it was terminated, and which child PID tree was orphaned) and what went wrong, so we can reproduce on Ubuntu.","created_at":"2026-01-02T07:12:55Z"}]}
{"id":"CP-l7yl","title":"Deduplicate JSON/JSONL update creation","description":"## Summary\nJSON/JSONL update creation is duplicated between DocumentStore (src/document.rs) and DocumentService (src/services/document.rs). Consolidate into a shared helper to keep base_state handling and error behavior consistent.\n\n## Files to modify\n- src/document.rs (use shared helper)\n- src/services/document.rs (use shared helper)\n- src/sync/yjs.rs or src/document_utils.rs (new helper for JSON/JSONL update creation)\n\n## Implementation steps\n1. Add a helper that takes content + optional base_state_b64 and returns update_b64 for JSON vs JSONL.\n2. Replace the local compute_json_diff/create_yjs_json_update branching in services/document.rs and document.rs with the helper.\n3. Keep error mapping identical (invalid JSON -\u003e ApplyError/ServiceError as today).\n\n## Example\nBefore: two different call sites decide between create_yjs_json_update and create_yjs_jsonl_update.\nAfter: both call create_structured_update(content_type, content, base_state).","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:27:32.880201682Z","created_by":"jes","updated_at":"2026-01-10T10:43:55.432096524Z","closed_at":"2026-01-10T10:43:55.432096524Z","close_reason":"Added create_yjs_structured_update() helper that dispatches to JSON/JSONL update functions based on ContentType. Made ContentType Copy, simplified compute_json_diff to take ContentType instead of bool."}
{"id":"CP-l9p","title":"Add lockfile to prevent multiple sync instances per checkout","description":"Summary: Prevent multiple sync processes from managing the same checkout (or nested checkouts), which can cause conflicting updates.\n\nFiles to modify:\n- src/bin/sync.rs (startup/CLI entry for sync client)\n- src/sync/state_file.rs or a new lock module for lockfile creation/validation\n- docs/DEVELOPMENT.md or README.md (document lock behavior)\n\nImplementation steps:\n1. On sync startup, compute a lockfile path scoped to the checkout root (e.g., .commonplace-sync.lock in fs-root or workspace root).\n2. Attempt to acquire an exclusive lock (PID + timestamp). If lock already exists and process is alive, exit with clear error.\n3. If lock is stale (PID not running), overwrite and continue.\n4. Detect nested sync roots: if a lockfile exists in a parent directory, refuse to start and explain the conflict.\n5. Ensure lockfile is released on clean shutdown (best-effort).\n\nExample:\n- Start sync in /home/jes/commonplace/workspace → creates .commonplace-sync.lock\n- Attempt to start another sync in /home/jes/commonplace/workspace or /home/jes/commonplace/workspace/subdir → error: sync","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T05:38:29.383950693Z","created_by":"jes","updated_at":"2026-01-03T07:16:04.212881149Z","closed_at":"2026-01-03T07:16:04.212881149Z","close_reason":"Added acquire_sync_lock() in sync/mod.rs with exclusive file lock (.commonplace-sync.lock)"}
{"id":"CP-lav","title":"Python client: y_py YDoc not thread-safe with paho-mqtt callbacks","design":"The y_py (Yjs Python bindings) YDoc panics when accessed from a different thread than where it was created. paho-mqtt runs message callbacks in a background thread. When command handlers try to publish_edit(), they access YDoc from the MQTT thread, causing panic. Fix options: 1) Use queue to dispatch updates from MQTT thread to main thread, 2) Create YDoc in MQTT thread, 3) Use asyncio-based MQTT client with single-threaded event loop.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-30T00:30:34.203735202Z","created_by":"jes","updated_at":"2025-12-30T01:17:06.747552054Z","closed_at":"2025-12-30T01:17:06.747552054Z","close_reason":"Fixed by implementing queue-based pattern for YDoc operations. MQTT callbacks queue work, main loop processes. Committed in ded73c4."}
{"id":"CP-li3","title":"Deprecate wiring diagram concept","description":"The wiring diagram concept should be deprecated and removed from the codebase.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T21:56:44.059396274Z","created_by":"jes","updated_at":"2025-12-29T08:56:46.462777465Z","closed_at":"2025-12-29T00:50:28.541758-08:00","close_reason":"PR #18 merged. Deprecated nodes abstraction, restored /nodes HTTP endpoints for sync, renamed /sse/nodes to /sse/docs, fixed fs-root ContentType. Follow-up issues filed: CP-bnv (path-based API), CP-pta (nodes→docs rename), CP-m2g (sync path-based)."}
{"id":"CP-ljyu","title":"Add spawn priority to commonplace.json for orchestrator order","description":"Summary: Add a priority field to commonplace.json so orchestrator spawn order is configurable instead of hardcoding server-first.\n\nFiles to modify:\n- src/orchestrator/config.rs (add priority field to process config)\n- src/orchestrator/manager.rs or discovered_manager.rs (use priority for startup order)\n- docs/DEVELOPMENT.md or README.md (document new field)\n\nImplementation steps:\n1. Add numeric priority to process config schema (lower = earlier).\n2. Update orchestrator to sort processes by priority before startup.\n3. Keep existing behavior as default (server priority = 0 if unspecified).\n4. Update docs and examples.\n\nExample:\n- server: priority 0\n- sync: priority 10\n- text-to-telegram: priority 20","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T19:42:45.772623557Z","created_by":"jes","updated_at":"2026-01-04T01:08:32.072250537Z","closed_at":"2026-01-04T01:08:32.072250537Z","close_reason":"Won't implement - user prefers depends_on over numeric priority for startup ordering"}
{"id":"CP-lr9","title":"Sync race condition causes output file overwrites","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-31T20:09:30.137946956Z","created_by":"jes","updated_at":"2025-12-31T21:51:40.994526815Z","closed_at":"2025-12-31T21:51:40.994526815Z","close_reason":"Fixed in PR #59. refresh_from_head now checks for pending local changes before overwriting.","comments":[{"id":7,"issue_id":"CP-lr9","author":"jes","text":"Root cause identified: sync client processes its own SSE edit events. Simple fix found - check edit.commit.author == sync-client in handle_server_edit. See CP-lr9-investigation.md for full analysis.","created_at":"2025-12-31T20:21:16Z"}]}
{"id":"CP-lu80","title":"Centralize HTTP response header building for doc responses","description":"Summary: commonplaced-2025/src/server/rest.ts and commonplaced-2025/src/server/http-utils.ts each assemble Content-Type/Content-Location/X-Commonplace-Commit headers for doc responses; unify into shared helpers to avoid drift.\n\nFiles to modify:\n- commonplaced-2025/src/server/rest.ts\n- commonplaced-2025/src/server/http-utils.ts\n\nImplementation steps:\n1) Add helper(s) in http-utils.ts to build headers for JSON/XML/text and HEAD responses.\n2) Replace inline header objects in rest.ts with the helper(s), keeping logic for content-type detection but sharing header assembly.\n3) Ensure HEAD responses still send headers without bodies.\n\nExample:\nBefore: rest.ts builds headers inline for XML/text/JSON in multiple branches.\nAfter: rest.ts calls buildDocHeaders({ docId, headCid, contentType }) and uses reply/handleError consistently.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:17:18.806202959Z","created_by":"jes","updated_at":"2026-01-10T10:17:35.896842092Z"}
{"id":"CP-lzb","title":"Fix P1: Start sync tasks for files created from server schema (PR #4)","description":"From PR #4 Codex review: When a new path appears in the server schema, the handler doesn't start sync tasks for the new files.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:22.281537-08:00","updated_at":"2025-12-26T23:43:55.692683-08:00","closed_at":"2025-12-26T23:43:55.692683-08:00","close_reason":"Already fixed and merged to main. The handle_schema_change function now takes a spawn_tasks parameter, and runtime SSE events (line 1186) pass spawn_tasks=true to spawn sync tasks for new server-created files."}
{"id":"CP-lzy","title":"Sync sandbox exposes commonplace CLI tools to child","description":"Sync sandbox should ensure commonplace command line tools are available to the child process (in PATH or similar).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:50:47.47569485Z","created_by":"jes","updated_at":"2025-12-30T22:09:01.350202303Z","closed_at":"2025-12-30T22:09:01.350202303Z","close_reason":"Closed","dependencies":[{"issue_id":"CP-lzy","depends_on_id":"CP-8ci","type":"blocks","created_at":"2025-12-30T17:50:58.721422649Z","created_by":"daemon"},{"issue_id":"CP-lzy","depends_on_id":"CP-q3m","type":"blocks","created_at":"2025-12-30T18:02:57.44355175Z","created_by":"daemon"}],"comments":[{"id":5,"issue_id":"CP-lzy","author":"jes","text":"Accidentally closed - depends on CP-8ci","created_at":"2025-12-30T21:02:26Z"}]}
{"id":"CP-m2g","title":"Convert sync client to use path-based API","design":"Once path-based HTTP APIs exist (CP-bnv), update commonplace-sync to use paths like /files/notes/todo.txt instead of requiring UUIDs. This would simplify the sync client - no need to create nodes or track UUIDs, just sync files by their natural paths.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-29T08:35:40.683147536Z","created_by":"jes","updated_at":"2025-12-29T22:21:03.49437558Z","closed_at":"2025-12-29T22:21:03.49437558Z","close_reason":"Added --use-paths flag to sync client for path-based API. URL builders select between /files/*path and /docs/:id routes with proper path encoding. Merged in PR #25.","dependencies":[{"issue_id":"CP-m2g","depends_on_id":"CP-bnv","type":"blocks","created_at":"2025-12-29T08:35:57.462826032Z","created_by":"daemon"},{"issue_id":"CP-m2g","depends_on_id":"CP-pta","type":"blocks","created_at":"2025-12-29T08:37:30.460677237Z","created_by":"daemon"}]}
{"id":"CP-m4q7","title":"Fix MQTT topic structure: port before path","description":"Current topic structure is wrong:\n  {workspace}/{path}/{port}  →  workspace/docs/notes.txt/edits\n\nThis requires file extensions to know where path ends and port begins, breaking wildcards.\n\nCorrect structure:\n  {workspace}/{port}/{path}  →  workspace/edits/docs/notes\n\nBenefits:\n- Parsing is trivial (port is always segment 1, known set of values)\n- No extension requirement\n- Wildcards just work:\n  - workspace/edits/# → all edits\n  - workspace/edits/docs/# → all edits under docs/\n  - workspace/+/docs/notes → all ports for one file\n\nThis is a breaking change to the MQTT protocol but fixes the fundamental design flaw.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-09T07:08:59.944243038Z","created_by":"jes","updated_at":"2026-01-09T07:50:33.778372965Z","closed_at":"2026-01-09T07:50:33.778372965Z","close_reason":"Fixed MQTT topic structure: port before path. Topics now use {workspace}/{port}/{path} format enabling natural wildcards."}
{"id":"CP-m6ua","title":"Orchestrator should maintain secure store of ED25519 keypairs","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-09T01:04:59.684693783Z","created_by":"jes","updated_at":"2026-01-09T01:04:59.684693783Z"}
{"id":"CP-mg8x","title":"Add MQTT topic for document info/metadata","description":"HTTP has GET /docs/:id/info to get document metadata (id, content_type) but there's no MQTT equivalent.\n\nAdd to sync protocol or commands port to allow MQTT clients to query document metadata.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:06.698434146Z","created_by":"jes","updated_at":"2026-01-09T08:58:17.108905739Z","closed_at":"2026-01-09T08:58:17.108905739Z","close_reason":"Added MQTT get-info command: topic {workspace}/commands/get-info with GetInfoRequest/Response message types"}
{"id":"CP-mpp0","title":"Replace endpoint must validate parent commit to prevent blind overwrites","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T17:50:41.866345851Z","created_by":"jes","updated_at":"2026-01-10T18:05:38.466377599Z","closed_at":"2026-01-10T18:05:38.466377599Z","close_reason":"Fixed: Server now returns 428 Precondition Required when parent_cid is missing but document has history. Sync client updated to fetch HEAD before initial push."}
{"id":"CP-mpt","title":"Factor dir_sync.rs into smaller modules","description":"dir_sync.rs is 63KB and handles multiple concerns:\n- Schema syncing and reconciliation\n- Directory watching and SSE tasks\n- File creation/modification/deletion handlers\n- UUID map building\n- Nested schema handling\n\nShould be split into focused modules like:\n- schema_sync.rs - schema fetching and reconciliation\n- handlers.rs - file event handlers (created/modified/deleted)\n- uuid_map.rs - UUID resolution from schemas\n- nested.rs - nested schema handling for node-backed directories","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-03T07:49:39.445210344Z","created_by":"jes","updated_at":"2026-01-03T09:08:39.000702126Z","closed_at":"2026-01-03T09:08:39.000702126Z","close_reason":"Extracted uuid_map.rs module from dir_sync.rs. Moved 7 UUID resolution and path mapping functions into a focused module. dir_sync.rs reduced from 1834 to 1580 lines (~254 lines). The handlers remain in dir_sync.rs as they're tightly coupled with schema operations."}
{"id":"CP-mxt2","title":"Add MQTT topic for replace with diff","description":"HTTP has POST /docs/:id/replace which computes a diff and applies it as a Yjs update. No MQTT equivalent.\n\nThis is useful for clients that don't have Yjs - they can send plain text and the server computes the CRDT update.\n\nAdd to sync protocol or as a commands verb.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:05:08.93457045Z","created_by":"jes","updated_at":"2026-01-09T09:00:21.094365782Z","closed_at":"2026-01-09T09:00:21.094365782Z","close_reason":"Deferred - not required for sync refactor. Sync client doesn't use replace with diff."}
{"id":"CP-n0x","title":"commonplace-replay only showing one commit","description":"commonplace-replay only shows a single commit for files that should have a longer history.\n\nExample:\n```\njes@commonplace:~/commonplace/workspace$ ../target/release/commonplace-replay bartleby/prompts.txt\nCommit: 9eb1736331c96195d0b5102a88a9ea7b6f02170a6708493c345b1c241bc1ea12\n```\n\nExpected: Should show the full commit history with timestamps and content changes, similar to git log.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-03T11:46:02.083601064Z","created_by":"jes","updated_at":"2026-01-03T18:49:05.462908871Z","closed_at":"2026-01-03T18:49:05.462908871Z","close_reason":"Fixed. Default output now shows commit count and hints at --list for full history."}
{"id":"CP-n4ga","title":"Support image files as base64-encoded text","description":"Summary: Allow image files to sync as base64-encoded text documents so images can travel through the existing text/Yjs pipeline without external blob storage.\n\nFiles to modify:\n- src/sync/content_type.rs\n- src/sync/file_sync.rs\n- src/services/document.rs\n- docs/ARCHITECTURE.md (or README.md)\n- tests (add/extend an image sync roundtrip test)\n\nImplementation steps:\n1. Detect image extensions/content types (png, jpg/jpeg, gif, webp) and map them to a new content type like ImageBase64 or similar.\n2. On upload (local -\u003e server), read the image bytes and encode to base64 text; store in the document as a data URL (e.g., \"data:image/png;base64,...\") or a structured JSON wrapper with mime + data.\n3. On download (server -\u003e local), decode the base64 text back to bytes and write the file with the original extension.\n4. Ensure the text/Yjs diff path treats the base64 payload as opaque (no line ending normalization).\n5. Add tests that roundtrip a small image fixture and assert exact byte equality after sync.\n\nExample:\nBefore: image.png sync attempts to treat bytes as text and fails/garbles content.\nAfter: image.png is stored as \"data:image/png;base64,iVBORw0...\" and decodes back to the same bytes when written locally.\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T04:10:13.558172021Z","created_by":"jes","updated_at":"2026-01-05T07:54:07.02193053Z","closed_at":"2026-01-05T07:54:07.02193053Z","close_reason":"Already implemented - binary files including images are detected by extension (BINARY_EXTENSIONS in content_type.rs) and content sniffing, encoded as base64 for sync, and decoded back to bytes on download"}
{"id":"CP-n5mj","title":"Directory sync should use inode tracking (shadow hardlinks)","description":"Summary: Inode tracking/shadow hardlinks are only enabled for file sync mode; directory sync does not use the tracker, so atomic server updates can still drop writes from old inodes in directory sync.\n\nFiles to modify:\n- src/bin/sync.rs\n- src/sync/dir_sync.rs\n- src/sync/sse.rs\n- src/sync/watcher.rs\n- src/sync/state.rs\n\nImplementation steps:\n1. Identify the directory sync path that handles server updates (directory_sse_task / handle_schema_change) and local writes.\n2. Add an InodeTracker scoped per-directory (or per-file) and wire it into directory SSE handling so server updates use atomic_write_with_shadow.\n3. Watch the shadow directory and process shadow writes for directory sync (reuse shadow_watcher_task/shadow_write_handler_task).\n4. Ensure tracker seeding for existing files with known commit_id after initial sync.\n5. Add integration tests covering atomic server replace with a long-lived writer in directory sync mode.\n\nExample:\nBefore: In directory sync mode, server updates replace inodes and old-writer edits are lost.\nAfter: Directory sync creates shadow hardlinks and merges old-writer edits via CRDT.\n","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-06T05:50:39.765330978Z","created_by":"jes","updated_at":"2026-01-06T17:15:46.905763027Z","closed_at":"2026-01-06T17:15:46.905763027Z","close_reason":"Added optional InodeTracker parameter to directory sync functions (spawn_file_sync_tasks, directory_sse_task, subdir_sse_task, handle_schema_change, handle_file_created). When tracker is provided, uses sse_task_with_tracker for atomic writes with shadow hardlinks. All call sites pass None for now - tracker initialization will be added in follow-up work. Merged in PR #101."}
{"id":"CP-n6sd","title":"CI integration: __processes.json add/remove starts and stops processes","description":"Summary: Add a CI integration test that verifies changes to __processes.json dynamically start and stop sandboxed processes (orchestrator reload).\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/processes-json-integration-test.sh)\n- .github/workflows/ci.yml\n- docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md (if test references acceptance criteria)\n\nImplementation steps:\n1) Start orchestrator with a temp workspace containing an initial __processes.json with a no-op process.\n2) Modify __processes.json to add a second process; assert its stdout file appears.\n3) Modify __processes.json to remove the process; assert the process stops (no new output, PID gone, or status update).\n4) Run in CI and fail if process does not start/stop.\n\nExample:\nBefore: editing __processes.json requires restart for new process.\nAfter: process starts/stops within N seconds of config change.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T23:15:39.831709582Z","created_by":"jes","updated_at":"2026-01-10T23:15:58.309490755Z","comments":[{"id":48,"issue_id":"CP-n6sd","author":"jes","text":"Covers acceptance criteria H3-H6 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nH3: Add new process to __processes.json\nH4: Verify new process appears in commonplace-ps within 10 seconds\nH5: Remove process from __processes.json\nH6: Verify process stopped and removed from commonplace-ps within 10 seconds","created_at":"2026-01-11T00:41:34Z"}]}
{"id":"CP-n7qu","title":"Bartleby STDIO not saved to sandbox log files","description":"## Summary\nBartleby runs in sandbox mode but its stdout/stderr are not being written to the expected log files in the sandbox directory, so there is no persisted STDIO for debugging.\n\n## Files to modify\n- src/bin/sync.rs (sandbox exec logging around __EXEC__.stdout.txt/__EXEC__.stderr.txt)\n- src/orchestrator/manager.rs (stdout/stderr capture and log file wiring if bartleby is launched via orchestrator)\n- src/orchestrator/discovered_manager.rs (same as manager.rs for discovered processes)\n\n## Implementation steps\n1. Reproduce by running bartleby in sandbox mode and confirm that __bartleby.stdout.txt and __bartleby.stderr.txt are missing or empty in the sandbox directory.\n2. Trace the sandbox exec path to confirm stdout/stderr are piped and the log files are opened/written (see logging tasks in src/bin/sync.rs).\n3. Fix the log file wiring so sandboxed processes always append stdout/stderr to the expected files and flush promptly.\n4. Add/adjust a regression test or integration harness to assert that sandboxed execs produce non-empty log files when the process writes to stdout/stderr.\n\n## Example\nBefore: run bartleby in sandbox; __bartleby.stdout.txt is missing or empty despite output.\nAfter: run bartleby in sandbox; __bartleby.stdout.txt and __bartleby.stderr.txt contain the process output.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-10T02:51:14.000383603Z","created_by":"jes","updated_at":"2026-01-10T03:39:11.180504264Z","closed_at":"2026-01-10T03:39:11.180504264Z","close_reason":"Fixed race condition: push log file schema entries to server BEFORE starting exec. Verified bartleby stdout captured correctly in sandbox log files."}
{"id":"CP-nnbx","title":"Deduplicate CLI fetch for changes endpoint","description":"## Summary\nCLI binaries log/show/replay duplicate HTTP fetch logic for /documents/{id}/changes (and related parsing). Extract a shared helper to fetch changes and parse CommitChange lists.\n\n## Files to modify\n- src/bin/log.rs\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/cli/changes.rs or src/cli/http.rs (new helper)\n\n## Implementation steps\n1. Add a helper like fetch_changes(client, server, uuid) -\u003e ChangesResponse.\n2. Replace repeated client.get(.../changes) calls with the helper.\n3. Keep error handling and JSON parsing consistent.\n\n## Example\nBefore: each CLI calls client.get(.../changes).send().await.\nAfter: shared fetch_changes() used everywhere.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:53:40.451809254Z","created_by":"jes","updated_at":"2026-01-10T11:53:13.013669165Z","closed_at":"2026-01-10T11:53:13.013669165Z","close_reason":"Added fetch_changes() helper to cli.rs; updated show.rs and replay.rs to use it"}
{"id":"CP-nno","title":"External process MQTT client as first-class citizen","description":"Allow a separate Unix process to connect to MQTT and perform all the same actions as the JS sandbox evaluator:\n- Subscribe to document commits\n- Create/own output documents\n- Receive commands on owned document paths\n- Emit events as owned documents\n\nThis makes external processes (any language) first-class participants in the commonplace reactive graph, using MQTT as the protocol.","design":"Needs design discussion: High-level feature request without implementation details. Requires specification of API design, data model, and integration points before implementation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.52436-08:00","updated_at":"2025-12-30T01:26:42.019335953Z","closed_at":"2025-12-30T01:26:42.019335953Z","close_reason":"Python client example complete with FileProcess base class and CounterExample. Bug fixes for sync protocol and thread safety merged in 00fa478."}
{"id":"CP-nol0","title":"Add Yjs compaction algorithm","description":"## Summary\nAdd a Yjs compaction helper that collapses a sequence of Yjs updates into a single state update so long histories can be replayed faster and stored more compactly.\n\n## Files to modify\n- src/replay.rs (add compaction helper and tests)\n- src/services/document.rs (use compaction when rebuilding state for history/replay if a threshold is exceeded)\n- src/bin/log.rs (optional: use compacted state for streaming output)\n\n## Implementation steps\n1. Add a helper in replay that applies a list of update bytes to a yrs::Doc and returns a compacted state update (base64-encoded) via encode_state_as_update_v1.\n2. Expose a function on CommitReplayer like compact_updates(commits) -\u003e update_b64 so callers can replace long chains with a single update.\n3. Wire compaction into replay paths that currently apply every update (guarded by a reasonable threshold, e.g. \u003e 100 commits).\n4. Add tests in src/replay.rs to verify that compacted updates reconstruct the same content/state as the original update sequence for text and JSON documents.\n\n## Example\nBefore: Replay updates u1..u200 individually to reach HEAD.\nAfter: compact_updates(u1..u200) -\u003e u_compact; replay u_compact once and get identical content/state.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-10T02:09:46.219910274Z","created_by":"jes","updated_at":"2026-01-10T02:10:54.014137232Z"}
{"id":"CP-nonc","title":"Refactor dir_sync.rs - reuse file sync functionality for JSON schemas","description":"dir_sync.rs has thousands of lines and appears to duplicate functionality that could be reused from file sync. Schema documents are JSON files - they should leverage the same CRDT sync path as regular JSON files instead of having separate fetch-modify-push logic.","notes":"Ideas:\n- Refactor schema sync to reuse file_sync JSON path: treat .commonplace.json as a normal JSON document and use push_json_content/create_yjs_json_update with base_state instead of bespoke push_schema_to_server.\n- Extract shared helpers for scanning dir -\u003e schema_json, then call the same client push routines used for JSON files; reduces duplication and fixes CRDT semantics.\n- Replace manual fetch-modify-push + cleanup loops with CRDT diffs and server-state-driven reconciliation to avoid races between multiple sandboxes.\n- Split dir_sync.rs into smaller modules (schema_io, schema_reconcile, schema_watch) to make reuse of file_sync functions practical.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:30:13.57079748Z","created_by":"jes","updated_at":"2026-01-10T07:54:10.560088701Z","closed_at":"2026-01-10T07:54:10.560088701Z","close_reason":"Decomposed into subtasks: CP-rrw8 (CRDT reuse), CP-wpty (ancestry), CP-htfn/CP-koj9/CP-x86d (module extraction)"}
{"id":"CP-nrq","title":"Recursive mode fails when server not running - needs to start server+sync first","description":"When --recursive is true (the default), the orchestrator immediately tries to connect to http://localhost:3000/fs-root, but the server hasn't been started yet. This creates a chicken-and-egg problem:\n\n1. commonplace.json defines server and sync processes\n2. Recursive mode needs the server to discover processes.json files\n3. But recursive mode doesn't start the server - it just assumes it's running\n\nThe fix should be:\n1. Start server and sync from commonplace.json first (using ProcessManager)\n2. Wait for sync to push initial content\n3. Then switch to recursive discovery for additional processes\n\nCurrently the only workaround is to use --no-recursive which disables dynamic process discovery entirely.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T18:01:00.270509783Z","created_by":"jes","updated_at":"2026-01-03T18:16:37.580622788Z","closed_at":"2026-01-03T18:16:37.580622788Z","close_reason":"Fixed recursive mode: orchestrator now starts server+sync from commonplace.json before running recursive discovery. Also fixed retry logic for failed processes.json files."}
{"id":"CP-nvc5","title":"Directory sync mode missing inode tracking (causes duplicate deliveries)","description":"The directory sync mode (including sandbox/exec mode) has inode_tracker hardcoded to None in 10 places with TODO comments. This means:\n\n1. Shadow hardlinks are never created for directory-synced files\n2. When atomic writes change the inode, writes to the old inode aren't detected\n3. No merge commits are created for concurrent edits\n4. This causes duplicate deliveries in bartleby-style file IO (CP-raid)\n\nThe single-file sync mode (run_file_mode) has working inode tracking, but run_directory_mode and run_exec_mode do not.\n\nAffected lines in src/bin/sync.rs:\n- 1025, 1116, 1148, 1171, 1200, 1327, 1418, 1449, 1470, 1499\n\nFix: Initialize InodeTracker for directory sync mode and pass it to all the places that currently pass None.","notes":"Fixed by enabling inode tracking in run_directory_mode and run_exec_mode. Added shadow_dir parameter, InodeTracker initialization, shadow watcher/handler/GC tasks, and passed inode_tracker to all SSE task calls that previously passed None.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-07T22:08:51.194605804Z","created_by":"jes","updated_at":"2026-01-07T22:29:31.848339689Z","closed_at":"2026-01-07T22:29:31.84835047Z"}
{"id":"CP-nzul","title":"Deduplicate SSE client loop across orchestrator and sync","description":"## Summary\nSSE connection loops are implemented separately in orchestrator/discovered_manager.rs and sync/sse.rs. Consider extracting a shared SSE client helper to centralize EventSource setup, reconnect logic, and error handling.\n\n## Files to modify\n- src/orchestrator/discovered_manager.rs\n- src/sync/sse.rs\n- src/sse_client.rs (new shared helper)\n\n## Implementation steps\n1. Add a shared SSE client helper that wraps EventSource creation and reconnection.\n2. Move common loop/error handling into the helper and accept a callback for event handling.\n3. Update both orchestrator and sync SSE usage to call the helper.\n\n## Example\nBefore: two separate EventSource loops with similar reconnect logic.\nAfter: sse_client::run_sse_loop(url, handler) used in both.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:06:09.985244767Z","created_by":"jes","updated_at":"2026-01-10T10:06:09.985244767Z"}
{"id":"CP-o1h","title":"Fix: Merge commits should include both parent_cid and current HEAD as parents","description":"When parent_cid differs from HEAD in replace endpoint, the new commit only includes parent_cid as its parent. This drops the current HEAD from the commit graph. On history replay, the concurrent edits from HEAD are lost.\n\nFix: When parent_differs is true, the commit should have BOTH parent_cid AND current_head as parents.\n\nFound by local codex review on PR #37.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-30T08:14:26.210722759Z","created_by":"jes","updated_at":"2025-12-30T08:24:36.109315944Z","closed_at":"2025-12-30T08:24:36.109315944Z","close_reason":"Fixed merge commits to include both parent_cid AND current HEAD when they differ. This preserves concurrent edits in commit DAG for proper history replay. Committed in 7ebc4bd."}
{"id":"CP-o2h","title":"Evaluate whether NodeRegistry is still needed after HTTP/store split","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-29T00:52:45.145726221Z","created_by":"jes","updated_at":"2025-12-29T04:12:26.221158382Z","closed_at":"2025-12-29T04:12:26.221158382Z","close_reason":"Evaluated: NodeRegistry is NOT duplicated after HTTP/store split. Store owns it (document state, wiring, fs reconciliation). HTTP is stateless (MQTT gateway only). Architecture is already optimal - no changes needed.","dependencies":[{"issue_id":"CP-o2h","depends_on_id":"CP-8t3","type":"blocks","created_at":"2025-12-29T00:52:52.493729194Z","created_by":"daemon"}]}
{"id":"CP-o2xm","title":"Deduplicate HTTP URL construction across CLIs","description":"## Summary\nHTTP URL construction is duplicated across CLI binaries and orchestrator/sync modules (format!(\"{}/docs/{}/head\", ...), /documents/{}/changes, /fs-root, /health). Introduce shared URL builder helpers to avoid drift.\n\n## Files to modify\n- src/sync/urls.rs (add builders for /documents/*, /health, /fs-root)\n- src/bin/log.rs\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/bin/orchestrator.rs\n- src/bin/sync.rs\n\n## Implementation steps\n1. Add URL helpers for endpoints used by CLIs (changes, commits, health, fs-root).\n2. Replace inline format! calls with the helpers.\n3. Keep path encoding consistent with existing behavior.\n\n## Example\nBefore: multiple format!(\"{}/documents/{}/changes\", ...).\nAfter: build_changes_url(server, uuid) used everywhere.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:52:20.941893811Z","created_by":"jes","updated_at":"2026-01-10T09:52:20.941893811Z"}
{"id":"CP-o7h0","title":"Refactor sync to use MQTT subscriptions instead of SSE","description":"The current sync architecture uses per-document SSE subscriptions, requiring N connections for N node-backed directories. This creates:\n- Startup race conditions (subdirs not discovered until other syncs push)\n- Dynamic SSE task spawning complexity (CP-4w00)\n- Polling/retry workarounds for initial sync\n\nMQTT has hierarchical topic subscriptions built in. Subscribe to docs/workspace/# and get events for the entire subtree. The system already has MQTT for orchestrator communication.\n\nRefactor sync clients to:\n1. Subscribe to MQTT topic wildcards for document changes\n2. Replace SSE connections with MQTT subscriptions\n3. Leverage retained messages to avoid startup races\n\nThis is a cleaner architectural solution than the current workarounds.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-09T06:59:56.442529501Z","created_by":"jes","updated_at":"2026-01-09T09:59:46.360838434Z","closed_at":"2026-01-09T09:59:46.360838434Z","close_reason":"Implemented MQTT subscription support for sync client. Sync client now accepts --mqtt-broker and --workspace arguments. When MQTT broker is provided, uses MQTT edits topic subscriptions instead of SSE. Falls back to SSE when no MQTT broker is configured.","dependencies":[{"issue_id":"CP-o7h0","depends_on_id":"CP-s3zg","type":"blocks","created_at":"2026-01-09T07:00:29.670289188Z","created_by":"jes"},{"issue_id":"CP-o7h0","depends_on_id":"CP-g9e8","type":"blocks","created_at":"2026-01-09T07:05:45.141237639Z","created_by":"jes"},{"issue_id":"CP-o7h0","depends_on_id":"CP-1isz","type":"blocks","created_at":"2026-01-09T07:05:45.178783822Z","created_by":"jes"},{"issue_id":"CP-o7h0","depends_on_id":"CP-mg8x","type":"blocks","created_at":"2026-01-09T07:05:45.220137044Z","created_by":"jes"},{"issue_id":"CP-o7h0","depends_on_id":"CP-eiyx","type":"blocks","created_at":"2026-01-09T07:05:45.438577195Z","created_by":"jes"},{"issue_id":"CP-o7h0","depends_on_id":"CP-m4q7","type":"blocks","created_at":"2026-01-09T07:09:14.257136908Z","created_by":"jes"}]}
{"id":"CP-occ","title":"Sync should self-terminate and cleanup children when orchestrator dies","description":"When the orchestrator dies (killed or crashes), commonplace-sync sandbox processes should detect this and clean up themselves and their child processes.\n\nCurrent state:\n- PR_SET_PDEATHSIG is set in orchestrator's child spawn code, so sync gets SIGTERM when orchestrator dies\n- But sync may not be properly handling SIGTERM to kill its own children (bartleby, text-to-telegram)\n- Result: orphaned grandchildren when orchestrator dies\n\nProposed fix:\n- Sync should have a SIGTERM handler that kills its spawned child process before exiting\n- Or sync should set PR_SET_PDEATHSIG on its own children so they die when sync dies\n- Could also periodically check if parent PID changed to 1 (reparented to init = parent died)\n\nRelated: CP-a0g (orphaned processes survive restarts)","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T18:51:44.298112292Z","created_by":"jes","updated_at":"2026-01-03T07:21:04.262369886Z","closed_at":"2026-01-03T07:21:04.262369886Z","close_reason":"Implemented: PR_SET_PDEATHSIG on children (line 1091), process groups for cleanup","dependencies":[{"issue_id":"CP-occ","depends_on_id":"CP-a0g","type":"relates-to","created_at":"2026-01-02T18:51:55.364061961Z","created_by":"daemon"},{"issue_id":"CP-occ","depends_on_id":"CP-yw8","type":"relates-to","created_at":"2026-01-02T19:21:00.695841219Z","created_by":"daemon"}]}
{"id":"CP-ofu9","title":"SDK onChange should fire with initial content after get()","description":"When using cp.doc().onChange() and then calling get(), the onChange callback should fire with the initial content. Currently you have to manually call your handler after get() which is awkward and error-prone.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T01:30:20.380563109Z","created_by":"jes","updated_at":"2026-01-07T03:57:27.069303983Z","closed_at":"2026-01-07T03:57:27.06931229Z"}
{"id":"CP-og0","title":"Fork detection: binary file hash mismatch","description":"In CP-rs3 implementation, fork detection for binary files won't work correctly because:\n- Initial sync stores hash of base64-encoded content (via scan_directory_with_contents)\n- New file detection hashes raw bytes before encoding\n\nThis means identical binary files won't be detected as copies. Text files work correctly.\n\nFix: Hash raw file bytes in both cases, before any base64 encoding.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-30T20:07:20.775344004Z","created_by":"jes","updated_at":"2025-12-31T23:04:39.527009741Z","closed_at":"2025-12-31T23:04:39.527009741Z","close_reason":"Fixed hash consistency - both paths now hash raw bytes before base64"}
{"id":"CP-ogpr","title":"Optimize Yjs diff rendering without full history replay","description":"Currently commonplace-log replays all Yjs updates from the beginning to compute diffs. This is O(n) for n commits. Investigate ways to compute diffs more efficiently:\n\n1. Determine minimum context needed to correctly render a delete operation\n2. Consider caching intermediate Yjs states at checkpoints\n3. Look into Yjs snapshot/state-vector capabilities for partial replay\n4. Support lazy diff computation (only compute when needed for display)\n\nThe goal is to avoid replaying the entire history when viewing recent commits.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-07T23:34:40.146179338Z","created_by":"jes","updated_at":"2026-01-07T23:34:40.146179338Z"}
{"id":"CP-oi3","title":"JSONL file support","description":"Internally stored as a Yjs array of maps, but sync tool should check it out as JSONL format (one JSON object per line).","notes":"Needs design: How should JSONL ↔ Yjs array/map conversion work? Need to define CRDT structure for array of maps and bidirectional conversion logic.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:46:27.750869028Z","created_by":"jes","updated_at":"2025-12-30T20:49:43.561621338Z","closed_at":"2025-12-30T20:49:43.561621338Z","close_reason":"Merged PR #48 - JSONL file support"}
{"id":"CP-oji","title":"Handle rename events so new paths get sync tasks","description":"notify reports renames as `Modify(Name)` events; the current mapping treats all `is_modify()` as `DirEvent::Modified`. The modified handler only re-pushes schema and never starts new per-file sync tasks or removes old ones, so a rename leaves the new file untracked and the old path's tasks running until restart.\n\nConsider handling rename explicitly (or treating `Modify(Name)` as delete+create).","design":"Implemented by detecting `Modify(Name)` events separately from other modifications in the directory watcher. Rename events are now classified as delete+create based on RenameMode: From→Deleted, To→Created, Both/Any/Other→check path.exists() to determine direction. This ensures sync tasks are properly stopped for old paths and started for new paths.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-26T17:03:59.632881-08:00","updated_at":"2025-12-26T17:49:57.531409-08:00","closed_at":"2025-12-26T17:49:57.531409-08:00","close_reason":"Rename events now handled properly - Modify(Name) events are classified as delete+create based on RenameMode. Merged in PR #5.","labels":["enhancement","sync"]}
{"id":"CP-okn","title":"New orchestrator binary for process management","description":"Create a new orchestrator binary which invokes the document store and (once they exist) other processes like HTTP interface and sandboxed evaluator.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T21:56:47.418639497Z","created_by":"jes","updated_at":"2025-12-29T08:45:22.029787664Z","closed_at":"2025-12-29T03:12:03.336139764Z","close_reason":"Orchestrator binary implemented with process supervision, dependency ordering, restart policies with exponential backoff, and graceful shutdown. Merged in PR #16.","dependencies":[{"issue_id":"CP-okn","depends_on_id":"CP-8t3","type":"blocks","created_at":"2025-12-28T21:57:25.7353176Z","created_by":"daemon"}]}
{"id":"CP-onyq","title":"Deduplicate shadow watcher setup","description":"## Summary\nShadow watcher setup (watcher + handler + GC tasks) is duplicated three times in src/bin/sync.rs. Extract a shared helper to start and return these task handles.\n\n## Files to modify\n- src/bin/sync.rs\n- src/sync/watcher.rs (add start_shadow_watchers helper)\n\n## Implementation steps\n1. Add a helper that takes shadow_dir, inode_tracker, and returns (watcher, handler, gc) handles.\n2. Replace the repeated setup blocks with the helper.\n3. Keep behavior identical (channels, logging, GC interval).\n\n## Example\nBefore: three inline blocks spawning shadow_watcher_task, shadow_write_handler_task, shadow_gc_task.\nAfter: start_shadow_watchers(...) called in each place.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:57:08.947131607Z","created_by":"jes","updated_at":"2026-01-10T12:35:29.300184996Z","closed_at":"2026-01-10T12:35:29.300184996Z","close_reason":"Added spawn_shadow_tasks helper to watcher.rs that encapsulates channel creation and task spawning. Updated 3 call sites in sync.rs to use the new helper, reducing code duplication."}
{"id":"CP-oop","title":"Orchestrator should have a global lock so only one can run","description":"Multiple orchestrator instances can run simultaneously, causing duplicate sandbox processes and resource conflicts.\n\nExpected: Only one orchestrator should be able to run at a time for a given server.\nSolution: Implement a global lock (file-based or server-side) that prevents multiple orchestrators from starting.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-02T19:03:07.375300682Z","created_by":"jes","updated_at":"2026-01-02T19:33:04.471612643Z","closed_at":"2026-01-02T19:33:04.471612643Z","close_reason":"Implemented file-based global lock using fs2. PR #91 merged."}
{"id":"CP-oto3","title":"CI integration: sandbox stdio capture + sync","description":"Summary: Add CI coverage to verify that sandboxed process stdout/stderr are captured into __\u003cprocess\u003e.stdout.txt/__\u003cprocess\u003e.stderr.txt and synced back to the workspace.\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/sandbox-stdio-test.sh)\n- .github/workflows/ci.yml\n- docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md (if test references acceptance criteria)\n\nImplementation steps:\n1) Launch a sandboxed process that prints to stdout and stderr (e.g., `sh -c 'echo out; echo err 1\u003e\u00262'`).\n2) Wait for __\u003cname\u003e.stdout.txt and __\u003cname\u003e.stderr.txt to appear in the sandbox and in the workspace mirror.\n3) Assert file contents include the emitted lines.\n4) Run this script in CI and fail if the files are missing or empty.\n\nExample:\nBefore: __proc.stdout.txt missing or empty in workspace.\nAfter: __proc.stdout.txt contains \"out\" and __proc.stderr.txt contains \"err\".","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T23:15:05.543874023Z","created_by":"jes","updated_at":"2026-01-10T23:15:24.544125452Z"}
{"id":"CP-ow96","title":"WebSocket polish: path routing, keep-alive, backpressure (Phase 3)","description":"Polish the WebSocket implementation with production-ready features.\n\nRequires: CP-59h Phase 1 (y-websocket core) - DONE\n\n## Features\n\n1. Path-based WebSocket endpoint\n   - /ws/files/*path - resolve path to doc ID like SSE does\n   - Reuse path resolution from sse.rs\n\n2. Keep-alive ping/pong\n   - Send periodic pings to detect dead connections\n   - Clean up connections that don't respond\n\n3. Backpressure handling\n   - Bounded channel for outgoing messages\n   - Drop slow clients gracefully\n   - Log warnings for lagging connections\n\n4. Graceful shutdown\n   - Clean close messages to clients on server shutdown\n   - Room cleanup on connection drop\n\n## Files to modify\n\n- src/ws/handler.rs - Add path handler, keep-alive\n- src/ws/room.rs - Backpressure handling\n- src/ws/mod.rs - Add /ws/files/*path route","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-04T02:01:47.683938373Z","created_by":"jes","updated_at":"2026-01-04T03:05:04.832447027Z","closed_at":"2026-01-04T03:05:04.832465152Z","dependencies":[{"issue_id":"CP-ow96","depends_on_id":"CP-59h","type":"discovered-from","created_at":"2026-01-04T02:02:24.683120491Z","created_by":"jes"},{"issue_id":"CP-ow96","depends_on_id":"CP-9abt","type":"blocks","created_at":"2026-01-04T02:02:31.577998264Z","created_by":"jes"}]}
{"id":"CP-oxqa","title":"commonplace-log should compute diffs lazily (avoid long prefetch)","description":"Summary: commonplace-log currently computes diffs for every commit before printing output, causing long delays/hangs for large histories. It should stream output and compute diffs lazily.\n\nObserved:\n- commonplace log output.txt hangs for large append-only files.\n- --no-patch or -n is instant, indicating eager diff prefetch.\n\nFiles to modify:\n- src/bin/log.rs (diff computation + output pipeline)\n\nImplementation steps:\n1. Change log output to stream commits as they are fetched instead of precomputing all diffs.\n2. Compute diffs only for commits that will be printed (respect -n/filters early).\n3. Optionally add progress output when diff computation is expensive.\n\nExample:\ncommonplace log output.txt\n→ should start printing immediately, even for large histories.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-04T09:53:09.108066457Z","created_by":"jes","updated_at":"2026-01-04T10:44:08.057512519Z","closed_at":"2026-01-04T10:44:08.057512519Z","close_reason":"Implemented Yjs incremental replay - O(n) instead of O(n²), 992 commits now complete in 0.047s"}
{"id":"CP-p3zu","title":"Deduplicate DirEvent dispatch in sync","description":"## Summary\nDirectory event handling in sync.rs duplicates the same Created/Modified/Deleted dispatch blocks in multiple loops. Extract a shared handler to reduce duplication.\n\n## Files to modify\n- src/bin/sync.rs\n- src/sync/file_events.rs (optional: move handler there)\n\n## Implementation steps\n1. Create a helper that takes a DirEvent and dispatches to handle_file_created/modified/deleted.\n2. Use it in all directory event loops in sync.rs.\n3. Keep logging and error handling identical.\n\n## Example\nBefore: two separate match blocks for DirEvent.\nAfter: handle_dir_event(...) used in both loops.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:57:42.726078144Z","created_by":"jes","updated_at":"2026-01-10T09:57:42.726078144Z"}
{"id":"CP-pga5","title":"Use Topic::edits in http_gateway SSE","description":"## Summary\nhttp_gateway/sse.rs builds MQTT edits topic via format!(\"{}/edits/{}\", ...) instead of using Topic::edits. Use the shared topic builder to avoid drift.\n\n## Files to modify\n- src/http_gateway/sse.rs\n- src/mqtt/topics.rs (if needed for helper)\n\n## Implementation steps\n1. Replace string formatting with Topic::edits(\u0026workspace, \u0026id).to_topic_string().\n2. Keep behavior identical.\n\n## Example\nBefore: format!(\"{}/edits/{}\", workspace, id).\nAfter: Topic::edits(\u0026workspace, \u0026id).to_topic_string().","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:08:58.363298212Z","created_by":"jes","updated_at":"2026-01-10T11:29:17.074967611Z","closed_at":"2026-01-10T11:29:17.074967611Z","close_reason":"Replaced format! string with Topic::edits().to_topic_string() to use the shared topic builder."}
{"id":"CP-pgx","title":"MCP server for firing commands to commonplace paths","description":"An MCP server that exposes commonplace document paths as callable tools. MCP clients can invoke commands on commonplace documents, bridging LLM tool-use with the commonplace reactive document system.","design":"Needs design discussion: High-level feature request without implementation details. Requires specification of API design, data model, and integration points before implementation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.349217-08:00","updated_at":"2025-12-30T01:43:40.761417202Z","closed_at":"2025-12-30T01:43:40.761417202Z","close_reason":"Implemented commonplace-mcp server. PR #31 created - pending merge.","dependencies":[{"issue_id":"CP-pgx","depends_on_id":"CP-ksf","type":"related","created_at":"2025-12-28T22:50:33.586139-08:00","created_by":"daemon"}]}
{"id":"CP-pk4h","title":"Sync pulls default {} content from newly created server documents, corrupting local files","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T05:53:18.140074334Z","created_by":"jes","updated_at":"2026-01-10T06:11:52.212590981Z","closed_at":"2026-01-10T06:11:52.212590981Z","close_reason":"Fixed by using proper CRDT ancestry checking. Commit 065d908.","comments":[{"id":26,"issue_id":"CP-pk4h","author":"jes","text":"Root cause: When the server creates a new document via get_or_create_with_id, it uses default_content() which returns {} for JSON files. If the sync client then fetches HEAD from this newly-created document (due to initial_sync=server), it overwrites the local file with {}. This corrupted all __processes.json files to have {} content instead of their actual process definitions. Potential fixes: 1) Don't overwrite local files if server has default/empty content 2) Push local content first before pulling from server 3) Detect when server document was just created and prefer local content","created_at":"2026-01-10T05:53:26Z"},{"id":27,"issue_id":"CP-pk4h","author":"jes","text":"INSIGHT: This is a failure to use merkle-CRDTs correctly. The fix should not be workarounds like echo detection or written_schemas tracking. The correct approach:\n\n1. Before overwriting local content, check ancestry:\n   - Fetch server HEAD (cid)\n   - Check local state (last known cid from state file)\n   - If server is ancestor of local → push local to server\n   - If local is ancestor of server → pull from server\n   - If diverged → merge\n\n2. The /docs/{id}/is-ancestor endpoint already exists for this purpose\n\n3. The initial_sync=server mode that blindly pulls is fundamentally wrong when local has commits that server doesn't have\n\nFix approach: Modify sync to use proper CRDT ancestry checking before any content overwrites.","created_at":"2026-01-10T05:57:12Z"}]}
{"id":"CP-pk78","title":"Script watch map not refreshed on config edits (P2 from codex)","description":"When the recursive watcher handles edits to an existing __processes.json, the script_watches map is only rebuilt if a processes file was added or removed. Edits that add/change an 'evaluate' entry in an already-watched file are reconciled (the process is started), but the map that triggers restarts on script changes remains stale. In that scenario, subsequent edits to the new script document will never restart the process until the orchestrator itself is restarted or a new __processes.json is added/removed. Consider rebuilding script_watches whenever fetch_and_reconcile changes processes, not just on added/removed files. File: src/orchestrator/discovered_manager.rs:1353-1362","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T07:07:47.641236109Z","created_by":"jes","updated_at":"2026-01-07T01:15:03.217350389Z","closed_at":"2026-01-07T01:15:03.217350389Z","close_reason":"Always rebuild script_watches during periodic re-discovery, not just when files added/removed"}
{"id":"CP-pkzx","title":"Clean up dead subdirs_migrated code from reconciler","description":"After removing inline subdirectory support (CP-e3jr), the subdirs_migrated field in MigrationResult is always 0. Remove this dead code:\n- Remove subdirs_migrated field from MigrationResult struct (reconciler.rs:17)\n- Remove logging code that checks subdirs_migrated \u003e 0 (reconciler.rs:66-71)\n- Update any callers that reference subdirs_migrated","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-08T18:35:08.616748246Z","created_by":"jes","updated_at":"2026-01-08T19:37:21.739875262Z","closed_at":"2026-01-08T19:37:21.739875262Z","close_reason":"Removed dead subdirs_migrated code"}
{"id":"CP-pr05","title":"Add --force-push flag to overwrite server content without CRDT merge","description":"Add --force-push flag to commonplace-sync:\n\n**Current behavior**: \n- Local edits are sent as CRDT updates based on the last checked-out commit\n- Server merges these with any concurrent server-side edits\n\n**--force-push behavior**:\n- Local file content replaces server HEAD entirely\n- No CRDT merge - local wins unconditionally\n- Uses /docs/{id}/replace endpoint instead of /docs/{id}/edit\n\nUse case: \n- Source-of-truth files where local is authoritative\n- Recovery scenarios where you want to restore from local\n- Combined with --push-only for one-way authoritative sync\n\nImplementation:\n- On local file change, POST full content to /replace instead of computing diff\n- Skip tracking of last-synced CID since we're not doing incremental updates","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T20:04:09.234108668Z","created_by":"jes","updated_at":"2026-01-04T00:26:46.165038252Z","closed_at":"2026-01-04T00:26:46.165038252Z","close_reason":"Implemented --force-push flag for file mode sync"}
{"id":"CP-pta","title":"Rename /nodes API to /docs throughout codebase","design":"Completed implementation. PR #20 created. Waiting for merge.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-29T08:37:04.362222409Z","created_by":"jes","updated_at":"2025-12-29T09:40:55.11574492Z","closed_at":"2025-12-29T09:40:55.11574492Z","close_reason":"Renamed /nodes API to /docs throughout codebase. Merged all node endpoints into document API. PR #20 merged."}
{"id":"CP-px7m","title":"Deduplicate MQTT JSON parse/encode helpers","description":"## Summary\nMQTT commands and edits handlers repeat JSON parse/serialize + error mapping patterns. Extract shared helpers to standardize InvalidMessage errors and reduce boilerplate.\n\n## Files to modify\n- src/mqtt/commands.rs\n- src/mqtt/edits.rs\n- src/mqtt/mod.rs or src/mqtt/json.rs (new helpers)\n\n## Implementation steps\n1. Add helpers like parse_json\u003cT\u003e(payload) and encode_json\u003cT\u003e(value) that return MqttError::InvalidMessage on failure.\n2. Replace repeated serde_json::from_slice/to_vec + map_err blocks in commands/edits with the helpers.\n3. Keep error messages consistent across all MQTT handlers.\n\n## Example\nBefore: each handler repeats serde_json::from_slice(...).map_err(...).\nAfter: mqtt::json::parse_json(payload) used everywhere.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:41:57.001610295Z","created_by":"jes","updated_at":"2026-01-10T09:41:57.001610295Z"}
{"id":"CP-pxi5","title":"Reuse workspace timestamp formatter in replay","description":"## Summary\nTimestamp formatting is duplicated in bin/replay.rs even though workspace.rs provides format_timestamp/format_timestamp_short. Reuse workspace helpers to avoid drift in date formatting.\n\n## Files to modify\n- src/bin/replay.rs (remove local format_timestamp)\n- src/workspace.rs (already has format_timestamp)\n\n## Implementation steps\n1. Remove the local format_timestamp function from bin/replay.rs.\n2. Import and use workspace::format_timestamp instead.\n3. Ensure output formatting stays identical.\n\n## Example\nBefore: replay.rs defines its own format_timestamp.\nAfter: replay.rs calls workspace::format_timestamp.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:50:28.022509604Z","created_by":"jes","updated_at":"2026-01-10T11:58:21.526681912Z","closed_at":"2026-01-10T11:58:21.526681912Z","close_reason":"Removed local format_timestamp, now uses workspace::format_timestamp"}
{"id":"CP-q3lm","title":"Deduplicate nested schema traversal","description":"## Summary\nNested schema traversal is duplicated between write_nested_schemas_recursive (schema_io.rs) and push_nested_schemas_recursive (dir_sync.rs). Extract a shared traversal helper for node-backed directories to reduce drift.\n\n## Files to modify\n- src/sync/schema_io.rs (write_nested_schemas_recursive)\n- src/sync/dir_sync.rs (push_nested_schemas_recursive)\n- src/sync/schema_traversal.rs (new shared traversal helper)\n\n## Implementation steps\n1. Extract a traversal function that iterates node-backed directories and yields (node_id, subdir_path, entry) callbacks.\n2. Use it in write_nested_schemas (fetch+write) and push_nested_schemas (read+push) to share the recursion logic.\n3. Keep current behavior differences (write vs push) in the callbacks.\n\n## Example\nBefore: two separate recursive traversals over FsSchema entries.\nAfter: one traversal with two different handlers.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T09:11:41.20806447Z","created_by":"jes","updated_at":"2026-01-10T09:51:29.385306996Z","comments":[{"id":29,"issue_id":"CP-q3lm","author":"jes","text":"After analysis: These functions do opposite operations (server→disk vs disk→server) with different parameters, return types, and deduplication strategies. Extracting a shared async traversal helper would add complexity (async callbacks/closures in Rust are non-trivial) without significant benefit. The ~50 lines of iteration code overlap is offset by the divergent logic. Recommend: keep separate or revisit when Rust async closures stabilize.","created_at":"2026-01-10T09:51:24Z"}]}
{"id":"CP-q3m","title":"CLI tool with relative path support in synced directories","description":"The command-line command-issuing tool should work inside a synced directory such that you can use relative paths to send commands to the relevant document.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:48:46.353930244Z","created_by":"jes","updated_at":"2025-12-30T18:24:55.356003983Z","closed_at":"2025-12-30T18:24:55.356003983Z","close_reason":"Added relative path support to commonplace-cmd - PR #45 merged"}
{"id":"CP-q52x","title":"CI integration: newly created node-backed subdir propagates without restart","description":"Summary: Add a CI integration test that creates a new node-backed subdirectory after sync start and verifies other clients receive the new directory/files without requiring a restart.\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/subdir-discovery-test.sh)\n- .github/workflows/ci.yml\n\nImplementation steps:\n1) Start server + workspace sync + sandbox sync.\n2) Create a new subdirectory and a file within it in one client after sync is running.\n3) Assert that the subdirectory schema is created on server and the file appears in the other client without restarting.\n4) Run in CI to catch regressions in subdir SSE/MQTT subscription handling.\n\nExample:\nBefore: new subdir appears only after restart.\nAfter: new subdir and file appear within N seconds.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T23:17:21.116067869Z","created_by":"jes","updated_at":"2026-01-10T23:17:39.748243701Z"}
{"id":"CP-q595","title":"P2: Doc handles created after start never subscribe","description":"After cp.start() has been called, later calls to doc.onChange() or doc.onEvent() only push the handle into pendingDocHandles, but no code ever activates those subscriptions unless get() is called manually. Consider activating subscriptions immediately when mqttStarted is true.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T06:28:05.025980861Z","created_by":"jes","updated_at":"2026-01-07T01:05:30.243209172Z","closed_at":"2026-01-07T01:05:30.243209172Z","close_reason":"Made activateSubscriptions() idempotent; onChange/onEvent now activate immediately when called after cp.start()"}
{"id":"CP-q5xt","title":"Migrate uuid_map.rs to use fetch_head helper (4 call sites)","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.340781397Z","created_by":"jes","updated_at":"2026-01-10T18:18:17.340781397Z"}
{"id":"CP-q7n3","title":"commonplace log shows misleading diffs for merge commits","description":"The commonplace log tool applies Yjs updates in chronological (timestamp) order and computes diffs between consecutive commits. However, when there are merge commits, the 'previous' commit in timestamp order may not be the actual parent, so the diff shown is misleading.\n\nExample from bartleby/output.txt:\n- Commit c8877826 is a merge with parents [99788154, 4cf8e3ba]\n- But the log shows diff against the previous timestamp commit, not against either parent\n- This makes the commit history look like nonsensical edits when it's actually showing the wrong base\n\nThe log tool should either:\n1. Show diffs against the actual parent commit(s), not timestamp-ordered predecessor\n2. For merge commits, show diffs against each parent separately\n3. At minimum, indicate when a commit is a merge and which parents it has\n\nThe underlying Yjs replay is correct (CRDTs are order-independent), but the diff visualization is confusing for merge commits.","notes":"Adding proper DAG topology visualization with ASCII art like git log --graph","status":"closed","priority":3,"issue_type":"bug","created_at":"2026-01-07T22:59:52.114403937Z","created_by":"jes","updated_at":"2026-01-10T02:50:19.391311934Z","closed_at":"2026-01-10T02:50:19.391311934Z","close_reason":"Completed"}
{"id":"CP-qf7t","title":"CI integration: sandbox process launches in sandbox cwd","description":"Summary: Add a CI integration test that verifies orchestrator launches a sandboxed process with cwd set to the sandbox directory and that the process can read/write sandbox files using relative paths.\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/sandbox-integration-test.sh)\n- .github/workflows/ci.yml (add integration job/step)\n- tests/ (if adding a Rust integration test harness)\n\nImplementation steps:\n1) Create a temp workspace with __processes.json defining a simple process (e.g., `sh -c 'pwd \u003e __proc.stdout.txt; echo ok \u003e output.txt'`).\n2) Start server + orchestrator pointing at the temp workspace and wait for the process to run.\n3) Assert that output.txt exists inside the sandbox directory and contains expected content, and that captured stdout includes the sandbox path.\n4) Wire the script into CI (new job or step after cargo test) so regressions are caught.\n\nExample:\nBefore: process runs with wrong cwd, relative writes land outside sandbox.\nAfter: process writes output.txt inside sandbox and stdout shows sandbox path.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T23:14:29.156485483Z","created_by":"jes","updated_at":"2026-01-10T23:14:48.82662764Z","comments":[{"id":49,"issue_id":"CP-qf7t","author":"jes","text":"Covers acceptance criteria P4-P6 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nP4: Bartleby is discovered and running (source: /)\nP5: Text-to-telegram is discovered and running (source: /text-to-telegram)\nP6: File-tmux-file is discovered and running (source: /tmux)\n\nVerify sandbox processes show correct CWD in commonplace-ps (/tmp/commonplace-sandbox-*).","created_at":"2026-01-11T00:41:36Z"}]}
{"id":"CP-qge2","title":"commonplace-beads-bridge binary to sync bd issues to commonplace","description":"New binary that wraps commonplace-sync for .beads/issues.jsonl files. Features:\n- Discovers repo and .beads/issues.jsonl\n- Creates .cbd.json with path mapping\n- Default path: {workspace}/cbd/data/{repo-name}.issues.jsonl\n- Pass-through flags: --push-only, --pull-only, --force-push\n- Configured in commonplace.json for orchestrator management","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T06:43:46.919010395Z","created_by":"jes","updated_at":"2026-01-05T06:51:40.052855937Z","closed_at":"2026-01-05T06:51:40.052855937Z","close_reason":"Implemented commonplace-beads-bridge binary that wraps commonplace-sync for .beads/issues.jsonl files. Auto-discovers repo, creates .cbd.json, supports --push-only/--pull-only/--force-push.","dependencies":[{"issue_id":"CP-qge2","depends_on_id":"CP-6kpq","type":"blocks","created_at":"2026-01-05T06:44:25.329308715Z","created_by":"jes"}]}
{"id":"CP-qmk","title":"Fix P1: Preserve schema deletions when pushing updates (PR #7)","description":"From PR #7 Codex review: When pushing schema updates, deletions are not being preserved. The code always posts an edit built by create_yjs_map_update but this may not handle entry deletions properly. Need to investigate and fix.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-26T23:24:21.270524-08:00","updated_at":"2025-12-27T00:14:06.786038-08:00","closed_at":"2025-12-27T00:14:06.786038-08:00","close_reason":"Preserve schema deletions implemented. Added state field to HEAD response, client now applies server CRDT state before making changes so deletions create proper tombstones. Merged in PR #9."}
{"id":"CP-qt6z","title":"Implement flock-aware sync spec","description":"Summary: Implement the flock-aware sync protocol described in docs/flock-aware-sync-spec.md so inbound writes never overwrite local edits and inode tracking prevents lost updates/deletions.\\n\\nFiles to modify:\\n- docs/flock-aware-sync-spec.md (reference spec)\\n- src/sync/state.rs\\n- src/sync/watcher.rs\\n- src/sync/file_sync.rs\\n- src/sync/dir_sync.rs\\n- src/sync/sse.rs\\n- tests (add regression coverage for flock gating and pending inbound/outbound)\\n\\nImplementation steps:\\n1. Add per-path state (pending_outbound, pending_inbound, inode) to track uploads and queued server writes.\\n2. Gate inbound writes on pending_outbound ancestry; queue pending_inbound until the server update includes local edits.\\n3. Use flock (LOCK_EX/LOCK_NB) to detect active agents and delay atomic renames until edits are uploaded and merged.\\n4. Integrate inode tracking/shadow links so writes from old inodes are merged instead of lost.\\n5. Add tests that cover: local edit during inbound, deletion staying deleted, and atomic replace with flocked editor.\\n\\nExample:\\nBefore: server update overwrites local edit or resurrects delete.\\nAfter: server update waits/merges so local edits are preserved and deletes stay deleted.\\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-07T23:28:25.38938468Z","created_by":"jes","updated_at":"2026-01-08T07:18:17.137624423Z","closed_at":"2026-01-08T07:18:17.137624423Z","close_reason":"Implemented flock-aware sync with ancestry checking, flock coordination, and integration tests. Includes bug fixes for queued inbound writes."}
{"id":"CP-qyj","title":"TypeScript support for sandboxed evaluator","description":"Extend the JS sandbox evaluator to support TypeScript. This could mean:\n- Transpiling TS → JS before evaluation\n- Supporting .ts file extensions in sandbox\n- Type definitions for the sandbox API (dependencies, output documents, events)\n\nThis makes the sandbox more ergonomic for larger/complex reactive transforms.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-28T23:53:18.488271-08:00","updated_at":"2025-12-28T23:53:18.488271-08:00","labels":["future-work"],"dependencies":[{"issue_id":"CP-qyj","depends_on_id":"CP-egb","type":"blocks","created_at":"2025-12-28T23:53:18.489746-08:00","created_by":"daemon"}]}
{"id":"CP-r2a","title":"Fix sync race condition: upload_task sends stale content during B-\u003eA sync","description":"When a server edit arrives via SSE, the handle_server_edit function fetches HEAD and writes to the local file. However, the file watcher can trigger upload_task with stale file content before the write completes, causing the old content to be synced back to the server.\n\nRoot cause: Echo detection relies on comparing file content to last_written_content, but there's a window between SSE notification and file write where upload_task can read stale content.\n\nPossible fixes:\n1. Add a flag to pause upload_task during server writes\n2. Use file modification timestamp comparison\n3. Add sequence numbers to track edit origin","design":"## Extended Barrier Pattern - REVISED AFTER CODEX REVIEW\n\n### Codex-Identified Issues:\n\n**CRITICAL: Partial writes can still trigger uploads**\nIf watcher fires during our write, content differs from pending, we upload partial/corrupted content. FIX: Don't immediately upload on mismatch - retry/re-read until content stabilizes or matches pending.\n\n**HIGH: Multiple server edits clobber pending fields**\nIf handle_server_edit runs while barrier is up, it overwrites pending_write_*, causing misclassification when first write's watcher event arrives. FIX: Use generation token (monotonic write_id) so barrier maps to exact write.\n\n**HIGH: Upload with stale parent_cid may fail**\nServer's replace endpoint may reject if parent doesn't match HEAD. FIX: Refresh HEAD before upload, or use CRDT edit endpoint for merges.\n\n**MEDIUM: Window between re-check and barrier set**\nLocal edit can slip in between re-check and write_in_progress=true. FIX: Do both atomically under write lock.\n\n**MEDIUM: Read→drop→write lock pattern is risky**\nState can change between drop(read) and acquire(write). FIX: Re-check under write lock, or use single write lock.\n\n**LOW: Watcher never fires leaves barrier stuck**\nlast_written_cid never updates, next upload uses stale parent. FIX: Timeout/fallback to finalize barrier.\n\n---\n\n### Improved Design: Token-Based Barrier\n\n```rust\nstruct SyncState {\n    last_written_cid: Option\u003cString\u003e,\n    last_written_content: String,\n    // Token-based barrier:\n    current_write_id: u64,  // Monotonic counter\n    pending_write: Option\u003cPendingWrite\u003e,\n}\n\nstruct PendingWrite {\n    write_id: u64,\n    content: String,\n    cid: String,\n    started_at: Instant,\n}\n```\n\n**handle_server_edit:**\n1. Acquire write lock\n2. If pending_write exists and not timed out, queue/skip this edit\n3. Increment current_write_id\n4. Set pending_write = Some(PendingWrite { write_id, content, cid, started_at })\n5. Release lock\n6. Write file\n7. Do NOT clear barrier (upload_task does it)\n\n**upload_task:**\n1. Read file content\n2. Acquire write lock (not read!)\n3. If pending_write exists:\n   a. If content == pending.content AND file stable (re-read matches):\n      - Clear pending, update last_written_*, done (echo)\n   b. If content != pending.content:\n      - Re-read file after short delay (50ms)\n      - If still differs after N retries: user edited, clear pending, upload\n   c. If pending.started_at \u003e 30s ago: timeout, clear pending\n4. Normal echo detection if no pending\n\n**Key improvements:**\n- Retry on mismatch instead of immediate upload (avoids partial writes)\n- Token prevents clobbering from concurrent server edits\n- Single write lock avoids ABA bugs\n- Timeout prevents stuck barrier\n- Refresh HEAD before upload on conflict path","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-29T16:02:47.797530633Z","created_by":"jes","updated_at":"2025-12-29T20:50:11.443695514Z","closed_at":"2025-12-29T20:50:11.443695514Z","close_reason":"Token-based write barrier implemented to fix sync race condition. Prevents upload_task from sending stale content during server-to-local sync. 13+ codex review iterations, all P1 issues fixed. Merged in PR #23."}
{"id":"CP-raid","title":"Duplicate deliveries in file-based IO (bartleby-style)","description":"Summary: In bartleby-style file read/write patterns (file-based input/output), messages sometimes get delivered twice; likely caused by duplicate watcher events or missing dedupe on upload.\n\nFiles to modify:\n- src/sync/watcher.rs\n- src/sync/file_sync.rs\n- src/sync/state.rs\n- tests (add a file-based IO dedupe regression test)\n\nImplementation steps:\n1. Reproduce with a file-based input/output pipeline (write to input file, observe output consumer) and capture when duplicates occur.\n2. Inspect watcher event handling (IN_MODIFY vs IN_CLOSE_WRITE) and upload batching/debouncing logic to identify double-send triggers.\n3. Add dedupe safeguards (e.g., ignore duplicate content hashes within a short window, or ensure only close-write triggers upload).\n4. Update state tracking to persist last-sent hash/commit per file to prevent re-sends.\n5. Add a regression test that writes a single update and asserts exactly one delivery.\n\nExample:\nBefore: Writing once to input file results in two downstream deliveries.\nAfter: Each write yields exactly one delivery.\n","notes":"Root cause was CP-nvc5 (missing inode tracking in directory sync mode). Now fixed - inode tracking enabled in run_directory_mode and run_exec_mode.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-06T03:50:29.132906948Z","created_by":"jes","updated_at":"2026-01-07T22:29:47.077064581Z","closed_at":"2026-01-07T22:29:47.077072492Z","dependencies":[{"issue_id":"CP-raid","depends_on_id":"CP-n5mj","type":"blocks","created_at":"2026-01-06T06:37:21.257315139Z","created_by":"jes"},{"issue_id":"CP-raid","depends_on_id":"CP-nvc5","type":"blocks","created_at":"2026-01-07T22:09:24.413670409Z","created_by":"jes"}],"comments":[{"id":22,"issue_id":"CP-raid","author":"jes","text":"Duplicates still occurring on 2026-01-07 per bartleby history.jsonl (entries 37-38, 47-48)","created_at":"2026-01-07T21:31:48Z"}]}
{"id":"CP-rci9","title":"Deduplicate SSE/MQTT directory subscription logic","description":"## Summary\nDirectory/subdirectory subscription handling is duplicated between SSE and MQTT paths in src/sync/subscriptions.rs. Extract shared handlers for schema-change processing and subdir spawning to reduce drift.\n\n## Files to modify\n- src/sync/subscriptions.rs (refactor SSE/MQTT tasks)\n- src/sync/subscriptions_helpers.rs (new shared helpers)\n\n## Implementation steps\n1. Extract shared logic for handling a schema change (cleanup, sync new files, create nested dirs).\n2. Extract shared logic for spawning newly discovered node-backed subdir watchers.\n3. Have both SSE and MQTT tasks call these helpers on edit events.\n\n## Example\nBefore: SSE and MQTT tasks contain near-identical edit handling blocks.\nAfter: shared handle_schema_edit(...) used by both, with transport-specific message receipt only.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T09:28:39.827919602Z","created_by":"jes","updated_at":"2026-01-10T10:17:06.820324717Z","closed_at":"2026-01-10T10:17:06.820324717Z","close_reason":"Added handle_subdir_edit() and collect_new_subdirs() shared helpers. Updated subdir_sse_task and subdir_mqtt_task to use these helpers, eliminating ~100 lines of duplicate code."}
{"id":"CP-rh2f","title":"SDK OutputHandleImpl.set() uses Y.Text for JSON/JSONL but server expects Y.Map/Y.Array","description":"Codex review found: OutputHandleImpl.set() always edits Y.Text 'content' for all output types. But JSON documents use Y.Map and JSONL uses Y.Array on server. This mismatch means JSON/JSONL outputs may not update correctly.\n\nHowever, filter-open IS producing output (13 lines in commonplace-issues.open.jsonl), so either:\n1. File sync is converting the text content to proper format\n2. The server reconciles text updates somehow\n3. Initial write works differently than updates\n\nNeeds investigation to confirm actual behavior and fix if needed.\n\nFile: workspace/sdk/mod.ts:289-316","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-07T03:10:32.447429803Z","created_by":"jes","updated_at":"2026-01-07T03:28:35.311231059Z","closed_at":"2026-01-07T03:28:35.311231059Z","close_reason":"Fixed SDK OutputHandleImpl to use correct Yjs types: Y.Map for JSON objects, Y.Array for JSON arrays and JSONL. The writeYjsContent() function now dynamically selects the type based on content, matching server behavior."}
{"id":"CP-rrw8","title":"Convert push_schema_to_server to use push_json_content","description":"Treat .commonplace.json as a normal JSON document and use push_json_content/create_yjs_json_update instead of bespoke fetch-modify-push logic. This is the core CRDT reuse win from CP-nonc.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T07:53:56.45113364Z","created_by":"jes","updated_at":"2026-01-10T08:06:20.937360184Z","closed_at":"2026-01-10T08:06:20.937360184Z","close_reason":"Refactored: added push_json_content_merge with merge mode, unified URL builders in push_schema_to_server and delete_schema_entry, extracted json_content_equal helper for feedback loop prevention"}
{"id":"CP-rs3","title":"File copy in sync directory triggers document fork","description":"When a new file appears in a synced directory with content that exactly matches another file already synced by the same process, automatically fork the original document rather than creating an unrelated new document. Detection: pure content hash matching against all currently synced files.","notes":"PR #47 created. Codex review requested. P2 limitation filed as CP-og0 for binary files.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:58:58.555594612Z","created_by":"jes","updated_at":"2025-12-30T20:22:08.593492297Z","closed_at":"2025-12-30T20:22:08.593492297Z","close_reason":"Merged PR #47 - file copy triggers document fork via content hash matching"}
{"id":"CP-rxf4","title":"Add process name to merkle commits for tracking who made changes","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-10T22:43:19.016885332Z","created_by":"jes","updated_at":"2026-01-10T22:43:19.016885332Z"}
{"id":"CP-s3zg","title":"Audit HTTP API and ensure full MQTT coverage","description":"Before refactoring sync to MQTT (CP-o7h0), audit the HTTP API to ensure all operations are available over MQTT.\n\nCurrent HTTP endpoints to audit:\n- POST /docs - Create document\n- GET /docs/{id} - Get content\n- DELETE /docs/{id} - Delete document\n- GET /docs/{id}/info - Get metadata\n- GET /docs/{id}/head - Get HEAD (cid, content, Yjs state)\n- POST /docs/{id}/commit - Persist Yjs update\n- POST /docs/{id}/edit - Send Yjs update\n- POST /docs/{id}/replace - Replace content with diff\n- POST /docs/{id}/fork - Fork document\n- GET /sse/docs/{id} - SSE subscription (replace with MQTT topic)\n\nFor each, ensure there's an equivalent MQTT topic/message pattern:\n- Request/response patterns (use reply-to topics)\n- Pub/sub for document changes\n- Wildcard subscriptions for tree-based watching\n\nThis audit should inform the MQTT refactor design.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T07:00:24.516868018Z","created_by":"jes","updated_at":"2026-01-09T07:05:51.902956541Z","closed_at":"2026-01-09T07:05:51.902956541Z","close_reason":"Audit complete. Created 9 beads for missing MQTT coverage: CP-g9e8 (delete), CP-1isz (get content), CP-mg8x (info), CP-mxt2 (replace), CP-6ke7 (fork), CP-ef0e (ancestry), CP-2cql (fs-root), CP-6o2m (path-based), CP-eiyx (wildcard subscriptions - P1 blocker)"}
{"id":"CP-s7sx","title":"CI integration: workspace \u003c-\u003e sandbox file sync create/edit/delete","description":"Summary: Add a CI integration test that exercises end-to-end sync between workspace and sandbox: create, edit, and delete files, verifying propagation both directions.\n\nFiles to modify:\n- scripts/integration-test.sh (or new scripts/sandbox-sync-acceptance-test.sh)\n- .github/workflows/ci.yml\n- docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md (reference expected behaviors)\n\nImplementation steps:\n1) Start server, sync, and orchestrator on a temp workspace with a sandbox process.\n2) Create a file in workspace; assert it appears in sandbox with matching content.\n3) Edit the file in sandbox; assert workspace reflects the change.\n4) Delete the file in workspace; assert sandbox deletes it.\n5) Run in CI with timeouts/retries to avoid flakes.\n\nExample:\nBefore: file create/edit/delete regressions slip through.\nAfter: CI fails if any step fails to propagate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T23:16:14.189658284Z","created_by":"jes","updated_at":"2026-01-10T23:16:33.332414091Z","comments":[{"id":47,"issue_id":"CP-s7sx","author":"jes","text":"Covers acceptance criteria C1-C6, E1-E6, D1-D4 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nFile Creation (C1-C6): Create files in workspace, verify they appear in sandbox with correct content\nEdit Propagation (E1-E6): Edit files in workspace/sandbox, verify changes sync bidirectionally\nFile Deletion (D1-D4): Delete files in workspace, verify they're removed from sandbox","created_at":"2026-01-11T00:41:33Z"}]}
{"id":"CP-se0","title":"One-time scheduled events (at-style)","description":"Schedule events to fire at a specific point in the future. Unlike cron (recurring), these are one-shot:\n- Reminders\n- Delayed actions\n- Future-dated triggers\n- Timeout/deadline events\n\nCould share infrastructure with cron scheduling but with single-fire semantics.","design":"OPEN QUESTIONS for implementation:\n1. How to persist scheduled events across restarts?\n2. What happens if target path doesn't exist when event fires?\n3. Should there be a 'list scheduled events' API?\n4. Can events be cancelled before they fire?\n5. What precision - seconds, milliseconds, or just minutes?\n6. Same storage and service questions as CP-eyi","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-28T23:53:18.644504-08:00","updated_at":"2025-12-30T01:44:47.476156216Z","labels":["future-work"],"dependencies":[{"issue_id":"CP-se0","depends_on_id":"CP-eyi","type":"related","created_at":"2025-12-28T23:53:29.351623-08:00","created_by":"daemon"}]}
{"id":"CP-sgme","title":"SDK should provide JS-native types based on output file extension","description":"Currently the SDK gives scripts plain text strings and expects plain text output. Instead:\n\n1. Input: When reading documents, SDK should parse based on file extension:\n   - .json → parsed JSON object\n   - .jsonl → array of parsed JSON objects (or iterator)\n   - .txt/.md → string (current behavior)\n\n2. Output: When writing via cp.output.set(), SDK should:\n   - Read output extension from __processes.json 'output' field\n   - Automatically serialize JS objects to appropriate format\n   - .json → JSON.stringify\n   - .jsonl → join with newlines after JSON.stringify each item\n   - .txt → string as-is\n\nThis makes evaluate scripts more ergonomic - work with native JS types instead of manual parsing/serializing.","design":"API approach: Make get()/set() return/accept unknown types. Content type detected from file extension (.json→object, .jsonl→array, .txt/.md→string). Breaking change but SDK is new. Add parseContent/serializeContent utilities.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-07T01:45:14.942480482Z","created_by":"jes","updated_at":"2026-01-07T01:52:23.848852673Z","closed_at":"2026-01-07T01:52:23.848852673Z","close_reason":"Implemented typed content based on file extension. SDK now auto-parses .json→object, .jsonl→array and auto-serializes on output. Scripts work with native JS types instead of strings."}
{"id":"CP-spgg","title":"Deduplicate WebSocket error mapping","description":"## Summary\nWebSocket handler maps protocol/room errors to String repeatedly in handle_binary_message. Provide a shared error conversion helper to keep error formatting consistent.\n\n## Files to modify\n- src/ws/handler.rs\n- src/ws/room.rs or src/ws/errors.rs (new helper)\n\n## Implementation steps\n1. Add a helper to convert RoomError/ProtocolError into a consistent String (or a dedicated error enum).\n2. Use it in handle_binary_message instead of repeated map_err(|e| e.to_string()).\n3. Keep log messages and behavior unchanged.\n\n## Example\nBefore: multiple map_err(|e| e.to_string()) calls.\nAfter: map_err(ws_error_to_string) helper.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:42:23.826639626Z","created_by":"jes","updated_at":"2026-01-10T09:42:23.826639626Z"}
{"id":"CP-stdf","title":"Support comment key in __processes.json for documentation","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-10T22:50:00.144197826Z","created_by":"jes","updated_at":"2026-01-10T22:50:00.144197826Z"}
{"id":"CP-sxhh","title":"Deduplicate SSE stream setup between NodeSSEStream and NatsSSEStream","description":"Summary: NodeSSEStream, NatsSSEStream, and SSEStream all implement the same SSE headers, keep-alive timer, write/keepAlive helpers, and close detection; extract a shared helper to reduce drift.\n\nFiles to modify:\n- commonplaced-2025/src/server/nodeSSE.ts\n- commonplaced-2025/src/server/natsSSE.ts\n- commonplaced-2025/src/server/sse.ts\n- commonplaced-2025/src/mcp/streaming.ts (or new helper module near SSE streams)\n\nImplementation steps:\n1) Add a shared SSE stream base/helper that handles header setup, keepAlive timer, write/writeMessage, and close detection.\n2) Update NodeSSEStream and SSEStream to compose/extend the shared helper and keep their minimal surface area.\n3) Update NatsSSEStream to reuse the helper for header setup and keepAlive logic, keeping NATS subscription cleanup specific to that class.\n\nExample:\nBefore: three SSE stream implementations duplicate header setup and timer logic.\nAfter: factories call a shared helper (ex: createBaseSseStream(response, ...) or BaseSseStream) and only add NATS-specific subscribe/cleanup in natsSSE.ts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T10:10:51.685963955Z","created_by":"jes","updated_at":"2026-01-10T10:14:33.611660628Z"}
{"id":"CP-sz8","title":"Refactor document.rs: Extract ContentType to dedicated module","description":"document.rs has ContentType enum with MIME conversion, default content generation, and detection logic. Extract to `src/content_type.rs` to be shared between document.rs and sync/ modules (which has partial duplication).","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-30T00:34:51.455397-08:00","updated_at":"2025-12-30T16:42:58.50460762Z","closed_at":"2025-12-30T16:42:58.50460762Z","close_reason":"Closed"}
{"id":"CP-t1e","title":"Subdirectory schema updates create feedback loop","description":"After fixing CP-13l (path resolution), the sync still enters a loop writing .commonplace.json files repeatedly to both workspace/ and workspace/bartleby/.\n\nThe issue is that when subdirectories are migrated to separate documents, SSE events for those subdirectory documents trigger schema writes, which trigger more events.\n\nThe PR #81 fix only skips .commonplace.json in the directory watcher, but SSE events from subdirectory document updates still cause writes.\n\nNeed to add similar skip logic for SSE-triggered schema updates, or deduplicate based on content.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-02T00:41:51.878675146Z","created_by":"jes","updated_at":"2026-01-02T01:03:52.848109258Z","closed_at":"2026-01-02T01:03:52.848109258Z","close_reason":"Fixed by preserving node-backed directory references during directory scanning. When a subdirectory has an existing node_id in .commonplace.json, the scanner now creates a node-backed reference (entries: None, node_id: Some) instead of inline entries, preventing the reconciler from generating new UUIDs on every sync."}
{"id":"CP-t90r","title":"Deduplicate path-to-UUID resolution helpers","description":"## Summary\nPath-to-UUID resolution via fs-root schema is duplicated in multiple CLI/tools (sync.rs, cbd.rs, discovered_manager.rs). Consolidate into a shared helper to keep traversal behavior consistent.\n\n## Files to modify\n- src/bin/sync.rs (use shared helper)\n- src/cbd.rs (use shared helper)\n- src/orchestrator/discovered_manager.rs (use shared helper)\n- src/sync/paths.rs or src/document.rs (new/shared helper for HTTP path resolution)\n\n## Implementation steps\n1. Create a shared async helper that fetches fs-root, traverses schemas, and returns a UUID for a path.\n2. Replace local resolve_path_to_uuid functions with the shared helper.\n3. Ensure consistent error messages and handling of empty/\"{}\" schemas.\n\n## Example\nBefore: three near-identical functions with slightly different schema parsing.\nAfter: one shared resolve_path_to_uuid_http(...) used everywhere.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:26:24.710670793Z","created_by":"jes","updated_at":"2026-01-10T11:02:29.716446359Z","closed_at":"2026-01-10T11:02:29.716446359Z","close_reason":"Added resolve_path_to_uuid_http() shared helper in sync/client.rs. Updated sync.rs and discovered_manager.rs to use it. Removed ~70 lines of duplicate code. cbd.rs left unchanged (uses blocking reqwest)."}
{"id":"CP-tale","title":"Implement inode-branch sync to preserve writes from replaced inodes","description":"Summary: Implement the hardlink shadow strategy from docs/commonplace-hardlink-sync-spec.md so non-atomic writers to old inodes are preserved and merged instead of lost on atomic replace.\n\nFiles to modify:\n- src/sync/sse.rs\n- src/sync/watcher.rs\n- src/sync/file_sync.rs\n- src/sync/state.rs\n- src/sync/state_file.rs\n- src/bin/sync.rs (config/CLI for shadow dir)\n- docs/commonplace-hardlink-sync-spec.md\n\nImplementation steps:\n1. Add inode tracking keyed by (st_dev, st_ino) with commit_id, shadow_path, shadowed_at, last_write; reset it on startup.\n2. On server update for path P: open P, fstat, link /proc/self/fd/{fd} to shadow_dir/{devHex}-{inoHex}, update inode_state, then atomic write temp+rename into P and create inode_state for the new inode.\n3. Watch the shadow directory (not individual files) for IN_MODIFY/IN_CLOSE_WRITE; read shadow path, create a Yjs update against inode_state.commit_id, merge into HEAD, and update commit_id and last_write.\n4. Distinguish atomic vs non-atomic local writes on primary path: same inode = non-atomic diff against inode_state.commit_id; inode change = treat as atomic replace and create inode_state for new inode.\n5. Implement GC: remove shadow links after last_write \u003e IDLE_TIMEOUT and shadowed_at \u003e MIN_SHADOW_LIFETIME; delete inode_state entry; on IN_Q_OVERFLOW, rescan watched paths.\n6. Enforce same-filesystem requirement for shadow_dir; default to /tmp/commonplace-sync/hardlinks; if the synced path is on a different filesystem, require user-configured shadow_dir on that volume.\n\nExample:\nBefore: server update temp+rename replaces inode; a long-lived writer appends to the old inode and those edits disappear.\nAfter: the old inode is hardlinked under shadow_dir and watched; its writes create branch commits that merge into HEAD.\n","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-04T10:52:20.006512646Z","created_by":"jes","updated_at":"2026-01-05T02:55:44.234501626Z","closed_at":"2026-01-05T02:55:44.234501626Z","close_reason":"Implemented inode-branch sync with shadow hardlinks per spec. Core functionality working: atomic writes create shadow hardlinks, shadow directory is watched, writes to old inodes are detected and merged via CRDT. Tested with 4 integration tests. GC can be added as follow-up."}
{"id":"CP-tfx9","title":"Deduplicate UUID map traversal variants","description":"## Summary\nUUID map building functions in sync/uuid_map.rs have multiple variants with duplicated traversal logic (with/without status, with/without write schemas). Extract a shared traversal core to reduce duplication.\n\n## Files to modify\n- src/sync/uuid_map.rs\n\n## Implementation steps\n1. Introduce a shared traversal function that accepts callbacks for schema handling (recording UUIDs, writing schemas) and returns a status.\n2. Update build_uuid_map_recursive, build_uuid_map_recursive_with_status, and build_uuid_map_and_write_schemas to call the shared traversal.\n3. Keep logging and error handling unchanged.\n\n## Example\nBefore: two separate recursive functions with similar fetch/parse loops.\nAfter: one traversal reused with different callbacks.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:55:55.898621905Z","created_by":"jes","updated_at":"2026-01-10T09:55:55.898621905Z"}
{"id":"CP-tmo","title":"Orchestrator should watch for fs-root schema changes","description":"When the server's fs-root schema changes (e.g., via /replace), the orchestrator should detect this and update sandboxes accordingly. Currently requires restart.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-01T19:41:28.363627394Z","created_by":"jes","updated_at":"2026-01-03T08:20:21.159846353Z","closed_at":"2026-01-03T08:20:21.159846353Z","close_reason":"Orchestrator now watches all schema documents recursively via /documents/stream SSE endpoint. Changes to any nested directory schema immediately trigger re-discovery."}
{"id":"CP-tnrc","title":"Deduplicate system-actor handler logic for file-based actors","description":"Summary: DataflowerHandler, TriggererHandler, RendererHandler, and MailboxHandler all share the same create/update/delete instance lifecycle and cleanup logic; extract a shared base/helper to reduce duplication.\n\nFiles to modify:\n- commonplaced-2025/src/system-actor/handlers/dataflower-handler.ts\n- commonplaced-2025/src/system-actor/handlers/triggerer-handler.ts\n- commonplaced-2025/src/system-actor/handlers/renderer-handler.ts\n- commonplaced-2025/src/system-actor/handlers/mailbox-handler.ts\n- commonplaced-2025/src/system-actor (new shared handler base/helper)\n\nImplementation steps:\n1) Create a generic FileActorHandler base that handles per-path instance lifecycle (create/update/delete, cleanup) and common logging.\n2) Move shared behavior (remove instance, create instance, cleanup loop) into the base class.\n3) Update each handler to provide file matching logic and actor factory.\n\nExample:\nBefore: each handler repeats remove/create/cleanup logic with only class-specific types changing.\nAfter: handler subclasses pass a file predicate and factory to FileActorHandler.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:23:46.726899019Z","created_by":"jes","updated_at":"2026-01-10T10:24:04.012605316Z"}
{"id":"CP-tvn6","title":"P1: Sandbox processes running in wrong CWD (workspace instead of temp dir)","description":"## Problem\ncommonplace-ps shows sandbox processes running in wrong CWD (parent commonplace-sync CWD instead of actual sandbox).\n\n## Root Cause\nThe sync process starts initial sync BEFORE spawning exec. Initial sync is BLOCKING and takes too long for workspaces with many files (e.g., bartleby with tmux/). The exec spawn at line 1929 is never reached.\n\nTimeline observed:\n- 23:11:03 - Sync starts\n- 23:11:07 - Still syncing tmux files (54 files)\n- 30 second timeout hit before exec spawn reached\n- Child never spawned, so CWD lookup returns parent's CWD\n\n## Evidence\nDebug log shows:\n- Initial file sync creates 54 files\n- No 'Launching' log message ever appears\n- Process stuck on sigsuspend\n\n## Fix Approaches\n1. Make initial sync non-blocking (spawn exec first, then sync)\n2. Add timeout for initial sync \n3. Limit initial sync to critical files only\n4. Spawn exec in parallel with sync\n\n## Affected Files\n- src/bin/sync.rs:1495-1575 (initial sync blocks exec)\n- src/bin/sync.rs:1876-1931 (exec spawn)","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T23:03:09.811540143Z","created_by":"jes","updated_at":"2026-01-10T23:31:38.347965329Z","closed_at":"2026-01-10T23:31:38.347965329Z","close_reason":"Fixed: Added 3-second timeout for sandbox file sync. Exec now spawns within seconds instead of blocking indefinitely. Verified with commonplace-ps showing correct sandbox CWDs."}
{"id":"CP-tvq9","title":"CI integration: editing __processes.json on disk starts/stops processes","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-11T00:47:32.849437314Z","created_by":"jes","updated_at":"2026-01-11T00:47:32.849437314Z","comments":[{"id":52,"issue_id":"CP-tvq9","author":"jes","text":"Test that file-based editing of __processes.json works correctly.\n\nThis is different from CP-n6sd (which tests add/remove via sync) - this tests:\n1. User directly edits workspace/__processes.json on disk\n2. Sync picks up the change\n3. Orchestrator receives the update via SSE\n4. Processes start/stop/restart accordingly\n\nTest steps:\n1. Verify bartleby is running\n2. Edit workspace/__processes.json to remove bartleby entry (using text editor, not API)\n3. Within 10 seconds, verify bartleby is stopped\n4. Edit workspace/__processes.json to add bartleby back\n5. Within 10 seconds, verify bartleby is running again\n\nThis simulates the actual user workflow of editing config files directly.","created_at":"2026-01-11T00:47:42Z"}]}
{"id":"CP-txg0","title":"Review inotify hardlink shadow code for correctness/perf","description":"Summary: Do a focused code review of the inotify hardlink shadow implementation (inode tracking + shadow dir) to identify correctness bugs, race conditions, or performance issues.\n\nFiles to review:\n- src/sync/sse.rs\n- src/sync/state.rs\n- src/sync/watcher.rs\n- src/bin/sync.rs\n\nReview steps:\n1. Verify inode tracking lifecycle (create, shadow, update, GC) is race-safe under concurrent writes and server updates.\n2. Check inotify watch setup and event handling for shadow dir vs primary path; confirm no missed events or infinite loops.\n3. Validate hardlink creation via /proc/self/fd and fallback paths, including error handling and TOCTOU exposure.\n4. Inspect atomic write path for temp file placement, rename semantics, and inode tracking updates.\n5. Assess performance risks (extra file I/O, hash/diff frequency, watch fanout, GC cadence).\n\nExample focus questions:\n- Can shadow links leak or accumulate under heavy churn?\n- Are old inode writes always merged against the correct commit_id?\n- Does shadow dir creation on every write cause contention?\n","notes":"## Review Completed\n\n### Critical (P2)\n1. **Shadow GC never called** - InodeTracker::gc() exists but is never invoked. Shadow hardlinks and state entries accumulate indefinitely. Fix: add periodic GC task.\n2. **tracker.shadow() return ignored** - sse.rs:817 ignores return value, silent failure if GC races.\n\n### Medium (P3)\n3. **No tracking when commit_id is None** - sse.rs:861-863 skips tracking for empty commits.\n4. **Extra syscalls** - create_dir_all on every write, multiple lock acquisitions.\n\n### Low (P4)\n5. **Unbounded debounce map** - shadow_watcher pending_events could grow.\n\n### Recommendation\nCreate follow-up beads for P2 fixes. P3/P4 can be addressed opportunistically.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T08:47:59.409692272Z","created_by":"jes","updated_at":"2026-01-05T08:57:37.039200038Z","closed_at":"2026-01-05T08:57:37.039200038Z","close_reason":"Review complete. Filed 3 bugs: CP-4r4d (P2 GC never called), CP-5wwk (P2 shadow() return ignored), CP-cr47 (P3 no tracking when commit_id None)"}
{"id":"CP-tz71","title":"Deduplicate base state retrieval in DocumentService","description":"## Summary\nDocumentService repeatedly fetches Yjs base state (get_yjs_state + base64 encode) in multiple branches. Extract a helper to reduce duplication and ensure consistent state handling.\n\n## Files to modify\n- src/services/document.rs\n- src/services/document_utils.rs (new helper)\n\n## Implementation steps\n1. Add helper like get_base_state_b64(doc_store, id) -\u003e Option\u003cString\u003e.\n2. Replace repeated get_yjs_state + b64::encode blocks with helper.\n3. Keep error handling and logging unchanged.\n\n## Example\nBefore: multiple blocks call get_yjs_state(id).await and b64::encode.\nAfter: base_state_b64 = get_base_state_b64(\u0026self.doc_store, id).await.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T10:03:06.939430806Z","created_by":"jes","updated_at":"2026-01-10T10:03:06.939430806Z"}
{"id":"CP-u37g","title":"commonplace-ps should not truncate CWD paths","description":"Summary: Output from `commonplace ps` truncates long CWD paths with \"...\", which hides the actual sandbox directory and makes it hard to identify processes.\n\nFiles to modify:\n- src/orchestrator/status.rs\n- src/bin/ps.rs (or wherever the CLI output formatting lives)\n\nImplementation steps:\n1. Identify the formatting code that truncates CWD paths in `commonplace ps`.\n2. Provide a full-path display option by default (or add a `--full` flag and make it the default for tty output).\n3. Ensure columns still align; if needed, wrap or allow a wider column for CWD.\n4. Add a test or snapshot that asserts full CWD output for long paths.\n\nExample:\nBefore: `...-8ede39a7-e0b3-48ef-99a0-3cd9a2ae589d`\nAfter: `/tmp/sandbox/.../workspace-8ede39a7-e0b3-48ef-99a0-3cd9a2ae589d`\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-05T02:08:17.480679729Z","created_by":"jes","updated_at":"2026-01-05T03:41:04.198495158Z","closed_at":"2026-01-05T03:41:04.198495158Z","close_reason":"Removed CWD truncation - now shows full sandbox paths"}
{"id":"CP-ubn3","title":"Include agent/process name in merkle commits","description":"Summary: Store the originating agent/process name in merkle commit metadata so we can attribute changes and debug sync behavior.\n\nFiles to modify:\n- src/commit.rs (Commit struct: add optional author/agent field)\n- src/services/document.rs (commit creation points)\n- src/api.rs (wire through request-supplied agent name if applicable)\n- src/mqtt/messages.rs (if commit metadata serialized over MQTT)\n- commonplaced-2025/src/documents/commitManager.ts (if TS side writes commits)\n\nImplementation steps:\n1) Add an optional field on Commit (e.g., agent: Option\u003cString\u003e) and update serde.\n2) Thread agent/process name into commit creation (from CLI args, env var, or request header like X-Commonplace-Agent).\n3) Preserve the field when replaying/serializing commits and in any API responses that include commit data.\n4) Add tests or fixtures to ensure commit metadata includes the agent name when provided.\n\nExample:\nBefore: Commit { timestamp, parents, changes, ... }\nAfter: Commit { timestamp, parents, changes, agent: Some(bartleby), ... }","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-10T22:42:18.080425651Z","created_by":"jes","updated_at":"2026-01-10T22:42:37.177870298Z"}
{"id":"CP-udk1","title":"Deduplicate base64 error mapping","description":"## Summary\nBase64 error mapping logic (invalid base64 -\u003e ServiceError::InvalidInput / MqttError::InvalidMessage) is duplicated across services and MQTT ingestion. Extract a shared helper to standardize error messages and avoid drift.\n\n## Files to modify\n- src/services/document.rs\n- src/mqtt/edits.rs\n- src/b64.rs or src/errors.rs (new helper)\n\n## Implementation steps\n1. Add a helper that decodes base64 and maps errors to the desired error type/message.\n2. Replace local map_err blocks with the helper.\n3. Ensure messages remain user-facing and consistent across HTTP and MQTT.\n\n## Example\nBefore: two different error messages for invalid base64.\nAfter: shared decode helper used by both.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:34:35.21224068Z","created_by":"jes","updated_at":"2026-01-10T12:01:18.387025253Z","closed_at":"2026-01-10T12:01:18.387025253Z","close_reason":"Added decode_with_context helper to b64.rs; updated document.rs and edits.rs to use it"}
{"id":"CP-uf2k","title":"Make --graph and --decorate defaults with opt-out","description":"Change commonplace-log defaults:\n- --graph on by default, --no-graph to disable\n- --decorate on by default, --no-decorate to disable\n- --oneline disables graph mode","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T21:11:12.51658411Z","created_by":"jes","updated_at":"2026-01-03T21:11:34.123331745Z","closed_at":"2026-01-03T21:11:34.123331745Z","close_reason":"Implemented --graph and --decorate as defaults with --no-graph and --no-decorate opt-outs"}
{"id":"CP-uj2","title":"Represent directory trees as Yjs JSON (Y.Map) instead of text JSON","description":"Currently, the fs-root document uses TEXT content type with JSON stored as a text string. This was a workaround because the edit system (diff.rs, replay.rs) uses TEXT-based Yjs updates (Y.Text), but DocumentNode for JSON type uses Y.Map internally.\n\nThe proper solution would be to use native Yjs JSON structure (Y.Map) for directory schemas, which would enable:\n- Proper CRDT merging of concurrent schema changes\n- More efficient updates (only changed entries, not full document replacement)\n- Better conflict resolution for concurrent file additions/deletions","design":"Implemented Y.Map support:\n\n1. replay.rs: Extended get_content_and_state_at_commit to support JSON content type by initializing Y.Map root and using map.to_json() for extraction.\n\n2. sync.rs: Added create_yjs_map_update() and json_value_to_any() helper to convert JSON strings to Y.Map updates. Updated push_schema_to_server to use Y.Map updates for initial schema push.\n\nThis enables proper CRDT merging of concurrent schema changes and more efficient updates (only changed entries, not full document replacement).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-26T17:04:59.967959-08:00","updated_at":"2025-12-26T18:01:42.374357-08:00","closed_at":"2025-12-26T18:01:42.374357-08:00","close_reason":"Added Y.Map support for directory schemas: replay.rs now supports JSON content type, sync.rs uses Y.Map updates for initial schema push. Merged in PR #6.","labels":["architecture","sync","yjs"]}
{"id":"CP-uoyx","title":"Deduplicate orchestrator status writing","description":"## Summary\nProcess status writing is duplicated between ProcessManager::write_status and DiscoveredProcessManager::write_status with only minor field differences. Extract a shared formatter to avoid drift and ensure consistent status output.\n\n## Files to modify\n- src/orchestrator/manager.rs (use shared status builder)\n- src/orchestrator/discovered_manager.rs (use shared status builder)\n- src/orchestrator/status.rs (add helper for assembling ProcessStatus entries)\n\n## Implementation steps\n1. Add a helper in status.rs that builds ProcessStatus entries given name, pid, cwd, state, and optional document/source paths.\n2. Update both write_status implementations to call the helper and avoid duplicating state formatting logic.\n3. Keep merge_and_write behavior the same (manager preserves discovered, discovered preserves base).\n\n## Example\nBefore: both managers reimplement state-\u003estring mapping and cwd lookup.\nAfter: shared helper handles state mapping; managers only pass their fields.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:08:38.718957536Z","created_by":"jes","updated_at":"2026-01-10T09:49:58.343681257Z","closed_at":"2026-01-10T09:49:58.343681257Z","close_reason":"Added build_process_status helper in status.rs. Updated both ProcessManager and DiscoveredProcessManager to use shared helper."}
{"id":"CP-usfc","title":"Deduplicate API request structs","description":"## Summary\nDocEditRequest and ReplaceParams request structs are duplicated in api.rs and files.rs. Extract shared request types to avoid drift.\n\n## Files to modify\n- src/api.rs\n- src/files.rs\n- src/http/request_types.rs (new shared module)\n\n## Implementation steps\n1. Move DocEditRequest and ReplaceParams into a shared module.\n2. Update api.rs and files.rs to import shared types.\n3. Keep serde defaults and field names identical.\n\n## Example\nBefore: DocEditRequest defined twice.\nAfter: http::request_types::DocEditRequest used in both.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:00:34.131544405Z","created_by":"jes","updated_at":"2026-01-10T12:10:13.036173009Z","closed_at":"2026-01-10T12:10:13.036173009Z","close_reason":"Made DocEditRequest and ReplaceParams public in api.rs; files.rs now imports them"}
{"id":"CP-uzkd","title":"Deduplicate CommitChange structs across CLIs","description":"## Summary\nCommitChange/ChangesResponse structs are duplicated across log/show/replay. Move shared structs into a common module to avoid schema drift.\n\n## Files to modify\n- src/bin/log.rs\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/cli/changes.rs (new shared structs)\n\n## Implementation steps\n1. Create shared CommitChange + ChangesResponse definitions in a common module.\n2. Update log/show/replay to use the shared definitions for JSON deserialization.\n3. Ensure serde field names remain identical.\n\n## Example\nBefore: three separate CommitChange structs.\nAfter: shared cli::changes::CommitChange used everywhere.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T09:53:06.744249952Z","created_by":"jes","updated_at":"2026-01-10T11:50:09.265350774Z","closed_at":"2026-01-10T11:50:09.265350774Z","close_reason":"Extracted CommitChange and ChangesResponse to cli.rs; updated show.rs, replay.rs, and log.rs to use shared structs"}
{"id":"CP-v5f7","title":"CI integration: __processes.json survives orchestrator restart","notes":"Proposed fix (preferred): add ancestry gating for schema pulls. Before applying server schema content for __processes.json, compare server head vs local last-applied (like SSE should_pull/determine_sync_direction) and skip applying if server is behind. This prevents empty server content from overwriting newer local schema after restarts. Implement in schema sync path (dir_sync.rs/schema_io.rs) so it applies to __processes.json.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-11T00:43:40.546493353Z","created_by":"jes","updated_at":"2026-01-11T01:11:47.101176616Z","closed_at":"2026-01-11T01:11:47.101176616Z","close_reason":"Fixed: Added ancestry gating to prevent server default content from overwriting local data on restart. Added is_default_content_for_type() helper and guards in handle_server_edit and handle_server_edit_with_tracker. When local_cid is None (fresh start) and server has default content ({} for JSON) but local file has substantive content, the pull is skipped. Added integration test test_local_content_survives_restart_with_default_server_content that verifies this scenario. All tests pass.","comments":[{"id":51,"issue_id":"CP-v5f7","author":"jes","text":"Critical bug discovered: __processes.json files were lost after orchestrator restart.\n\nRoot cause investigation needed:\n1. Server had empty content {} for workspace document\n2. __processes.json documents had empty content {}\n3. Local files were intact but sync didn't push them\n4. Had to manually restore via /replace endpoint\n\nThis test should verify:\n1. Start orchestrator with processes running\n2. Stop orchestrator\n3. Restart orchestrator  \n4. Verify all discovered processes are still running\n5. Verify __processes.json content matches local files","created_at":"2026-01-11T00:47:31Z"}]}
{"id":"CP-v6s1","title":"Deduplicate commit storage/head update helper","description":"## Summary\nCommit creation/storage logic is duplicated between MQTT edits ingestion and DocumentService methods. Extract a shared helper for store_commit + set_document_head + commit notification to keep behavior aligned across ingress paths.\n\n## Files to modify\n- src/mqtt/edits.rs (use shared commit helper)\n- src/services/document.rs (expose helper or reuse)\n- src/commit.rs or src/services/commit_utils.rs (new helper)\n\n## Implementation steps\n1. Add a helper that accepts CommitStore, document_id, and Commit and returns (cid, timestamp).\n2. Use it in mqtt/edits.rs and services/document.rs to avoid divergent head/notification behavior.\n3. Keep existing parent inference and merge-commit logic intact; only dedupe the storage/update steps.\n\n## Example\nBefore: mqtt/edits.rs manually store_commit + set_document_head; services has separate logic.\nAfter: both call store_and_set_head(commit_store, doc_id, commit).","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T09:25:41.297754698Z","created_by":"jes","updated_at":"2026-01-10T10:11:15.499378214Z","closed_at":"2026-01-10T10:11:15.499378214Z","close_reason":"Added store_commit_and_set_head() helper to CommitStore. Updated mqtt/edits.rs (handle_edit) and services/document.rs (edit_document, fork_document, replace_content) to use shared logic."}
{"id":"CP-v7wc","title":"Use friendly process name for stdio/stdin log filenames","description":"## Summary\nSandboxed STDIO/STDIN log files are named using raw exec identifiers (or internal names), which makes it hard to tell which process produced them. Filenames should use a friendlier process name (e.g., display name or script basename) so logs are discoverable.\n\n## Files to modify\n- src/bin/sync.rs (sandbox exec log file naming for __\u003cname\u003e.stdout.txt/__\u003cname\u003e.stderr.txt)\n- src/orchestrator/manager.rs (stdio file naming for managed processes)\n- src/orchestrator/discovered_manager.rs (stdio file naming for discovered processes)\n\n## Implementation steps\n1. Identify the process naming data available at spawn time (display name, script basename, or manifest-provided name).\n2. Update the filename builder to use the friendly name, with a safe fallback to the existing exec identifier if not available.\n3. Ensure names are sanitized for filesystem safety (strip path separators, whitespace, etc.).\n4. Add a small test or integration note to confirm filenames match the friendly name for a known process.\n\n## Example\nBefore: __process-1234.stdout.txt\nAfter: __bartleby.stdout.txt (or __my-script.stdout.txt)","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-10T02:51:57.73438549Z","created_by":"jes","updated_at":"2026-01-10T09:07:43.703841133Z","closed_at":"2026-01-10T09:07:43.703841133Z","close_reason":"Already fixed in commit fa87a4f (CP-n7qu). Added --name parameter to sync.rs and pass it from orchestrator. New sandbox processes get friendly names. Old __sh.* files are legacy."}
{"id":"CP-v9r","title":"Sandbox sync uses wrong identifiers for nested node-backed dirs - blocks acceptance","description":"When sandbox sync syncs a directory containing node-backed subdirectories, it constructs wrong document identifiers.\n\nObserved:\n- Sandbox sync tries to connect to: 0e1967af-...:bartleby/output.txt\n- This returns 404 Not Found\n- The actual output.txt has its own node_id: 12504d60-...\n\nExpected:\n- Sandbox sync should resolve the bartleby directory schema first\n- Then find output.txt's actual node_id (12504d60-...)\n- Then sync that document\n\nThe sync is using directory-node-id:relative-path format instead of looking up the actual document node_ids from the subdirectory schema.\n\nThis blocks the acceptance criteria requiring bartleby to sync the full workspace tree.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T06:11:33.596118234Z","created_by":"jes","updated_at":"2026-01-02T06:29:26.741953764Z","closed_at":"2026-01-02T06:29:26.741961394Z"}
{"id":"CP-vbnh","title":"CI integration: process config change triggers restart","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T00:37:14.593387969Z","created_by":"jes","updated_at":"2026-01-11T00:37:14.593387969Z","comments":[{"id":45,"issue_id":"CP-vbnh","author":"jes","text":"Covers acceptance criteria H1-H2 from docs/plans/2026-01-02-sandbox-sync-acceptance-criteria.md:\n\nH1: Edit __processes.json to change a process command\nH2: Verify process restarted with new PID within 10 seconds\n\nRelated to CP-n6sd which covers H3-H6 (add/remove processes).","created_at":"2026-01-11T00:41:14Z"}]}
{"id":"CP-vge7","title":"Persist local_cid to disk for sync state recovery","notes":"Currently SyncState.last_written_cid is only held in memory. On sync client restart, local_cid is None which triggers the CP-v5f7 default content guard as a fallback.\n\nPersisting the local_cid to disk (e.g. .commonplace-sync-state or similar) would allow proper ancestry checking even after restart, making the sync more robust.\n\nCould store per synced path:\n- last_written_cid\n- last_written_content hash  \n- file mtime\n\nThis would eliminate the need for the default content heuristic in most cases.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-11T01:17:16.491039432Z","created_by":"jes","updated_at":"2026-01-11T01:17:43.975226604Z"}
{"id":"CP-vgt5","title":"Migrate dir_sync.rs to use fetch_head helper (9 call sites)","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T18:18:17.244183832Z","created_by":"jes","updated_at":"2026-01-10T18:18:17.244183832Z"}
{"id":"CP-vhp6","title":"P1: cbd head fetch failure silently truncates local data","description":"In cbd.rs around lines 637-645, when fetching head from server fails, the code falls back to local data but may have already modified/truncated it. The fetch_head_state function should not modify local state until server response is confirmed.\n\nLocation: src/cbd.rs:637-645\nFound by: codex review","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T09:09:49.343784378Z","created_by":"jes","updated_at":"2026-01-05T19:51:02.502970461Z","closed_at":"2026-01-05T19:51:02.502970461Z","close_reason":"Fixed: append_issue now fails on non-404 errors instead of silently truncating to empty content"}
{"id":"CP-vqs","title":"Sandbox sync corrupts file content - blocks acceptance","description":"When syncing new files to sandboxes that are nested subdirectories (like bartleby/ inside workspace), the content gets corrupted.\n\nTest case:\n1. Start server, sync, orchestrator as per acceptance criteria\n2. Create file: echo -n 'note' \u003e workspace/bartleby/test-note.txt\n3. Wait 5 seconds\n4. Check sandbox: cat /tmp/commonplace-sandbox-*/bartleby/test-note.txt\n\nExpected: 'note' (4 bytes: 6e6f7465)\nActual: Binary garbage (3 bytes: 9e8b5e)\n\nServer has correct content. Workspace sync works. Only sandbox sync for node-backed subdirectories corrupts content.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T08:19:40.294061839Z","created_by":"jes","updated_at":"2026-01-02T08:39:25.355282132Z","closed_at":"2026-01-02T08:39:25.355282132Z","close_reason":"Fixed - added looks_like_base64_binary() check to prevent misinterpreting short text as base64 binary","comments":[{"id":18,"issue_id":"CP-vqs","author":"jes","text":"Likely root cause: initial server-\u003elocal write in src/sync/dir_sync.rs:554-587 tries to base64-decode text content to detect binary. For a text file containing \"note\", STANDARD.decode(\"note\") succeeds and yields bytes 9e8b5e (confirmed via python3), which is then treated as binary (is_binary_content) and written, matching the reported 3-byte garbage. This path runs when creating new local files from server (sandbox starts empty). Fix likely requires avoiding base64 decode unless server explicitly marks content as base64/binary, or using metadata to decide.","created_at":"2026-01-02T08:23:38Z"}]}
{"id":"CP-vqs4","title":"Sync should use shared lock (LOCK_SH) when reading files","description":"When sync reads a file to compute diffs or upload content, it should acquire LOCK_SH (shared/read lock) to signal to agents that a read is in progress. This prevents agents from starting writes while sync is mid-read, ensuring consistent content is captured. Complements the flock-aware sync spec where sync uses LOCK_EX for writes.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T01:18:01.475757635Z","created_by":"jes","updated_at":"2026-01-08T17:34:13.509925696Z","closed_at":"2026-01-08T17:34:13.509925696Z","close_reason":"Added try_flock_shared() and integrated into file_watcher_task for read coordination with agents","dependencies":[{"issue_id":"CP-vqs4","depends_on_id":"CP-qt6z","type":"blocks","created_at":"2026-01-08T01:18:55.92740908Z","created_by":"jes"}]}
{"id":"CP-vrnb","title":"Add --decorate flag to commonplace-log","description":"Add git-style --decorate flag to commonplace-log that shows ref-like decorations next to commits. For commonplace this would show:\n- (HEAD) marker on the most recent commit\n- Possibly file path on first line\n\nUsage: commonplace log --decorate path/to/file.txt","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T20:59:59.765451441Z","created_by":"jes","updated_at":"2026-01-03T21:01:40.220736743Z","closed_at":"2026-01-03T21:01:40.220736743Z","close_reason":"Added --decorate flag showing cyan (HEAD) marker on newest commit in all output modes (full, oneline, graph)"}
{"id":"CP-vyd6","title":"cbd: add create/update flags for full field set","description":"Summary: Expand cbd create/update to accept the full beads field set plus description from --body-file/stdin.\n\nFiles to modify:\n- src/cbd.rs\n- tests (new CLI parsing tests)\n\nImplementation steps:\n1. Add clap flags for create: --title, --id, --type, --priority, --description, --body-file, --assignee, --labels, --parent, --deps, --due, --defer, --estimate, --acceptance, --design, --notes, --external-ref.\n2. Add matching update flags for the same fields and ensure updates only change fields that are explicitly set.\n3. Parse --labels as comma-separated values and --deps in beads format (type:id or id defaults to blocks), preserving existing dependencies when not supplied.\n4. Support --body-file - to read description from stdin.\n5. Add tests that parse CLI args into Issue updates and verify fields are set/unchanged as expected.\n\nExample:\ncommonplace-bd create \"Fix importer\" --type bug --priority 1 --assignee jes --labels sync,urgent --deps blocks:CP-123 --body-file -\n","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T07:02:37.489749137Z","created_by":"jes","updated_at":"2026-01-05T08:19:58.136660047Z","closed_at":"2026-01-05T08:19:58.136660047Z","close_reason":"Added full field set to create/update: body-file, id, assignee, labels, parent, deps, due, defer, estimate, acceptance, design, notes, external-ref","dependencies":[{"issue_id":"CP-vyd6","depends_on_id":"CP-1evh","type":"blocks","created_at":"2026-01-05T07:02:37.49395258Z","created_by":"jes"},{"issue_id":"CP-vyd6","depends_on_id":"CP-1j5m","type":"discovered-from","created_at":"2026-01-05T07:02:37.497204178Z","created_by":"jes"}]}
{"id":"CP-w2v","title":"Directory JSON as hidden file inside checkout","description":"Synced directory JSON should be a hidden file inside the checked out directory, not outside it. Sync tool should handle this intelligently (avoid syncing the hidden file itself, proper conflict handling, etc.).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:54:00.14442659Z","created_by":"jes","updated_at":"2025-12-30T18:10:05.226276237Z","closed_at":"2025-12-30T18:10:05.226276237Z","close_reason":"Already implemented - schema stored as .commonplace.json inside checkout, ignored in scanning and watcher"}
{"id":"CP-w2xd","title":"Implement hardlink shadow strategy from commonplace-hardlink-sync-spec.md","description":"Summary: Implement the hardlink shadow sync strategy described in docs/commonplace-hardlink-sync-spec.md so writes to replaced inodes are preserved and merged instead of lost.\n\nFiles to modify:\n- src/sync/sse.rs\n- src/sync/watcher.rs\n- src/sync/file_sync.rs\n- src/sync/state.rs\n- src/sync/state_file.rs\n- src/bin/sync.rs\n- docs/commonplace-hardlink-sync-spec.md\n\nImplementation steps:\n1. Add inode_state keyed by (st_dev, st_ino) with commit_id, shadow_path, shadowed_at, last_write; reset it on startup and ensure the shadow dir is on the same filesystem.\n2. On server update for path P: open P, fstat to get inode, link /proc/self/fd/{fd} to shadow_dir/{devHex}-{inoHex}, update inode_state for the old inode, then atomic write temp+rename into P and create inode_state for the new inode.\n3. Watch the shadow directory for IN_MODIFY/IN_CLOSE_WRITE; read the shadow path, create a Yjs update against inode_state.commit_id, merge into HEAD, and update commit_id and last_write.\n4. On primary path writes, detect same inode vs inode change to separate non-atomic vs atomic writes; diff against the correct commit_id and update inode_state.\n5. Implement GC: remove shadow links after idle timeout and minimum shadow lifetime, delete inode_state entry, and rescan on IN_Q_OVERFLOW.\n6. Add tests for atomic replace with long-lived writer (old inode updates preserved), same-filesystem validation for shadow dir, and GC cleanup behavior.\n\nExample:\nBefore: server update replaces inode; writes to the old fd are lost.\nAfter: old inode is hardlinked in shadow_dir and its writes produce mergeable commits.\n","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-05T02:20:21.390276888Z","created_by":"jes","updated_at":"2026-01-05T04:09:13.636563917Z","closed_at":"2026-01-05T04:09:13.636563917Z","close_reason":"Duplicate of CP-tale - already implemented inode shadow sync"}
{"id":"CP-w5sw","title":"WebSocket integration tests","description":"Add integration tests for WebSocket endpoint.\n\nRequires: CP-59h Phase 1 (y-websocket core) - DONE\n\n## Test cases\n\n1. Single client connect and sync\n   - Connect to /ws/docs/{id}\n   - Verify initial SyncStep1/SyncStep2\n   - Verify document state received\n\n2. Two clients bidirectional sync\n   - Client A sends update\n   - Verify Client B receives update\n   - Client B sends update\n   - Verify Client A receives update\n\n3. Reconnection with state recovery\n   - Client connects, receives state\n   - Client disconnects\n   - Document is modified via HTTP\n   - Client reconnects\n   - Verify client receives missed updates\n\n4. Subprotocol negotiation\n   - Connect with y-websocket protocol\n   - Connect with commonplace protocol\n   - Verify correct protocol selected\n\n## Files to create\n\n- tests/ws_tests.rs\n\n## Dependencies\n\nMay need tokio-tungstenite for test client.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-04T02:01:58.469147163Z","created_by":"jes","updated_at":"2026-01-04T02:51:19.951189842Z","closed_at":"2026-01-04T02:51:19.951189842Z","close_reason":"Added 8 integration tests for WebSocket endpoint: connect/sync, bidirectional sync, reconnection, subprotocol negotiation. Fixed bug with empty state vector encoding.","dependencies":[{"issue_id":"CP-w5sw","depends_on_id":"CP-59h","type":"discovered-from","created_at":"2026-01-04T02:02:24.749750465Z","created_by":"jes"}]}
{"id":"CP-w925","title":"Server ContentType::from_mime doesn't recognize text/typescript and other text/* types","description":"ContentType::from_mime() only handles:\n- application/json\n- application/x-ndjson  \n- application/xml, text/xml\n- text/plain\n\nBut sync sends text/typescript for .ts files, text/x-rust for .rs files, etc.\nThese should all map to ContentType::Text.\n\nImpact: .ts files get stored with wrong content type, causing replace endpoint to fail with 'JSON parse error'.\n\nFix: Add catch-all for text/* MIME types to map to ContentType::Text.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T02:10:31.81572127Z","created_by":"jes","updated_at":"2026-01-07T02:22:36.764757592Z","closed_at":"2026-01-07T02:22:36.764757592Z","close_reason":"Added text/* catch-all to ContentType::from_mime(). All text/* MIME types now map to ContentType::Text. Merged in PR #108."}
{"id":"CP-wh1l","title":"Yjs state from Rust yrs not compatible with JS yjs library","description":"When applying Yjs state (from head.state) generated by Rust yrs crate to JavaScript yjs library:\n- Y.applyUpdate() succeeds without error\n- doc.share.entries() shows 'content: AbstractType' instead of 'content: Text'\n- getText('content').toString() returns empty string\n\nWorkaround: SDK now uses head.content directly instead of Yjs state for initial load.\n\nRoot cause: Unknown - may be version mismatch, encoding difference, or yrs/yjs protocol incompatibility.\n\nImpact: Real-time CRDT sync between Rust server and JS clients may not work correctly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-07T01:56:31.290590813Z","created_by":"jes","updated_at":"2026-01-07T02:05:14.137624977Z","closed_at":"2026-01-07T02:05:14.137624977Z","close_reason":"Misdiagnosed - yrs/yjs ARE compatible. Real issue is SDK uses getText() for all files but JSONL uses Y.Array. See new issue for correct fix."}
{"id":"CP-wkgn","title":"Deduplicate content-type upload dispatch","description":"## Summary\nFile content upload selection (json/jsonl/text/binary) is duplicated between file_events.rs (initial push) and file_sync.rs (upload task). Extract a shared helper to decide content type and dispatch to push_* functions consistently.\n\n## Files to modify\n- src/sync/file_events.rs (use shared helper)\n- src/sync/file_sync.rs (use shared helper)\n- src/sync/mod.rs or src/sync/upload.rs (new helper)\n\n## Implementation steps\n1. Add a helper like push_content_by_type(client, server, identifier, content, content_info, state, use_paths).\n2. Reuse in file_events and file_sync for initial pushes and normal uploads.\n3. Ensure JSON/JSONL handling is identical in both paths (content type detection, error logging).\n\n## Example\nBefore: file_events and file_sync each branch on json/jsonl and call push_* manually.\nAfter: both call push_content_by_type(...) and share the same dispatch.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T09:10:42.436973908Z","created_by":"jes","updated_at":"2026-01-10T09:38:45.519203483Z","closed_at":"2026-01-10T09:38:45.519203483Z","close_reason":"Added push_content_by_type helper in client.rs. Updated 5 callers (1 in file_events.rs, 4 in file_sync.rs) to use shared dispatch."}
{"id":"CP-woul","title":"Remove startup retry loop after MQTT refactor","description":"Once CP-o7h0 (MQTT subscriptions) is complete, remove the polling/retry workarounds in sync:\n\n1. Remove the retry loop in sync_schema() (dir_sync.rs:2089-2118) that polls 3x with 3s delays\n2. Remove the dynamic SSE task spawning added in CP-4w00 (no longer needed with wildcard subscriptions)\n3. Remove individual subdir_sse_task spawning at startup\n4. Simplify directory_sse_task or remove entirely\n\nThese workarounds exist because HTTP SSE requires per-document subscriptions. With MQTT wildcards, a single subscription covers the whole tree.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-09T07:00:05.525019105Z","created_by":"jes","updated_at":"2026-01-10T10:46:02.515746107Z","dependencies":[{"issue_id":"CP-woul","depends_on_id":"CP-o7h0","type":"blocks","created_at":"2026-01-09T07:00:11.849292661Z","created_by":"jes"}],"comments":[{"id":32,"issue_id":"CP-woul","author":"jes","text":"The retry loop in sync_schema (lines 1256-1288) runs during initial sync BEFORE any MQTT subscriptions are active. MQTT subscriptions only help after the main event loop starts. Without MQTT retained messages (the code uses retain=false), the retry loop may still be necessary to handle the startup race condition where other sync clients haven't pushed their schemas yet. Need human input to clarify if MQTT should use retained messages or if the retry loop is still needed.","created_at":"2026-01-10T10:46:10Z"},{"id":37,"issue_id":"CP-woul","author":"jes","text":"Skipping: requires human decision on whether to use MQTT retained messages or keep retry loop for startup race condition. Cannot proceed without architectural guidance.","created_at":"2026-01-10T12:41:43Z"}]}
{"id":"CP-wpty","title":"Add ancestry checking to schema sync","description":"Apply the same should_pull() ancestry pattern we use in SSE handlers to schema sync operations. Prevents schema overwrites when local is ahead.","notes":"Reopen: ancestry check is still useful as a guard against pulling stale/empty schema content while local is ahead. Current dedup/CRDT checks do not guarantee recency; they prevent redundant writes but can still allow rollback if server head lags. Consider adding lightweight ancestry gating for schema pulls (or a minimal SyncState for schemas) to skip applying head.content when its commit ancestry does not include local last-applied.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T07:53:57.277966097Z","created_by":"jes","updated_at":"2026-01-10T08:51:45.116653814Z","closed_at":"2026-01-10T08:51:45.116653814Z","close_reason":"Added ancestry checking to schema sync using determine_sync_direction. Prevents schema overwrites when local is ahead.","dependencies":[{"issue_id":"CP-wpty","depends_on_id":"CP-rrw8","type":"blocks","created_at":"2026-01-10T07:54:09.733265589Z","created_by":"jes"}]}
{"id":"CP-x86d","title":"Extract SSE/MQTT subscription spawning into subscriptions.rs","description":"Move directory_sse_task, spawn_subdir_sse_task, subdir_sse_task, directory_mqtt_task, spawn_subdir_mqtt_task, subdir_mqtt_task from dir_sync.rs into a new subscriptions.rs module. Pure extraction, no behavior change.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T07:54:01.655796943Z","created_by":"jes","updated_at":"2026-01-10T08:43:22.387176935Z","closed_at":"2026-01-10T08:43:22.387176935Z","close_reason":"Extracted SSE/MQTT subscription tasks into subscriptions.rs"}
{"id":"CP-x8md","title":"Deduplicate CLI HeadResponse struct","description":"## Summary\nHeadResponse struct is duplicated in bin/show.rs and bin/replay.rs. Extract a shared HeadResponse type for CLI HTTP parsing.\n\n## Files to modify\n- src/bin/show.rs\n- src/bin/replay.rs\n- src/cli/head.rs (new shared struct)\n\n## Implementation steps\n1. Move HeadResponse struct to a shared module.\n2. Update show/replay to import the shared type.\n3. Keep serde field names and Option semantics unchanged.\n\n## Example\nBefore: HeadResponse defined twice.\nAfter: cli::head::HeadResponse used in both.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-10T10:08:08.775962188Z","created_by":"jes","updated_at":"2026-01-10T11:39:41.315432369Z","closed_at":"2026-01-10T11:39:41.315432369Z","close_reason":"Extracted HeadResponse to cli.rs and updated show.rs and replay.rs to use the shared type."}
{"id":"CP-x9ca","title":"Deduplicate JSON file read/write helpers","description":"## Summary\nJSON read/write + error wrapping is repeated between sync/state_file.rs and orchestrator/status.rs. Add shared helpers for read_json_file/write_json_file to reduce duplication.\n\n## Files to modify\n- src/sync/state_file.rs\n- src/orchestrator/status.rs\n- src/util/json_file.rs (new shared helpers)\n\n## Implementation steps\n1. Add async/sync helpers to read/write JSON with consistent error mapping.\n2. Replace local serde_json::to_string_pretty + fs::write + from_str patterns with helpers.\n3. Keep behavior identical (pretty formatting and error kinds).\n\n## Example\nBefore: multiple manual JSON file read/write blocks.\nAfter: json_file::read_json / write_json_pretty used everywhere.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T09:42:47.8833015Z","created_by":"jes","updated_at":"2026-01-10T09:42:47.8833015Z"}
{"id":"CP-xdsc","title":"at_commit replay returns wrong state for some documents","description":"When fetching /docs/{id}/head?at_commit={cid}, the server sometimes returns the same Yjs state for different commits instead of replaying to the correct historical state.\n\nObserved: Commits 130-140 for output.txt all return identical state_hash despite having different commit IDs and timestamps. The content shows '/typing' for all of them.\n\nExpected: Each commit should return the document state AS IT WAS at that commit.\n\nThe changes API correctly lists distinct commits with different timestamps, but the replay mechanism isn't working correctly.\n\nThis breaks commonplace-log diff output since diffs between identical states are empty.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T22:03:35.198195708Z","created_by":"jes","updated_at":"2026-01-03T23:39:55.236438817Z","closed_at":"2026-01-03T23:39:55.236438817Z","close_reason":"Not a bug: at_commit requires documents in memory (via orchestrator/sync). The replay logic is correct - documents must be loaded before historical state can be reconstructed. Added debug logging in 0b6d6b3 for future troubleshooting."}
{"id":"CP-xe35","title":"Add should_pull() helper to SyncDirection enum","description":"Add a helper method to SyncDirection to simplify the repeated pattern in 4 call sites:\n\n```rust\nimpl SyncDirection {\n    /// Returns true if server content should be applied (server is ahead or in sync).\n    pub fn should_pull(\u0026self) -\u003e bool {\n        matches!(self, SyncDirection::Pull | SyncDirection::InSync)\n    }\n}\n```\n\nCurrent pattern repeated in:\n- src/sync/file_sync.rs:1195-1215\n- src/sync/sse.rs:742-751 (refresh_from_head)\n- src/sync/sse.rs:924-937 (handle_server_edit)\n- src/sync/sse.rs:1240-1253 (handle_server_edit_with_tracker)\n\nEach has the same logic: Pull|InSync -\u003e proceed, Push|Diverged -\u003e skip with needs_head_refresh=true","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T07:30:42.465147847Z","created_by":"jes","updated_at":"2026-01-10T07:36:40.746479132Z","closed_at":"2026-01-10T07:36:40.746479132Z","close_reason":"Added should_pull() helper, simplified 3 SSE handlers from 16-line match to 4-line if"}
{"id":"CP-xlrf","title":"Nested node-backed directory schemas not pushed to server by workspace sync","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-10T04:50:06.753577878Z","created_by":"jes","updated_at":"2026-01-10T05:09:49.646815329Z","closed_at":"2026-01-10T05:09:49.646815329Z","close_reason":"Fixed recursion bug in push_nested_schemas_recursive - now properly iterates over child entries and recurses with joined paths"}
{"id":"CP-xuw","title":"Make .processes.json dynamically add/remove processes in running conductor","description":"The conductor should watch for changes to .processes.json files and dynamically start/stop processes without requiring a restart.\n\nCurrently .processes.json is only read at startup. We need:\n1. Watch for .processes.json changes via document subscription\n2. Start new processes when added to .processes.json\n3. Stop processes when removed from .processes.json\n4. Handle graceful restarts when command/args change","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T10:51:20.920681904Z","created_by":"jes","updated_at":"2026-01-01T11:53:36.875939089Z","closed_at":"2026-01-01T11:53:36.875939089Z","close_reason":"Fixed in PR #77. Added --watch-processes flag to orchestrator for dynamic process management. Processes are started/stopped based on document changes with periodic health checks for automatic restart.","comments":[{"id":14,"issue_id":"CP-xuw","author":"jes","text":"Investigation notes:\n\n**Current Architecture:**\n1. ProcessManager (manager.rs) - Handles core processes from orchestrator config file\n2. DiscoveredProcessManager (discovered_manager.rs) - Has machinery for .processes.json but not wired up\n\n**DiscoveredProcessManager already has:**\n- add_process() / remove_process() methods\n- spawn_process() / check_and_restart()\n- Tracks processes in HashMap with ManagedDiscoveredProcess\n\n**What's needed for dynamic updates:**\n1. Add method to watch .processes.json document via SSE\n2. On document change, parse new ProcessesConfig\n3. Diff current vs new config:\n   - Stop processes that were removed\n   - Start processes that were added  \n   - Restart processes where command/args changed\n4. Wire this into the orchestrator or create dedicated mode\n\n**Implementation approach:**\nAdd watch_processes_document() method to DiscoveredProcessManager that:\n- Subscribes to document SSE stream\n- Parses edits as ProcessesConfig\n- Calls reconcile_processes() to diff and update\n","created_at":"2026-01-01T11:43:13Z"}]}
{"id":"CP-y0v","title":"File creation sync fails - blocks acceptance C5/C6","description":"When creating a new file in workspace/bartleby/, the sync client tries to subscribe to SSE before creating the file on the server. This causes repeated 404 errors and the file never syncs to sandboxes.\n\nSteps to reproduce:\n1. echo 'note' \u003e workspace/bartleby/test-note.txt\n2. Wait 5+ seconds\n3. Check bartleby sandbox - file exists but is empty\n4. Check server - returns 404\n5. Check workspace sync logs - repeated 'SSE error: Invalid status code: 404 Not Found'\n\nExpected: File should be created on server and synced to sandbox\nActual: Sync client loops trying to subscribe to non-existent file","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-03T01:27:19.469011853Z","created_by":"jes","updated_at":"2026-01-03T01:53:38.677730662Z","closed_at":"2026-01-03T01:53:38.677730662Z","close_reason":"Fixed by generating UUIDs locally in scan_directory() instead of relying on server reconciliation"}
{"id":"CP-y3t4","title":"Deduplicate functions sync CLI behavior between sync-cli and sync-functions-cli","description":"Summary: sync-cli.ts and sync-functions-cli.ts both implement update-functions.json and functions dry-run logic; consolidate into shared helpers to avoid drift.\n\nFiles to modify:\n- commonplaced-2025/src/sync/filesystem/sync-cli.ts\n- commonplaced-2025/src/sync/filesystem/sync-functions-cli.ts\n- commonplaced-2025/src/sync/filesystem/functions.ts (or new CLI helper module)\n\nImplementation steps:\n1) Extract shared helpers for functions dry-run output and update-json behavior.\n2) Replace duplicated command handlers in both CLIs with helper calls.\n3) Keep CLI flags and output text consistent across commands.\n\nExample:\nBefore: both CLIs manually call reconstituteFunctions/updateFunctionsJson and print the same module/function list.\nAfter: runFunctionsDryRun(functionsDir) reused in both.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:22:53.734852483Z","created_by":"jes","updated_at":"2026-01-10T10:23:11.20797081Z"}
{"id":"CP-y43g","title":"Scope orchestrator lock to config and declared fs subtrees","description":"Summary: Replace the global /tmp/commonplace-orchestrator.lock with a lock keyed to the specific commonplace.json config and its declared fs subtrees so multiple orchestrators can run for different scopes.\n\nFiles to modify:\n- src/bin/orchestrator.rs (lockfile creation/validation)\n- src/orchestrator/config.rs (add explicit config field for managed subtrees)\n- docs/DEVELOPMENT.md or README.md (document new lock behavior)\n\nImplementation steps:\n1. Add config field that lists which subtrees of the commonplace filesystem the orchestrator should manage (e.g., [\"/workspace\", \"/teams/foo\"]).\n2. Compute a lockfile path derived from the config path or a hash of the config + subtree list (e.g., /tmp/commonplace-orchestrator-\u003chash\u003e.lock).\n3. On startup, acquire the scoped lock; refuse to start if another orchestrator owns the same scope.\n4. Ensure subtrees do not overlap between running orchestrators; if overlap detected, error with a clear message.\n\nExample:\n- Orchestrator A manages /workspace/team-a → lockfile A\n- Orchestrator B manages /workspace/team-b → lockfile B\n- Starting B when it overlaps A should error.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-03T19:41:42.285986729Z","created_by":"jes","updated_at":"2026-01-04T00:51:27.414770096Z","closed_at":"2026-01-04T00:51:27.414770096Z","close_reason":"Scoped lock to config file path, multiple orchestrators can now run with different configs"}
{"id":"CP-y9l","title":"tmux ↔ file integration","description":"Bidirectional sync between tmux pane content and commonplace documents:\n- Capture tmux pane content to a document (scrollback, current view)\n- Send content from a document to a tmux pane (sendkeys)\n- Subscribe to pane output as a stream\n- Control tmux sessions/windows/panes via document commands\n\nEnables terminal-as-document patterns and terminal automation.","design":"OPEN QUESTIONS for implementation:\n1. Use tmux command-line or libtmux/tmux control mode?\n2. What's the document schema for pane content?\n3. How often to capture pane content (poll vs events)?\n4. How to identify target pane (session:window.pane)?\n5. Handle scrollback or just visible content?\n6. Security - limit which sessions can be controlled?","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-28T23:53:18.734354-08:00","updated_at":"2026-01-03T20:00:49.121841567Z","closed_at":"2026-01-03T20:00:49.121841567Z","close_reason":"Working","labels":["future-work"],"dependencies":[{"issue_id":"CP-y9l","depends_on_id":"CP-nno","type":"blocks","created_at":"2025-12-28T23:53:18.735182-08:00","created_by":"daemon"}]}
{"id":"CP-yhov","title":"Deduplicate filesystem checkout CLI argument handling","description":"Summary: checkout-cli.ts and inode-checkout-cli.ts are nearly identical for arg parsing, URL validation, and verbose logging; consolidate into a shared CLI helper to avoid drift.\n\nFiles to modify:\n- commonplaced-2025/src/sync/filesystem/checkout-cli.ts\n- commonplaced-2025/src/sync/filesystem/inode-checkout-cli.ts\n- commonplaced-2025/src/sync/filesystem (new shared CLI helper module)\n\nImplementation steps:\n1) Create a shared CLI helper that parses args, validates URL, and prints common logs.\n2) Update both CLI entrypoints to call the helper and pass the specific checkout function and extra verbose message.\n3) Ensure exit codes and error logging remain unchanged.\n\nExample:\nBefore: both files repeat parsing of -v/--verbose, URL validation, and error handling.\nAfter: call runCheckoutCli({ checkoutFn, extraVerboseMessage }).","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-10T10:22:05.571428153Z","created_by":"jes","updated_at":"2026-01-10T10:22:22.093079542Z"}
{"id":"CP-ylp","title":"Enforce mandatory file extensions in filesystem sync","description":"Update commonplace filesystem so that file extensions are mandatory. Only .json, .txt, .xml, .xhtml, and .bin files can be synced, and they must correspond to their Yjs implementations.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T21:56:43.205364505Z","created_by":"jes","updated_at":"2025-12-28T23:14:28.396705982Z","closed_at":"2025-12-28T23:14:28.396705982Z","close_reason":"Implemented mandatory file extension filtering for filesystem sync. Only .json, .txt, .xml, .xhtml, .bin, .md files are now synced. Added is_allowed_extension() function and filtering in scan_dir_recursive(), scan_files_recursive(), DirEvent::Created handler, and handle_schema_change(). Merged in PR #12."}
{"id":"CP-yq8","title":"Refactor sync.rs: Extract SSE subscription loop","description":"Extract SSE subscription handling from sync.rs into `src/sync/sse_client.rs`. This manages server event subscriptions, reconnection, and incoming edit processing.","status":"closed","priority":1,"issue_type":"chore","created_at":"2025-12-30T00:34:51.027152-08:00","updated_at":"2025-12-30T15:51:57.815913081Z","closed_at":"2025-12-30T15:51:57.815913081Z","close_reason":"Extracted SSE subscription loop to src/sync/sse.rs. Merged in PR #40.","dependencies":[{"issue_id":"CP-yq8","depends_on_id":"CP-jwf","type":"blocks","created_at":"2025-12-30T00:36:37.050903-08:00","created_by":"daemon"}]}
{"id":"CP-yw8","title":"Use PR_SET_PDEATHSIG in BOTH orchestrator and sync binaries","description":"When orchestrator and sync spawn child processes, use:\n\n```rust\nunsafe {\n    libc::prctl(libc::PR_SET_PDEATHSIG, libc::SIGKILL);\n}\n```\n\nThis ensures child processes automatically receive SIGKILL when their parent process dies, preventing orphaned processes.\n\nImplementation:\n- In orchestrator: set before spawning sandbox sync processes\n- In sync --sandbox: set before spawning the --exec command\n- Use pre_exec hook in Command::new() to set this before exec\n\nThis is Linux-specific. For cross-platform support, may need conditional compilation.\n\nRelated: CP-a0g (orphaned processes), CP-occ (sync self-cleanup)","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-02T19:20:42.927092195Z","created_by":"jes","updated_at":"2026-01-02T19:35:53.37633517Z","closed_at":"2026-01-02T19:35:53.37633517Z","close_reason":"Added PR_SET_PDEATHSIG to sync binary. PR #92 merged.","dependencies":[{"issue_id":"CP-yw8","depends_on_id":"CP-a0g","type":"relates-to","created_at":"2026-01-02T19:21:00.665641844Z","created_by":"daemon"},{"issue_id":"CP-yw8","depends_on_id":"CP-occ","type":"relates-to","created_at":"2026-01-02T19:21:00.694166033Z","created_by":"daemon"}],"comments":[{"id":19,"issue_id":"CP-yw8","author":"jes","text":"The full process chain requires PR_SET_PDEATHSIG at EACH level:\n\n1. orchestrator spawns sync → sync gets PDEATHSIG\n2. sync --sandbox spawns exec command → exec process gets PDEATHSIG\n\nWithout both, you get partial cleanup:\n- Only in orchestrator: sync dies but leaves exec orphaned\n- Only in sync: exec dies but sync becomes orphaned\n\nBoth binaries must set this in their pre_exec hooks.","created_at":"2026-01-02T19:21:51Z"}]}
{"id":"CP-yx4l","title":"P2: Schema deletions don't propagate - additive merge skips removals when files are deleted locally","description":"When files are deleted locally, push_schema_to_server now uses additive merge which won't remove keys from server schema. This means local deletions (e.g., from handle_file_deleted) won't propagate to other sync clients. Need to decide: use full replacement for deletion pushes or add explicit delete operation.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T20:45:05.446587016Z","created_by":"jes","updated_at":"2026-01-06T21:47:16.270985102Z","closed_at":"2026-01-06T21:47:16.270985102Z","close_reason":"Implemented targeted schema deletions with nested path support. Added create_yjs_json_delete_key() for Yjs updates, delete_schema_entry() for HTTP calls, and detection of node-backed subdirectories. Base state now required to fail loud vs silent no-op. Merged in PR #104."}
{"id":"CP-z7t","title":"MCP server using commonplace JSON file for tool/resource definitions","description":"An MCP server that reads a specified commonplace JSON document and exposes its contents as MCP tools, resources, and prompts. The JSON document defines the MCP schema, and the server dynamically serves those definitions to MCP clients.","design":"OPEN QUESTIONS for implementation:\n1. What JSON schema should define tools? MCP-native schema or custom format?\n2. Should this dynamically reload when the document changes?\n3. How to handle tool implementations - call external commands, evaluate JS, or just return content?\n4. Should it support resources and prompts in addition to tools?\n5. How does this relate to CP-pgx (already implemented) - replace or complement it?","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-28T22:49:01.180475-08:00","updated_at":"2025-12-30T01:44:22.367660722Z","labels":["future-work"]}
{"id":"CP-z8rx","title":"Deduplicate file watcher logic for trigger and dataflow decorators","description":"Summary: TriggerFileWatcher and DataflowFileWatcher both do the same periodic file stat/read/parse/patch loop; extract shared watcher utilities to avoid drift and keep behaviors consistent.\n\nFiles to modify:\n- commonplaced-2025/src/actors/common/trigger-file-watcher.ts\n- commonplaced-2025/src/actors/common/dataflow-file-watcher.ts\n- commonplaced-2025/src/actors/common (add shared watcher helper module if needed)\n\nImplementation steps:\n1) Identify shared behavior (resolve functions file, stat/mtime comparison, read JS, parse errors, periodic timer, start/stop lifecycle).\n2) Create a shared watcher helper/base class that encapsulates timing, lastModified tracking, and file access checks.\n3) Update both watchers to delegate to the helper and keep only trigger/dataflow-specific parse+patch logic.\n\nExample:\nBefore: each watcher manages its own timer, file stat comparison, and access checks.\nAfter: a shared FileWatcherBase handles timer+mtime tracking, and each watcher only supplies parse+update callbacks.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T10:11:36.172329196Z","created_by":"jes","updated_at":"2026-01-10T10:11:54.497235031Z"}
{"id":"CP-zdy","title":"[blocks acceptance] New files in node-backed subdirs don't propagate to other sync clients","description":"When a new file is created in a sandbox that syncs with a node-backed subdirectory (like text-to-telegram), the file is correctly synced to the server and added to that subdirectory's schema. However, other sync clients (main workspace sync, other sandboxes) don't receive the new file because they are only subscribed to the fs-root SSE, not the subdirectory SSE.\n\nTest case:\n1. Create file in text-to-telegram sandbox: echo 'test' \u003e /tmp/commonplace-sandbox-.../c1-sandbox-test.txt\n2. File appears on server with correct content ✓\n3. File does NOT appear in shared workspace /workspace/text-to-telegram/ ✗\n4. File does NOT appear in bartleby sandbox /text-to-telegram/ ✗\n\nRoot cause: Sync clients only subscribe to fs-root SSE for schema changes. They need to also subscribe to all node-backed subdirectory SSEs to detect new file schema entries.\n\nWorkaround: Restart sync clients to pick up new files from initial schema scan.","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-02T09:24:10.775971235Z","created_by":"jes","updated_at":"2026-01-02T09:42:55.095977376Z","closed_at":"2026-01-02T09:42:55.095977376Z","close_reason":"Closed"}
{"id":"CP-zif","title":"Commonplace link tool for shared document references","description":"A symlink/hardlink-like tool (possible names: synclink, commonlink) that works in a checked-out sync directory to create another file pointing to the same document UUID. Implementation likely involves editing the directory JSON to add another entry referencing the same document.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T17:52:42.458856392Z","created_by":"jes","updated_at":"2025-12-30T22:44:51.153396044Z","closed_at":"2025-12-30T22:44:51.153396044Z","close_reason":"Closed"}
{"id":"CP-zjep","title":"File/tmux sync logs 404 on initial commit (reads still succeed)","description":"Summary: Orchestrator logs show 404 errors during initial commit/subscribe for file↔tmux file sync, even though subsequent reads succeed; likely a race where SSE subscribes before the document is created.\n\nFiles to modify:\n- src/sync/sse.rs\n- src/sync/file_sync.rs\n- src/bin/sync.rs\n- src/orchestrator/discovered_manager.rs (log handling)\n\nImplementation steps:\n1. Reproduce by starting orchestrator with tmux/file sync and observe logs for initial 404s (e.g., \"SSE error: Invalid status code: 404 Not Found\").\n2. Trace the creation/subscribe sequence for the file path: ensure the document is created before SSE subscription begins.\n3. If the doc may not exist yet, treat initial 404 as a retryable condition without logging as an error, or gate SSE subscription until creation completes.\n4. Add a test or log assertion that initial sync does not emit 404 errors for newly created files.\n\nExample:\nBefore: Orchestrator log shows 404 errors on startup, but file reads still work.\nAfter: No 404 errors during initial sync; file reads still succeed.\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T03:50:21.407771931Z","created_by":"jes","updated_at":"2026-01-06T17:53:13.369111325Z","closed_at":"2026-01-06T17:53:13.369111325Z","close_reason":"Handle 404 gracefully in SSE subscriptions during initial sync. Log at debug level and retry in 1s instead of treating as error with 5s wait. Used labeled loops to properly skip outer sleep. Applied to all 4 SSE functions. Merged in PR #102."}
{"id":"CP-zjl","title":"Sync tool wrapper mode: run executable in synced directory context","description":"Add a mode to invoke the sync tool that:\n\n1. Starts commonplace-sync to check out a directory tree to a local path\n2. Launches a specified executable in that synced directory context\n3. Keeps sync running while the executable is active\n4. When the child executable exits, the sync tool also exits and cleans up\n\nExample usage:\n```\ncommonplace-sync --exec \"vim .\" --server http://localhost:3000 --prefix notes --local ./workspace\n```\n\nThis enables workflows where a user wants to work on synced files with their preferred editor/tool, and have everything tear down cleanly when they're done.\n\n**Orchestrator compatibility:**\nThe sync tool should respect environment variable conventions of orchestrator-launched processes:\n- Pass through relevant env vars to the child process (e.g., `$EDITOR`, `$SHELL`, `$TERM`)\n- Respect standard orchestrator signals (SIGTERM, SIGINT) for graceful shutdown\n- Propagate child exit codes to the parent\n- Honor env-based configuration (e.g., `COMMONPLACE_SERVER`, `COMMONPLACE_PREFIX`)\n- Support `--` separator for passing args to the child executable","design":"Exec mode implementation approach: Add --exec flag to Args struct. When provided, after initial sync completes, spawn child process in synced directory using tokio::process::Command. Pass through env vars (EDITOR, SHELL, TERM, etc). Handle SIGTERM/SIGINT to gracefully shutdown both sync tasks and child. Propagate child exit code. Support -- separator via clap's trailing_var_arg.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-30T00:13:55.940464-08:00","updated_at":"2025-12-30T08:40:24.04936003Z","closed_at":"2025-12-30T08:40:24.04936003Z","close_reason":"Implemented exec mode for sync tool. Added --exec flag that syncs directory, launches command, keeps sync running during execution, and propagates exit code on completion. Handles signals gracefully. Pushed directly to main in 1e83d1b."}
{"id":"CP-zkk","title":"Implement CRDT merge for offline changes in sync tool","description":"Follow-up from CP-dgu. When the sync tool detects offline local changes on restart (via state file hash comparison), it should:\n\n1. Fetch historical Yjs state at last_synced_cid using ?at_commit endpoint\n2. Compute diff from historical state to current local content\n3. Create Yjs update from diff\n4. Push update to server (CRDT automatically merges with any server changes)\n5. Pull merged result and update local file\n\nThe infrastructure is ready:\n- State file tracks last_synced_cid and file hashes (CP-dgu, PR #36)\n- ?at_commit endpoint returns Yjs state at any historical commit\n- Offline change detection logs 'Detected offline local changes' on startup\n\nJust needs the merge logic implementation in run_file_mode after detecting changes.","notes":"Reviewed code: offline CRDT merge is already implemented in run_file_mode (lines 322-381). Detects offline changes, pushes via replace endpoint with last_synced_cid as parent for proper CRDT merge.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T06:27:49.553275883Z","created_by":"jes","updated_at":"2025-12-30T18:42:48.412284179Z","closed_at":"2025-12-30T18:42:48.412284179Z","close_reason":"Closed","labels":["feature"],"comments":[{"id":3,"issue_id":"CP-zkk","author":"jes","text":"Discovered from CP-dgu during implementation. Design doc: docs/plans/2025-01-02-sync-state-persistence-design.md","created_at":"2025-12-30T06:28:03Z"}]}
{"id":"CP-zpmt","title":"commonplace-ps should show source path for each process","description":"Add a new column to commonplace-ps output showing where each process is defined:\n- For discovered processes: show the document_path (e.g., /beads, /bartleby)\n- For orchestrator-defined processes: show 'commonplace.json'\n\nThis helps users understand which __processes.json or config file is responsible for each running process.\n\nCurrent output:\nNAME                      PID STATE      CWD\nfilter-open            777796 Running    /home/jes/commonplace\n\nProposed output:\nNAME                      PID STATE      SOURCE              CWD\nfilter-open            777796 Running    /beads              /home/jes/commonplace\nserver                 777478 Running    commonplace.json    /home/jes/commonplace","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-07T03:13:50.458532903Z","created_by":"jes","updated_at":"2026-01-07T03:46:33.119844942Z","closed_at":"2026-01-07T03:46:33.119844942Z","close_reason":"Added SOURCE column showing where each process is defined (/beads, /tmux, etc. for discovered; commonplace.json for base processes)"}
{"id":"CP-zr8","title":"Orchestrator shutdown should allow sync to terminate exec child or kill child group","description":"Summary: Orchestrator shutdown can SIGKILL sync before it has time to terminate its sandbox exec child, leaving orphaned app processes. Increase the shutdown grace period and/or add a second-stage kill that targets the exec child process group when known.\n\nFiles to modify:\n- src/orchestrator/manager.rs (ProcessManager::shutdown timeout/kill sequence)\n- src/orchestrator/discovered_manager.rs (DiscoveredProcessManager::shutdown timeout/kill sequence)\n- src/bin/sync.rs (optional: emit child PID/PGID for orchestrator to use)\n\nImplementation steps:\n1. Make the shutdown timeout configurable (CLI flag or config entry) or increase the default grace period in both managers.\n2. Add a second-stage kill path that, after the timeout, attempts to kill the exec child process group if its PID/PGID is available.\n3. If needed, have sync write its exec child PID/PGID to a pidfile or log line so the orchestrator can target it.\n4. Update logs to show which PIDs/groups were terminated and which path was taken.\n\nExample:\nBefore: orchestrator shutdown waits 5s, then SIGKILLs sync process group only.\nAfter: orchestrator waits N seconds, then SIGKILLs sync group and exec child group (if known).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-02T19:25:28.238493346Z","created_by":"jes","updated_at":"2026-01-02T19:38:29.091230678Z","closed_at":"2026-01-02T19:38:29.091230678Z","close_reason":"Increased shutdown grace period from 5s to 10s, added debug logging. PR #93 merged."}
